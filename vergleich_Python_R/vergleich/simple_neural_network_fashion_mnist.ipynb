{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages and functions and set the session seed\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import tensorflow as tf\n",
    "from tensorflow.random import set_seed \n",
    "tf.random.set_seed(1234)\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout, SpatialDropout2D\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST data from Keras\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image data by dividing through the maximum pixel value (=255)\n",
    "train_images = train_images / train_images.max()\n",
    "test_images = test_images / test_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-20 12:07:56.579508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:56.624771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:56.625300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:56.627092: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-20 12:07:56.673936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:56.674527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:56.674955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:57.538297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:57.538737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:57.539133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 12:07:57.539810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6203 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Build a simple three-layer (1 hidden layer) model\n",
    "# The input size is 28 x 28 pixels and is flattened to a vector of length 784\n",
    "# The activation function is RELU (rectified linear unit) and performs the \n",
    "# multiplication of input and weights (plus bias)\n",
    "# The output (softmax) layer returns probabilities for all ten classes\n",
    "three_layer_model = Sequential()\n",
    "three_layer_model.add(Flatten(input_shape = (28, 28)))\n",
    "three_layer_model.add(Dense(128, activation = 'relu'))\n",
    "three_layer_model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1313/1313 - 4s - loss: 0.0742 - acc: 0.9728 - val_loss: 0.5767 - val_acc: 0.8888 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "1313/1313 - 3s - loss: 0.0717 - acc: 0.9729 - val_loss: 0.5901 - val_acc: 0.8881 - 3s/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "1313/1313 - 3s - loss: 0.0710 - acc: 0.9733 - val_loss: 0.5622 - val_acc: 0.8883 - 3s/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "1313/1313 - 3s - loss: 0.0699 - acc: 0.9745 - val_loss: 0.5675 - val_acc: 0.8917 - 3s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "1313/1313 - 3s - loss: 0.0667 - acc: 0.9753 - val_loss: 0.5999 - val_acc: 0.8850 - 3s/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "1313/1313 - 3s - loss: 0.0639 - acc: 0.9767 - val_loss: 0.5730 - val_acc: 0.8912 - 3s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "1313/1313 - 3s - loss: 0.0684 - acc: 0.9753 - val_loss: 0.5932 - val_acc: 0.8913 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "1313/1313 - 3s - loss: 0.0611 - acc: 0.9775 - val_loss: 0.5933 - val_acc: 0.8874 - 3s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "1313/1313 - 3s - loss: 0.0661 - acc: 0.9757 - val_loss: 0.6046 - val_acc: 0.8893 - 3s/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "1313/1313 - 3s - loss: 0.0611 - acc: 0.9775 - val_loss: 0.6132 - val_acc: 0.8863 - 3s/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf803eebb0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with accuracy metric and adam optimizer\n",
    "# Sparse categorical cross-entropy is the loss function for integer labels\n",
    "# Fit the model using 70 percent of the data and 10 epochs\n",
    "three_layer_model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                          optimizer = 'adam', metrics = ['acc'])\n",
    "three_layer_model.fit(train_images, train_labels, epochs = 10, \n",
    "                      validation_split = 0.3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6749 - acc: 0.8820\n",
      "Model with three layers and ten epochs -- Test loss: 67.49342679977417\n",
      "Model with three layers and ten epochs -- Test accuracy: 88.20000290870667\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = three_layer_model.evaluate(test_images, test_labels)\n",
    "print(\"Model with three layers and ten epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Model with three layers and ten epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly as before, build a five-layer (3 hidden layers) model\n",
    "five_layer_model = Sequential()\n",
    "five_layer_model.add(Flatten(input_shape = (28, 28)))\n",
    "five_layer_model.add(Dense(128, activation = 'relu'))\n",
    "five_layer_model.add(Dense(128, activation = 'relu'))\n",
    "five_layer_model.add(Dense(128, activation = 'relu'))\n",
    "five_layer_model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1313/1313 - 4s - loss: 0.5237 - accuracy: 0.8125 - val_loss: 0.4040 - val_accuracy: 0.8561 - 4s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "1313/1313 - 4s - loss: 0.3867 - accuracy: 0.8576 - val_loss: 0.3647 - val_accuracy: 0.8676 - 4s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "1313/1313 - 4s - loss: 0.3482 - accuracy: 0.8709 - val_loss: 0.3467 - val_accuracy: 0.8736 - 4s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "1313/1313 - 3s - loss: 0.3250 - accuracy: 0.8794 - val_loss: 0.3636 - val_accuracy: 0.8699 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "1313/1313 - 4s - loss: 0.3048 - accuracy: 0.8866 - val_loss: 0.3394 - val_accuracy: 0.8794 - 4s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "1313/1313 - 4s - loss: 0.2904 - accuracy: 0.8909 - val_loss: 0.3499 - val_accuracy: 0.8703 - 4s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "1313/1313 - 3s - loss: 0.2763 - accuracy: 0.8959 - val_loss: 0.3189 - val_accuracy: 0.8847 - 3s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "1313/1313 - 4s - loss: 0.2636 - accuracy: 0.8991 - val_loss: 0.3248 - val_accuracy: 0.8841 - 4s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "1313/1313 - 4s - loss: 0.2528 - accuracy: 0.9051 - val_loss: 0.3513 - val_accuracy: 0.8797 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "1313/1313 - 3s - loss: 0.2440 - accuracy: 0.9078 - val_loss: 0.3381 - val_accuracy: 0.8799 - 3s/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf803ee4f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with accuracy metric and adam optimizer\n",
    "# Fit the model using 70 percent of the data and 10 epochs\n",
    "five_layer_model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                         optimizer = 'adam', metrics = ['accuracy'])\n",
    "five_layer_model.fit(train_images, train_labels, epochs = 10, \n",
    "                     validation_split = 0.3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3664 - accuracy: 0.8701\n",
      "Model with five layers and ten epochs -- Test loss: 36.636048555374146\n",
      "Model with five layers and ten epochs -- Test accuracy: 87.01000213623047\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = five_layer_model.evaluate(test_images, test_labels)\n",
    "print(\"Model with five layers and ten epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Model with five layers and ten epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly as before, build a ten-layer (8 hidden layers) model\n",
    "ten_layer_model = Sequential()\n",
    "ten_layer_model.add(Flatten(input_shape = (28, 28)))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1313/1313 - 5s - loss: 0.5936 - accuracy: 0.7814 - val_loss: 0.4528 - val_accuracy: 0.8426 - 5s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "1313/1313 - 4s - loss: 0.4271 - accuracy: 0.8459 - val_loss: 0.3891 - val_accuracy: 0.8614 - 4s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "1313/1313 - 4s - loss: 0.3792 - accuracy: 0.8639 - val_loss: 0.3790 - val_accuracy: 0.8624 - 4s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "1313/1313 - 4s - loss: 0.3565 - accuracy: 0.8712 - val_loss: 0.3591 - val_accuracy: 0.8683 - 4s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "1313/1313 - 4s - loss: 0.3366 - accuracy: 0.8783 - val_loss: 0.3483 - val_accuracy: 0.8809 - 4s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "1313/1313 - 4s - loss: 0.3276 - accuracy: 0.8815 - val_loss: 0.3765 - val_accuracy: 0.8645 - 4s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "1313/1313 - 4s - loss: 0.3104 - accuracy: 0.8885 - val_loss: 0.3317 - val_accuracy: 0.8830 - 4s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "1313/1313 - 4s - loss: 0.3010 - accuracy: 0.8900 - val_loss: 0.3892 - val_accuracy: 0.8668 - 4s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "1313/1313 - 4s - loss: 0.2869 - accuracy: 0.8956 - val_loss: 0.3427 - val_accuracy: 0.8818 - 4s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "1313/1313 - 4s - loss: 0.2825 - accuracy: 0.8980 - val_loss: 0.3773 - val_accuracy: 0.8708 - 4s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb01592e220>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with accuracy metric and adam optimizer\n",
    "# Fit the model using 70 percent of the data and 10 epochs\n",
    "ten_layer_model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                        optimizer = 'adam', metrics = ['accuracy'])\n",
    "ten_layer_model.fit(train_images, train_labels, epochs = 10, \n",
    "                    validation_split = 0.3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4019 - accuracy: 0.8646\n",
      "Model with ten layers and ten epochs -- Test loss: 40.19055962562561\n",
      "Model with ten layers and ten epochs -- Test accuracy: 86.46000027656555\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = ten_layer_model.evaluate(test_images, test_labels)\n",
    "print(\"Model with ten layers and ten epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Model with ten layers and ten epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1313/1313 - 3s - loss: 0.2349 - accuracy: 0.9117 - val_loss: 0.3197 - val_accuracy: 0.8884 - 3s/epoch - 2ms/step\n",
      "Epoch 2/50\n",
      "1313/1313 - 3s - loss: 0.2270 - accuracy: 0.9152 - val_loss: 0.3832 - val_accuracy: 0.8742 - 3s/epoch - 2ms/step\n",
      "Epoch 3/50\n",
      "1313/1313 - 3s - loss: 0.2213 - accuracy: 0.9178 - val_loss: 0.3222 - val_accuracy: 0.8885 - 3s/epoch - 2ms/step\n",
      "Epoch 4/50\n",
      "1313/1313 - 2s - loss: 0.2135 - accuracy: 0.9200 - val_loss: 0.3209 - val_accuracy: 0.8922 - 2s/epoch - 2ms/step\n",
      "Epoch 5/50\n",
      "1313/1313 - 3s - loss: 0.2058 - accuracy: 0.9217 - val_loss: 0.3335 - val_accuracy: 0.8905 - 3s/epoch - 2ms/step\n",
      "Epoch 6/50\n",
      "1313/1313 - 3s - loss: 0.1984 - accuracy: 0.9259 - val_loss: 0.3623 - val_accuracy: 0.8768 - 3s/epoch - 2ms/step\n",
      "Epoch 7/50\n",
      "1313/1313 - 3s - loss: 0.1943 - accuracy: 0.9261 - val_loss: 0.3249 - val_accuracy: 0.8922 - 3s/epoch - 3ms/step\n",
      "Epoch 8/50\n",
      "1313/1313 - 3s - loss: 0.1880 - accuracy: 0.9295 - val_loss: 0.3389 - val_accuracy: 0.8888 - 3s/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "1313/1313 - 3s - loss: 0.1807 - accuracy: 0.9326 - val_loss: 0.3453 - val_accuracy: 0.8907 - 3s/epoch - 3ms/step\n",
      "Epoch 10/50\n",
      "1313/1313 - 3s - loss: 0.1769 - accuracy: 0.9332 - val_loss: 0.3582 - val_accuracy: 0.8856 - 3s/epoch - 2ms/step\n",
      "Epoch 11/50\n",
      "1313/1313 - 3s - loss: 0.1695 - accuracy: 0.9355 - val_loss: 0.3516 - val_accuracy: 0.8910 - 3s/epoch - 2ms/step\n",
      "Epoch 12/50\n",
      "1313/1313 - 3s - loss: 0.1675 - accuracy: 0.9377 - val_loss: 0.3766 - val_accuracy: 0.8829 - 3s/epoch - 2ms/step\n",
      "Epoch 13/50\n",
      "1313/1313 - 3s - loss: 0.1629 - accuracy: 0.9395 - val_loss: 0.3602 - val_accuracy: 0.8863 - 3s/epoch - 2ms/step\n",
      "Epoch 14/50\n",
      "1313/1313 - 3s - loss: 0.1607 - accuracy: 0.9388 - val_loss: 0.3522 - val_accuracy: 0.8914 - 3s/epoch - 2ms/step\n",
      "Epoch 15/50\n",
      "1313/1313 - 3s - loss: 0.1544 - accuracy: 0.9425 - val_loss: 0.3626 - val_accuracy: 0.8921 - 3s/epoch - 3ms/step\n",
      "Epoch 16/50\n",
      "1313/1313 - 3s - loss: 0.1504 - accuracy: 0.9437 - val_loss: 0.3898 - val_accuracy: 0.8796 - 3s/epoch - 2ms/step\n",
      "Epoch 17/50\n",
      "1313/1313 - 3s - loss: 0.1463 - accuracy: 0.9444 - val_loss: 0.3693 - val_accuracy: 0.8920 - 3s/epoch - 2ms/step\n",
      "Epoch 18/50\n",
      "1313/1313 - 3s - loss: 0.1446 - accuracy: 0.9454 - val_loss: 0.3840 - val_accuracy: 0.8887 - 3s/epoch - 2ms/step\n",
      "Epoch 19/50\n",
      "1313/1313 - 3s - loss: 0.1383 - accuracy: 0.9480 - val_loss: 0.3853 - val_accuracy: 0.8886 - 3s/epoch - 2ms/step\n",
      "Epoch 20/50\n",
      "1313/1313 - 4s - loss: 0.1371 - accuracy: 0.9490 - val_loss: 0.3976 - val_accuracy: 0.8868 - 4s/epoch - 3ms/step\n",
      "Epoch 21/50\n",
      "1313/1313 - 3s - loss: 0.1334 - accuracy: 0.9503 - val_loss: 0.3872 - val_accuracy: 0.8904 - 3s/epoch - 3ms/step\n",
      "Epoch 22/50\n",
      "1313/1313 - 3s - loss: 0.1288 - accuracy: 0.9516 - val_loss: 0.3897 - val_accuracy: 0.8940 - 3s/epoch - 3ms/step\n",
      "Epoch 23/50\n",
      "1313/1313 - 3s - loss: 0.1258 - accuracy: 0.9530 - val_loss: 0.4403 - val_accuracy: 0.8849 - 3s/epoch - 2ms/step\n",
      "Epoch 24/50\n",
      "1313/1313 - 3s - loss: 0.1255 - accuracy: 0.9535 - val_loss: 0.4323 - val_accuracy: 0.8877 - 3s/epoch - 3ms/step\n",
      "Epoch 25/50\n",
      "1313/1313 - 3s - loss: 0.1213 - accuracy: 0.9547 - val_loss: 0.4147 - val_accuracy: 0.8893 - 3s/epoch - 2ms/step\n",
      "Epoch 26/50\n",
      "1313/1313 - 3s - loss: 0.1166 - accuracy: 0.9568 - val_loss: 0.4209 - val_accuracy: 0.8883 - 3s/epoch - 3ms/step\n",
      "Epoch 27/50\n",
      "1313/1313 - 3s - loss: 0.1157 - accuracy: 0.9570 - val_loss: 0.4119 - val_accuracy: 0.8922 - 3s/epoch - 2ms/step\n",
      "Epoch 28/50\n",
      "1313/1313 - 3s - loss: 0.1139 - accuracy: 0.9568 - val_loss: 0.4301 - val_accuracy: 0.8914 - 3s/epoch - 2ms/step\n",
      "Epoch 29/50\n",
      "1313/1313 - 3s - loss: 0.1096 - accuracy: 0.9589 - val_loss: 0.4434 - val_accuracy: 0.8857 - 3s/epoch - 2ms/step\n",
      "Epoch 30/50\n",
      "1313/1313 - 3s - loss: 0.1093 - accuracy: 0.9583 - val_loss: 0.4368 - val_accuracy: 0.8902 - 3s/epoch - 2ms/step\n",
      "Epoch 31/50\n",
      "1313/1313 - 3s - loss: 0.1052 - accuracy: 0.9609 - val_loss: 0.4638 - val_accuracy: 0.8868 - 3s/epoch - 2ms/step\n",
      "Epoch 32/50\n",
      "1313/1313 - 3s - loss: 0.1050 - accuracy: 0.9606 - val_loss: 0.4522 - val_accuracy: 0.8887 - 3s/epoch - 2ms/step\n",
      "Epoch 33/50\n",
      "1313/1313 - 4s - loss: 0.1031 - accuracy: 0.9616 - val_loss: 0.4523 - val_accuracy: 0.8891 - 4s/epoch - 3ms/step\n",
      "Epoch 34/50\n",
      "1313/1313 - 3s - loss: 0.0982 - accuracy: 0.9633 - val_loss: 0.4646 - val_accuracy: 0.8857 - 3s/epoch - 2ms/step\n",
      "Epoch 35/50\n",
      "1313/1313 - 3s - loss: 0.0977 - accuracy: 0.9633 - val_loss: 0.4607 - val_accuracy: 0.8890 - 3s/epoch - 2ms/step\n",
      "Epoch 36/50\n",
      "1313/1313 - 3s - loss: 0.0977 - accuracy: 0.9638 - val_loss: 0.4558 - val_accuracy: 0.8906 - 3s/epoch - 2ms/step\n",
      "Epoch 37/50\n",
      "1313/1313 - 3s - loss: 0.0919 - accuracy: 0.9652 - val_loss: 0.4722 - val_accuracy: 0.8900 - 3s/epoch - 2ms/step\n",
      "Epoch 38/50\n",
      "1313/1313 - 3s - loss: 0.0927 - accuracy: 0.9642 - val_loss: 0.4934 - val_accuracy: 0.8897 - 3s/epoch - 2ms/step\n",
      "Epoch 39/50\n",
      "1313/1313 - 3s - loss: 0.0889 - accuracy: 0.9672 - val_loss: 0.5012 - val_accuracy: 0.8862 - 3s/epoch - 2ms/step\n",
      "Epoch 40/50\n",
      "1313/1313 - 4s - loss: 0.0881 - accuracy: 0.9680 - val_loss: 0.5322 - val_accuracy: 0.8826 - 4s/epoch - 3ms/step\n",
      "Epoch 41/50\n",
      "1313/1313 - 3s - loss: 0.0852 - accuracy: 0.9675 - val_loss: 0.4906 - val_accuracy: 0.8901 - 3s/epoch - 2ms/step\n",
      "Epoch 42/50\n",
      "1313/1313 - 3s - loss: 0.0825 - accuracy: 0.9699 - val_loss: 0.5305 - val_accuracy: 0.8893 - 3s/epoch - 3ms/step\n",
      "Epoch 43/50\n",
      "1313/1313 - 3s - loss: 0.0827 - accuracy: 0.9692 - val_loss: 0.4972 - val_accuracy: 0.8881 - 3s/epoch - 2ms/step\n",
      "Epoch 44/50\n",
      "1313/1313 - 3s - loss: 0.0817 - accuracy: 0.9695 - val_loss: 0.5257 - val_accuracy: 0.8871 - 3s/epoch - 2ms/step\n",
      "Epoch 45/50\n",
      "1313/1313 - 3s - loss: 0.0824 - accuracy: 0.9692 - val_loss: 0.5402 - val_accuracy: 0.8824 - 3s/epoch - 2ms/step\n",
      "Epoch 46/50\n",
      "1313/1313 - 3s - loss: 0.0768 - accuracy: 0.9718 - val_loss: 0.5482 - val_accuracy: 0.8894 - 3s/epoch - 2ms/step\n",
      "Epoch 47/50\n",
      "1313/1313 - 4s - loss: 0.0775 - accuracy: 0.9706 - val_loss: 0.5562 - val_accuracy: 0.8888 - 4s/epoch - 3ms/step\n",
      "Epoch 48/50\n",
      "1313/1313 - 3s - loss: 0.0757 - accuracy: 0.9716 - val_loss: 0.5655 - val_accuracy: 0.8863 - 3s/epoch - 2ms/step\n",
      "Epoch 49/50\n",
      "1313/1313 - 3s - loss: 0.0766 - accuracy: 0.9715 - val_loss: 0.5614 - val_accuracy: 0.8854 - 3s/epoch - 3ms/step\n",
      "Epoch 50/50\n",
      "1313/1313 - 3s - loss: 0.0740 - accuracy: 0.9728 - val_loss: 0.5696 - val_accuracy: 0.8911 - 3s/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with accuracy metric and adam optimizer\n",
    "# Fit the model using 70 percent of the data and 50 epochs\n",
    "three_layer_model_50_epochs = three_layer_model.fit(train_images, train_labels, \n",
    "                                                  epochs = 50, validation_split = 0.3,\n",
    "                                                  verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6749 - acc: 0.8820\n",
      "Model with three layers and fifty epochs -- Test loss: 67.49342679977417\n",
      "Model with three layers and fifty epochs -- Test accuracy: 88.20000290870667\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = three_layer_model.evaluate(test_images, test_labels)\n",
    "print(\"Model with three layers and fifty epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Model with three layers and fifty epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Plot accuracy as function of epochs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mthree_layer_model_50_epochs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(three_layer_model_50_epochs\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss as function of epochs\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(three_layer_model_50_epochs.history['val_loss'], 'blue')\n",
    "plt.plot(three_layer_model_50_epochs.history['loss'], 'red')\n",
    "plt.legend(['Cross-validation', 'Training'], loc = 'upper left')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "# Plot accuracy as function of epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(three_layer_model_50_epochs.history['acc'], 'red')\n",
    "plt.plot(three_layer_model_50_epochs.history['val_acc'], 'blue')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplots_adjust(wspace = .35)\n",
    "\n",
    "# Include plot title and show the plot\n",
    "plt.suptitle('Model loss and accuracy over epochs for a three-layer neural network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print predictions versus actual labels\n",
    "predictions = three_layer_model.predict(test_images)\n",
    "for i in range(10):\n",
    "  print(\"Prediction \" + str(i) + \": \" + str(np.argmax(np.round(predictions[i]))))\n",
    "  print(\"Actual \" + str(i) + \": \" + str(test_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data for a convolutional neural network\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to the correct format (the last 1 stands for greyscale)\n",
    "train_images = train_images.reshape(60000, 28, 28, 1)\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image data to numeric data and normalize them\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / train_images.max()\n",
    "test_images = test_images / test_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the label data\n",
    "# Convert every number to a vector of the length of the number of categories\n",
    "# The vector has zero everywhere except a one on the position of the number it \n",
    "# represents. Example: 3 = [0 0 0 1 0 0 0 0 0 0]\n",
    "train_labels_bin = to_categorical(train_labels)\n",
    "test_labels_bin = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a convolutional neural network with two convolutional layers\n",
    "conv_model = Sequential()\n",
    "conv_model.add(Conv2D(128, (3, 3), input_shape = (28, 28, 1)))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "conv_model.add(Conv2D(128, (3, 3)))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(128))\n",
    "conv_model.add(Dense(10))\n",
    "conv_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compile and fit the model with adam optimizer and accuracy metric\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Categorical cross-entropy is the loss function for one-hot encoded labels and\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# batch size equal to the number of neurons in the convolutional layers and 10 epochs\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mconv_model\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      5\u001b[0m                    optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m conv_model\u001b[38;5;241m.\u001b[39mfit(train_images, train_labels_bin, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, \n\u001b[1;32m      7\u001b[0m                epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile and fit the model with adam optimizer and accuracy metric\n",
    "# Categorical cross-entropy is the loss function for one-hot encoded labels and\n",
    "# batch size equal to the number of neurons in the convolutional layers and 10 epochs\n",
    "conv_model.compile(loss = \"categorical_crossentropy\", \n",
    "                   optimizer = 'adam', metrics = ['accuracy'])\n",
    "conv_model.fit(train_images, train_labels_bin, batch_size = 128, \n",
    "               epochs = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute and print the test loss and accuracy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mconv_model\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(test_images, test_labels_bin)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvolutional model ten epochs -- Test loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvolutional model ten epochs -- Test accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_acc \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = conv_model.evaluate(test_images, test_labels_bin)\n",
    "print(\"Convolutional model ten epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Convolutional model ten epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a convolutional neural network with two convolutional layers\n",
    "# Decrease number of neurons and add dropout to reduce overfitting\n",
    "conv_model_reduce_overfit = Sequential()\n",
    "conv_model_reduce_overfit.add(Conv2D(64, (3, 3), input_shape = (28, 28, 1)))\n",
    "conv_model_reduce_overfit.add(Activation('relu'))\n",
    "conv_model_reduce_overfit.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "conv_model_reduce_overfit.add(Dropout(0.5))\n",
    "conv_model_reduce_overfit.add(Conv2D(64, (3, 3)))\n",
    "conv_model_reduce_overfit.add(SpatialDropout2D(0.5))\n",
    "conv_model_reduce_overfit.add(Activation('relu'))\n",
    "conv_model_reduce_overfit.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "conv_model_reduce_overfit.add(Flatten())\n",
    "conv_model_reduce_overfit.add(Dense(64))\n",
    "conv_model_reduce_overfit.add(Dropout(0.5))\n",
    "conv_model_reduce_overfit.add(Dense(10))\n",
    "conv_model_reduce_overfit.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model with adam optimizer and accuracy metric\n",
    "# Categorical cross-entropy is the loss function for one-hot encoded labels and\n",
    "# batch size equal to the number of neurons in the convolutional layers and 10 epochs\n",
    "# Add early stopping to avoid overfitting\n",
    "conv_model_reduce_overfit.compile(loss = \"categorical_crossentropy\", \n",
    "                   optimizer = 'adam', metrics = ['accuracy'])\n",
    "conv_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)\n",
    "conv_model_reduce_overfit.fit(train_images, train_labels_bin, validation_split = 0.3,\n",
    "               epochs = 10, verbose = 2, callbacks = [conv_callback], batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = conv_model_reduce_overfit.evaluate(test_images, test_labels_bin)\n",
    "print(\"Convolutional model ten epochs reduced overfit -- Test loss:\", test_loss * 100)\n",
    "print(\"Convolutional model ten epochs reduced overfit -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print predictions versus actual labels\n",
    "predictions = conv_model_reduce_overfit.predict(test_images)\n",
    "for i in range(10):\n",
    "  print(\"Prediction \" + str(i) + \": \" + str(np.argmax(np.round(predictions[i]))))\n",
    "  print(\"Actual \" + str(i) + \": \" + str(test_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
