{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKbUpIykTBel"
   },
   "source": [
    "# Vision Transformer Use-Case\n",
    "\n",
    "In diesem Jupyterlab-notebook wird gezeigt wie der Code zu dem Vision Transformer Paper [An Image is Worth 16x16 Words: Transformer for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf) erstellt und auf einen neuen Datensatz trainiert und angewandt wird. Der Datensatz hierfür ist der [WOD: Web Object Dataset](https://www.acin.tuwien.ac.at/vision-for-robotics/software-tools/autonomous-robot-indoor-dataset/). Dieses Jupyterlab ist eine überarbeitete Form von [GitHub Repository von mrdbourke](https://github.com/mrdbourke/pytorch-deep-learning), welcher ein frei zugängliches PyTorch Tutorial auf seiner [Homepage](https://www.learnpytorch.io/) bereitstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDqRJMUxS7ee"
   },
   "source": [
    "## 0. Imports\n",
    "\n",
    "Zuerst werden die nötigen Libraries für diesen Code importiert. Ein Teil davon: data_setup, engine, download_data, set_seeds, plot_loss_curve kommt von einem externen [GitHub Repository](https://github.com/mrdbourke/pytorch-deep-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.2)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0+e621604)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mtorch version: 2.1.2+cu121\n",
      "torchvision version: 0.16.2+cu121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] torchinfo konnte nicht gefunden werden ...installeieren über pip.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    print(\"[INFO] going_modular oder helper_functions konnten nicht gefunden werden ...herunterladen von GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !mv pytorch-deep-learning/helper_functions.py .\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ylqq1kO9Unpt",
    "outputId": "0272b7bc-3fba-4aa9-8b32-af23f33679e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup für geräteunabhänigen Code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-29 20:03:42--  https://cloud.technikum-wien.at/s/R24MNnYAy8cGqwQ/download\n",
      "Resolving cloud.technikum-wien.at (cloud.technikum-wien.at)... 172.31.251.28, 172.31.251.28\n",
      "Connecting to cloud.technikum-wien.at (cloud.technikum-wien.at)|172.31.251.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 527073659 (503M) [application/zip]\n",
      "Saving to: ‘web_object_dataset.zip’\n",
      "\n",
      "web_object_dataset. 100%[===================>] 502.66M  5.11MB/s    in 96s     \n",
      "\n",
      "2024-01-29 20:05:17 (5.26 MB/s) - ‘web_object_dataset.zip’ saved [527073659/527073659]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/data\"\n",
    "# Holen des Datensatzes von von GitHub\n",
    "\n",
    "!wget https://cloud.technikum-wien.at/s/R24MNnYAy8cGqwQ/download -LO web_object_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import zipfile \n",
    "\n",
    "# check ob .tar file wirklich ein .tar file ist\n",
    "print(zipfile.is_zipfile(\"web_object_dataset.zip\"))\n",
    "\n",
    "# öffnen der Datei\n",
    "#file = zipfile.open(\"web_object_dataset.zip\") \n",
    "#!unzip file.zip -d destination_folder\n",
    "\n",
    "with zipfile.ZipFile('web_object_dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# auspacken der Datei \n",
    "#file.extractall('./data') \n",
    "\n",
    "# Datei schließen  \n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './data/washington_web/washington_web.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# löschen der .tar Datei\n",
    "!rm -r web_object_dataset.zip\n",
    "!rm -r ./data/washington_web/washington_web.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erstellen eines Test und Train Aufteilung/Splits der Daten\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(\"./data/washington_web\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnötige python checkpoints löschen um Errors zu vermeiden\n",
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "# Pfade zu deinen Daten\n",
    "data_directory = \"./data/washington_web\"\n",
    "train_directory = \"./data/train\"\n",
    "test_directory = \"./data/test\"\n",
    "\n",
    "# Liste aller Klassen im Datensatz\n",
    "classes = os.listdir(data_directory)\n",
    "\n",
    "# Erstelle Train- und Testverzeichnisse\n",
    "os.makedirs(train_directory, exist_ok=True)\n",
    "os.makedirs(test_directory, exist_ok=True)\n",
    "\n",
    "train_images_count = 0\n",
    "test_images_count = 0\n",
    "\n",
    "# Gehe durch jede Klasse und teile die Daten auf\n",
    "for class_name in classes:\n",
    "    images_directory = os.path.join(data_directory, class_name)\n",
    "    images = os.listdir(images_directory)\n",
    "    \n",
    "    # Reduziere den Datensatz um 90% da ansonsten zu viel overhead für unsere bregernzte Hardware\n",
    "    reduced_images, _ = train_test_split(images, test_size=0.9, random_state=42)\n",
    "    \n",
    "    # Aufteilen der reduzierten Bilder in Train und Test\n",
    "    train_images, test_images = train_test_split(reduced_images, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Erstellen der Ordnerstruktur für Train\n",
    "    train_class_directory = os.path.join(train_directory, class_name)\n",
    "    os.makedirs(train_class_directory, exist_ok=True)\n",
    "    for image in train_images:\n",
    "        src = os.path.join(images_directory, image)\n",
    "        dest = os.path.join(train_class_directory, image)\n",
    "        shutil.copy(src, dest)\n",
    "        train_images_count = train_images_count + 1\n",
    "    \n",
    "    # Erstellen der Ordnerstruktur für Test\n",
    "    test_class_directory = os.path.join(test_directory, class_name)\n",
    "    os.makedirs(test_class_directory, exist_ok=True)\n",
    "    for image in test_images:\n",
    "        src = os.path.join(images_directory, image)\n",
    "        dest = os.path.join(test_class_directory, image)\n",
    "        shutil.copy(src, dest)\n",
    "        test_images_count = test_images_count + 1\n",
    "        \n",
    "print(f\"Anzahl der Trainingsbilder: {train_images_count}, Anzahl der Testbilder: {test_images_count}, Anzahl der Klassen: {len(classes)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StVb2gHVUn-4",
    "outputId": "efba43b5-646c-47f3-9f01-e6c7f3c2023c"
   },
   "outputs": [],
   "source": [
    "# Erzeugung von Datensätzen und Dataloader\n",
    "from torchvision import transforms\n",
    "from going_modular.going_modular import data_setup\n",
    "\n",
    "# Festlegen der Bild größe\n",
    "IMG_SIZE = 224  # entnommes aus Tabelle 3 vom VIT Paper\n",
    "\n",
    "# erzeugen der Tansform Pipeline\n",
    "manual_transforms = transforms.Compose([\n",
    "                                        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "                                        transforms.ToTensor()\n",
    "])\n",
    "\n",
    "print(f\"manuell erstellte transformation: {manual_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./data/train\"\n",
    "test_dir = \"./data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Trv9be5lUoBE",
    "outputId": "20a27908-ce4c-4e6a-ced6-1b49a3818122"
   },
   "outputs": [],
   "source": [
    "# Definition der Batch Size von 32  (Das Paper nutzt 4096 dies ist jedoch zu viel für unsere \"handelsübliche\" Hardware)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# erzeugung der Dataloader\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir = train_dir,\n",
    "    test_dir = test_dir,\n",
    "    transform = manual_transforms,\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "# checking\n",
    "len(train_dataloader), len(test_dataloader), class_names, len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4ndkRaPUoDe",
    "outputId": "7fe6ae19-afa9-4163-f839-722e35510b04"
   },
   "outputs": [],
   "source": [
    "## Visualisieren eines einzelnen Bildes\n",
    "\n",
    "# holen einer Batch voller images\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# holen eines einzlenen Bildes & Label vom Batch\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# anzeigen der Batch For\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "8W0eVCsoUoF8",
    "outputId": "88a48116-918a-4d4d-d622-93317e02150a"
   },
   "outputs": [],
   "source": [
    "# ploten des Bildes mit matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image.permute(1,2,0)) # (color_chan, height, widht) -> (height, width, color_chan)\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unKcM0lrfkM2"
   },
   "source": [
    "## Replizierung ViT: Übersicht\n",
    "\n",
    "\n",
    "\n",
    "Ein ganzes Research-Paper über maschinelles Lernen zu lesen, kann sehr abschreckend sein.\n",
    "Um es verständlicher zu gestalten, teilen wir es in kleinere Teile auf:\n",
    "\n",
    "* **Inputs** -  Was geht in das Modell rein? (in unserem Fall, Bild-Tensoren)\n",
    "* **Outputs** - Was kommt bei dem Modell heraus? (in unserem Fall wollen wir ein Modell, das Bildklassifizierungen ausgibt)\n",
    "* **Layers** - Nimmt eine Eingangsinformation auf und manipuliert sie mit einer Funktion (z.B. self-attention).\n",
    "* **Blocks** -Eine Sammlung von Schichten die Datentransformationen durchführen.\n",
    "* **Model or architecture** - Eine Sammlung von Blöcken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3V0K6R7g089"
   },
   "source": [
    "### ViT Übersicht: Teile des Originalpapers\n",
    "\n",
    "* Abbildung 1: Visuelle Übersicht der Modellarchitektur\n",
    "* Vier Gleichungen: mathematische Gleichungen, welche die Funktion von jeder Schicht und Block definieren\n",
    "* Tabelle 1/3: verschiedene Hyperparameter für die Arichtektur des Modells  bzw. für den Trainingsprozess\n",
    "* Textueller beschreibender Inhalt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZWSTNVqkDFC"
   },
   "source": [
    "#### Pseudo code, ersten Gleichung:\n",
    "```python\n",
    "x_input =  [class_token, image_patch_1, image_patch_2, ..., image_patch_N] + [class_token_pos, image_patch_1_pos, image_patch_2_pos, ..., image_patch_N_pos]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olNU7WTFv2mY"
   },
   "source": [
    "#### Pseudo code, zweite Gleichung:\n",
    "```python\n",
    "x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n",
    "```\n",
    "\n",
    "#### Pseudo code, dritte Gleichung:\n",
    "```python\n",
    "x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n",
    "```\n",
    "\n",
    "* MLP = multilayer perception = ein neuronales Netz mit Anzahl X Schichten\n",
    "* MLP = eine versteckte Schicht beim Training\n",
    "* MLP = ein einzelner lineare Schicht beim fine tunen\n",
    "\n",
    "#### Pseudo code, vierte Gleichung:\n",
    "\n",
    "```python\n",
    "y_output = Linear_layer(LN_layer(x_output_MLP_block))\n",
    "```\n",
    "\n",
    "* ViT-Base, ViT-Large und ViT-Huge sind alles die selben Modellarchitekturen mit verschiedenen Größen (Anzahl von Parametern)\n",
    "* Layers (Lx): Anzahl von transformer encoder Blöcken/Layers  \n",
    "* Hidden size $D$ - die Einbettungsgröße über die gesamten Modellarchitektur (wenn D = 768 -> jedes patch hat eine Größe von 768 => 16x16)\n",
    "* MLP size - die Anzahl der versteckten Einheiten in unserer MLP\n",
    "* Head - die Anzahl der multi-head self-attention Blöcken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4ZDW5NqA5wk"
   },
   "source": [
    "## Gleichung 1: Aufteilen der Daten in Patches und erzeugung des Klassen Tokens sowie der Patch Embeddings\n",
    "\n",
    "Schichten = Eingabe -> Funktion() -> Ausgabe\n",
    "\n",
    "Was ist unsere Eingabe Form?\n",
    "\n",
    "Was ist unsere Ausgabe Form?\n",
    "\n",
    "*Eingabe Form:  $H\\times{W}\\times{C}$ (Höhe x Breite x Farbkanäle)\n",
    "\n",
    "*Ausgabe Form: ${N \\times\\left(P^{2} \\cdot C\\right)}$\n",
    "\n",
    "* H = Höhe\n",
    "\n",
    "* W = Breite\n",
    "\n",
    "* C = Farbkanäle (RGB = 3, Schwarz-Weiß = 1)\n",
    "\n",
    "* P = Patch Größe\n",
    "\n",
    "* N = Anzahl der Patches = (Höhe * Breite) / P^2\n",
    "\n",
    "* D = konstante Größe des latenten Vektors = Einbettungs (embedding) Dimension (Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jv6OY8nDUoKX",
    "outputId": "2d9d1b05-54ca-41c3-eba7-8c8e02eab152"
   },
   "outputs": [],
   "source": [
    "# Erzeugung von Beispielswerten\n",
    "height = 224\n",
    "width = 224\n",
    "color_channels = 3\n",
    "patch_size = 16\n",
    "\n",
    "# Berechnung der Number of patches und Ausgabe\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "number_of_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32HlC02DgYPr",
    "outputId": "9f31d096-c1b0-4dac-e41c-6c3b318c3023"
   },
   "outputs": [],
   "source": [
    "# Eingabe Form\n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "\n",
    "# Ausgabe Form\n",
    "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
    "\n",
    "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
    "print(f\"Output shape (single 1D sequence of patches): {embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "CGizwbobGWIS",
    "outputId": "7f4bbb50-9fdc-408e-e9b9-36ba3e56f52b"
   },
   "outputs": [],
   "source": [
    "# Anzeigen eines einzelnen Bildes\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ze7MlbXUoM2",
    "outputId": "55ed29d8-59a7-44ac-e6d4-18ee20c93fa6"
   },
   "outputs": [],
   "source": [
    "# Bild als Tensor anzeigen\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "QsuuAOHRUoPE",
    "outputId": "8403c30a-350a-44c1-dfd0-d564add015e5"
   },
   "outputs": [],
   "source": [
    "# wir nehmen die oberste Reihe des Bildes\n",
    "image_permuted = image.permute(1,2,0)  # convert image to color channels last (H,W,C)\n",
    "\n",
    "# Anhängen einer Indexierung\n",
    "patch_size = 16\n",
    "plt.figure(figsize=(patch_size, patch_size))\n",
    "plt.imshow(image_permuted[:patch_size, : ,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "2Dr-bs19UoRe",
    "outputId": "68cadaa8-3526-4814-cc6a-5ecd2f5174e4"
   },
   "outputs": [],
   "source": [
    "# Code um die oberste Reihe als Patches darzustellen\n",
    "img_size =  224\n",
    "patch_size = 16\n",
    "num_patches = img_size/patch_size\n",
    "\n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\nPatch_size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# erzeugung einer Serie von Subplots\n",
    "fig, axs = plt.subplots(nrows = 1,\n",
    "                        ncols = img_size // patch_size,  # one column for each patch\n",
    "                        sharex = True,\n",
    "                        sharey = True,\n",
    "                        figsize=(patch_size, patch_size))\n",
    "\n",
    "# Iteration durch die Anzahl der Patches in der obersten Zeile\n",
    "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
    "  axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]);\n",
    "  axs[i].set_xlabel(i+1) # set the patch label\n",
    "  axs[i].set_xticks([])\n",
    "  axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UWbOEp3djznD",
    "outputId": "4f5bf80c-80e0-4a26-dd43-e533e334d4c9"
   },
   "outputs": [],
   "source": [
    "# Code um das ganze Bild als Patches anzuzeigen\n",
    "img_size =  224\n",
    "patch_size = 16\n",
    "num_patches = img_size/patch_size\n",
    "assert img_size % patch_size == 0, \"Image size must devisible by patch size\"\n",
    "print(f\"Number of patches per row {num_patches}\\\n",
    "      \\nNumber of patches per column: {num_patches}\\\n",
    "      \\nTotal patches: {num_patches*num_patches}\\\n",
    "      \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows = img_size // patch_size,\n",
    "                        ncols = img_size // patch_size,\n",
    "                        figsize = (num_patches, num_patches),\n",
    "                        sharex = True,\n",
    "                        sharey = True)\n",
    "\n",
    "# Iterieren durch die hähe und breite eines Bildes\n",
    "for i , patch_height in enumerate(range(0, img_size, patch_size)): # iterieren durch höhe\n",
    "  for j, patch_width in enumerate(range(0, img_size, patch_size)):\n",
    "    # plot permuted image on different axes\n",
    "    axs[i,j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterieren durch höhe\n",
    "                                   patch_width:patch_width+patch_size,   # iterieren durch breite\n",
    "                                   :])  # alle color channels\n",
    "    \n",
    "    # Einrichten von Beschriftungsinformationen für jede Teilfläche (Patch)\n",
    "    axs[i,j].set_ylabel(i+1,\n",
    "                        rotation=\"horizontal\",\n",
    "                        horizontalalignment=\"right\",\n",
    "                        verticalalignment=\"center\")\n",
    "    axs[i,j].set_xlabel(j+1)\n",
    "    axs[i,j].set_xticks([])\n",
    "    axs[i,j].set_yticks([])\n",
    "    axs[i,j].label_outer()\n",
    "\n",
    "# Titel für Plot\n",
    "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOKj1U_Kwik1",
    "outputId": "543d263b-84cb-4504-eb41-44d5cc59b8e2"
   },
   "outputs": [],
   "source": [
    "## Erzeugen von Bild Patches und transformieren in Einbettungen\n",
    "# wir nutzen hierfür torch.nn.Conv2d() und setzen die Kernel Größe sowie Stride auf die Patchgröße\n",
    "# Erzeugnung einer conv2d Schicht um das Bild in Patches von learnbaren feature-maps (Einbettungen) zu transferieren\n",
    "from torch import nn\n",
    "\n",
    "# Setzen der Patchgröße\n",
    "patch_size = 16\n",
    "\n",
    "# erzeugung einer conv2d schicht mit Hyperparameter vom ViT Paper\n",
    "conv2d = nn.Conv2d(in_channels = 3,     # für Farbbilder\n",
    "                    out_channels = 768,  # D size von Tablle 1 für ViT-Base\n",
    "                    kernel_size = patch_size,\n",
    "                    stride = patch_size,\n",
    "                    padding = 0)\n",
    "conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "iabj03VC41t3",
    "outputId": "7d5eef7c-d702-4652-e026-abc05fec41eb"
   },
   "outputs": [],
   "source": [
    "# Anzeigen eines einzelnen Bildes\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dom5fbi67VMW",
    "outputId": "c4422054-06bf-4003-8d05-3863c1e215a2"
   },
   "outputs": [],
   "source": [
    "# Das Bild durch die convolutional Schicht schicken  \n",
    "image_out_of_conv = conv2d(image.unsqueeze(0))  # hinzufügen einer batch Dimension -> (batch_size, color_channels, height, width)\n",
    "print(image_out_of_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "RdKnDPru7qZm",
    "outputId": "028bfc97-a2b9-4d1a-dccc-82bd868c3ee2"
   },
   "outputs": [],
   "source": [
    "# plotten von zufälligen convolutional feature-maps (embeddings)\n",
    "import random\n",
    "random_indexes = random.sample(range(0, 758), k = 5)\n",
    "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
    "\n",
    "# erzeugung eines Plots\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 5, figsize=(12,12))\n",
    "\n",
    "# plot zufällige feature maps eines Bildes\n",
    "for i, idx in enumerate(random_indexes):\n",
    "  image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the ouput tensor of the conv2d layer\n",
    "  axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy()) #remove batch dimension, remove from grad tracking / switch to numpy for matplotlib\n",
    "  axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axjZsttM8-AY",
    "outputId": "7c35b48a-9e9d-4a20-eedc-4f59d31465fb"
   },
   "outputs": [],
   "source": [
    "# anzeigen einer einzelnen feature-map in Tensorform\n",
    "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
    "single_feature_map, single_feature_map.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3F23l6tGCBg2",
    "outputId": "5bdc96f2-cc1c-43ff-ac0c-f34663c36c5d"
   },
   "outputs": [],
   "source": [
    "## Flatten der Patch embedding with torch.nn.Flatten()\n",
    "flatten_layer = nn.Flatten(start_dim=2,\n",
    "                           end_dim=3)\n",
    "flatten_layer(image_out_of_conv).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "leCAoFFXCBkC",
    "outputId": "29db69df-88ea-4850-c493-5194e315e298"
   },
   "outputs": [],
   "source": [
    "## Alles zusammenfügen\n",
    "# Bild anzeigen\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "# Transformieren eines Bildes in feature-maps\n",
    "image_out_if_conv = conv2d(image.unsqueeze(0)) #add bachdim\n",
    "print(f\"Image feature map (patches) shape: {image_out_of_conv.shape}\")\n",
    "\n",
    "# flatten der feature-maps\n",
    "image_out_of_conv_flattened = flatten_layer(image_out_of_conv)\n",
    "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9YE9t8tCBmX",
    "outputId": "8c27f2fa-0352-4de9-abc3-90bba7e5dcbd"
   },
   "outputs": [],
   "source": [
    "# Ausgabe der geflattenden Schicht neu anordnen\n",
    "image_out_of_conv_flattened_permuted = image_out_of_conv_flattened.permute(0,2,1)\n",
    "\n",
    "print(f\"{image_out_of_conv_flattened.permute(0,2,1).shape} -> (batch_size, number_of_patches, embedding_dims)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "bwi4rQIJCBoZ",
    "outputId": "95f7e392-cd8b-40b4-e4c6-4b49a8ef766c"
   },
   "outputs": [],
   "source": [
    "# Verwenden einer einzelnen gefalttende feature-map\n",
    "single_flattened_feature_map = image_out_of_conv_flattened_permuted[:,  :, 0]\n",
    "single_flattened_feature_map\n",
    "\n",
    "# anzeigen der geflattenden feature-map\n",
    "plt.figure(figsize=(22,22))\n",
    "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
    "plt.title(f\" Form der feature map: {single_flattened_feature_map.shape}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIuVOE12fQ-y"
   },
   "source": [
    "### Transferieren der ViT Patch Embeddings in ein PyTorch Modul.\n",
    "\n",
    "Wir wollen das dieses Modul folgende Dinge macht:\n",
    "1. Erstellen einer Klasse namens `PatchEmbedding`, die von `nn.Module` erbt.\n",
    "2. Initialisierung mit geeigneten Hyperparametern, wie Kanäle, Embeddingdimension, Patchgröße.\n",
    "3. Erstellen einer Schicht zur Umwandlung eines Bildes in Patch-Embeddings unter Verwendung von `nn.Conv2d()`.\n",
    "4. Erstellen einer Schicht, um die feature-maps der Ausgabe von Schicht in 3.) zu flatten.\n",
    "5. Definieren einer `forward()` Funktion, die das Vorwärtsberechnung definiert\n",
    "6. Vergewissern, dass die Ausgabeform der Schicht die erforderliche Ausgabeform der Patch-Embedding widerspiegelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuhRrox_CBsr"
   },
   "outputs": [],
   "source": [
    "# Erster Schritt: erzeugung der Klasse mit dem namen PatchEmbedding\n",
    "class PatchEmbedding (nn.Module):\n",
    "  # Zweiter Schritt: Initilaisierung der Schicht mit den geeigneten Hyperparametern\n",
    "  def __init__(self,\n",
    "               in_channels:int=3,\n",
    "               patch_size:int=16,\n",
    "               embedding_dim:int=768): # from Table 1 for ViT-Base\n",
    "    super().__init__()\n",
    "\n",
    "    self.patch_size = patch_size\n",
    "\n",
    "    # Dritter Schritt: erzugung einer Schicht um Bild in Patche-Embeddings zu transferieren\n",
    "    self.patcher = nn.Conv2d(in_channels = in_channels,\n",
    "                             out_channels = embedding_dim,\n",
    "                             kernel_size = patch_size,\n",
    "                             stride = patch_size,\n",
    "                             padding = 0)\n",
    "\n",
    "    # vierter Schritt: Erzeugung einer Schicht um die feateure-maps (Output von Conv2d) zu flatten\n",
    "    self.flatten = nn.Flatten(start_dim = 2,\n",
    "                              end_dim = 3)\n",
    "\n",
    "  # Fünfter Schritt: Vorwärtsmethode definieren, um die Schritte der Vorwärtsberechnung festzulegen\n",
    "  def forward(self, x):\n",
    "    # Assertion erstellen, um zu prüfen, ob die Eingaben die richtige Form haben\n",
    "    image_resolution = x.shape[-1]\n",
    "    assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {self.patch_size}\"\n",
    "\n",
    "    # Den Vorwärtspass ausführen\n",
    "    x_patched = self.patcher(x)\n",
    "    x_flattened = self.flatten(x_patched)\n",
    "    # Sechster Schritt: Sicherstellen, dass die zurückgegebenen Sequenzeinbettungsdimensionen in der richtigen Reihenfolge sind (batch_size, number_of_patches, embedding)\n",
    "    return x_flattened.permute(0,2,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRYzhjp_CBvC",
    "outputId": "4b3632d6-9ef0-4d27-9175-db45fd727c92"
   },
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "# Erzeugung einer Instanz von Patch-Einbettungsschicht\n",
    "patchify = PatchEmbedding(in_channels = 3,\n",
    "                          patch_size = 16,\n",
    "                          embedding_dim = 768)\n",
    "\n",
    "# ein einzelnes Bild durch die Patch-Einbettungsschicht schicken \n",
    "print(f\"Eingangsgröße des Bildes: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # extra Dimension hinzufügen\n",
    "print(f\"Ausgangsform der eingebetteten Patchsequenz: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gA64ITjCBxX",
    "outputId": "b7ad6477-a1d0-4842-b298-d6a868fb2af3"
   },
   "outputs": [],
   "source": [
    "# Anzeigen des Outputs als Tensor\n",
    "rand_image_tensor = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "patchify(rand_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmAtGINOrS5l"
   },
   "source": [
    "### Erzeugen der Klassen Embeddings\n",
    "\n",
    "Wir wollen: ein lernbaren Klassen Token an den Anfang der Patchembeddings anhängen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_r2PXZtTCBzj",
    "outputId": "2e8cd0b4-eb2a-47db-d272-d596bb5d854b"
   },
   "outputs": [],
   "source": [
    "# Form anzeigen\n",
    "patch_embedded_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BH2QxyIlCB1p",
    "outputId": "2553408a-5bf2-40fb-fccb-2741617272c8"
   },
   "outputs": [],
   "source": [
    "# Batchgröße und Embedding Dimension anzeigen\n",
    "batch_size  = patch_embedded_image.shape[0]\n",
    "embedding_dimension = patch_embedded_image.shape[-1]\n",
    "batch_size, embedding_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipJxy-dcCB4C",
    "outputId": "68814e00-f09f-4475-cc01-b26c996480c7"
   },
   "outputs": [],
   "source": [
    "# Erzeugen eines Klassen-Token-Embeddings als lernbarer Parameter, der die selbe Größe wie die Einbettungsdimension (D) hat. \n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad = True)\n",
    "class_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUaSVz88sdgz",
    "outputId": "5bd147c5-d180-4219-9e08-518c8fa3939c"
   },
   "outputs": [],
   "source": [
    "# Hinzufügen des Klassen Tokens an den Anfang des Patch Embeddings\n",
    "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n",
    "                                                       dim = 1) # number of patches dimensi\n",
    "\n",
    "print(patch_embedded_image_with_class_embedding)\n",
    "print(f\"Sequenz von Patch-Einbettungen mit vorangestelltem Klassen-Token Form: {patch_embedded_image_with_class_embedding.shape} -> (batch_size, class_tokens + number_of_patches, emedding_dims)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsPhuKj_wRcu"
   },
   "source": [
    "### Erzeugen des Positionstoken Embedding\n",
    "\n",
    "wir wollen: eine Serie von 1 Dimensionalen lernbaren Positions Embeddings erstellen und diese an die Sequenz von Patch Embeddings anhängen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLBoqElitVCj",
    "outputId": "4d34550d-83ee-4482-de79-4ab607f6ca58"
   },
   "outputs": [],
   "source": [
    "# anzeigen der Sequenz von Patch Embeddings mit dem angehängten Klassen Token\n",
    "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZKopQNQwsAj",
    "outputId": "ad0a9efa-9504-4d3e-ccde-2e8d5f7f2a9f"
   },
   "outputs": [],
   "source": [
    "# Berechnung von N (anzahl der patches)\n",
    "number_of_patches = int((height*width)/ patch_size**2)\n",
    "\n",
    "# embedding dimension abgreifen\n",
    "embedding_dimension = patch_embedded_image_with_class_embedding.shape[-1]\n",
    "\n",
    "# erzeugung des lernbaren 1 Dimensionalen Positions Embeddings\n",
    "position_embedding = nn.Parameter(torch.ones(1,\n",
    "                                             number_of_patches+1,\n",
    "                                             embedding_dimension),\n",
    "                                  requires_grad = True)\n",
    "position_embedding, position_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KED31Z7pxXOD",
    "outputId": "f7876f79-de4f-4c5a-97b7-314ea6932c13"
   },
   "outputs": [],
   "source": [
    "# Den Positions Embedding an das Patch und den Klassentoken hinzufügen\n",
    "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
    "patch_and_position_embedding\n",
    "\n",
    "print(f\"Patch und Positions Embedding Form: {patch_and_position_embedding}\")\n",
    "print(patch_and_position_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfTzkFnFa1DH"
   },
   "source": [
    "## Alles zusammenfügen: vom Bild bis zum Embedding (Gleichung 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXT6SLmsyGXT",
    "outputId": "60ee888e-83b4-4555-dced-e52c004a30f4"
   },
   "outputs": [],
   "source": [
    "# festlegen des seeds\n",
    "set_seeds()\n",
    "\n",
    "# 1. festlegen der patchgröße\n",
    "patch_size = 16\n",
    "\n",
    "# 2. ausgeben der Form des originalen Bildes und ermitteln der Bilddimensionen\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# 3. den Bildtensor abfragen und eine Batchdimension hinzufügen\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image shape: {x.shape}\")\n",
    "\n",
    "# 4. erzeugung eines Patch-Einbettungsschicht\n",
    "patch_embedding_layer = PatchEmbedding(in_channels = 3,\n",
    "                                       patch_size = patch_size,\n",
    "                                       embedding_dim = 768)\n",
    "\n",
    "# 5. Eingabe durch PatchEmbedding schicken\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patch embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "# 6. Erzeugen der Klassen Token Einbettung\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]       # -1 -> letzte Dimension\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad = True) # versichern dass es \"lernbar\" ist\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. anhängen des Klassen Tokens an die Patch Einbettung\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim = 1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "# 8. Erzeugung des Positional Embeddings/Tokens\n",
    "number_of_patches = int((height*width / patch_size**2))\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad = True)\n",
    "\n",
    "# 9. Hinzufügen der Positions Einbettung an das Patch embedding mit Klassen Token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV5AYKLj4M2z"
   },
   "source": [
    "## Gleichung 2: Multihead Self-Attention (MSA block)\n",
    "\n",
    "* Multihead self-attention: welcher Teil einer Sequenz die meiste Aufmerksamkeit auf sich ziehen sollte\n",
    " * In unserem Fall haben wir eine Reihe von eingebetteten Bildfeldern, die in signifikantem Zusammenhang mit einem anderen Feld stehen.\n",
    " * Wir wollen, dass unser neuronales Netz (ViT) diese Beziehung/Darstellung lernt.\n",
    "* Um MSA in PyTorch zu replizieren, können wir https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html verwenden.\n",
    "* LayerNorm = Die Ebenennormalisierung (LayerNorm) ist eine Technik zur Normalisierung der Verteilungen von Zwischenschichten. Sie ermöglicht glattere Gradienten, schnelleres Training und eine bessere Generalisierungsgenauigkeit.\n",
    " * Normalization = Sorgt dafür, dass alle Daten den gleichen Mittelwert und die gleiche Standardabweichung haben.\n",
    " * In pytorch https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html, normalisiert die Werte über $D$ dimension, in unserem Fall ist $D$ Dimension die Embedding Dimension.\n",
    "   * Wenn wir entlang der Embeddingdimension normalisieren, ist das so, als würden wir alle Stufen einer Treppe gleich groß machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YrhwbENdbng"
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "  \"\"\" Erzeugt einen multi-head self-attention block (\"MSA block\").\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               embedding_dim:int=768, # Versteckte Größe D (embedding dimenson) von Tabelle 1 von ViT-Base\n",
    "               num_heads:int=12, # Heads from Table 1 for ViT Base\n",
    "               attn_dropout:int=0):\n",
    "    super().__init__()\n",
    "\n",
    "    # erzeugen der nomralisierung Schicht (LN)\n",
    "    self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "\n",
    "    # erzeugen der multihead attention Schicht (MSA)\n",
    "    self.multihead_attn = nn.MultiheadAttention(embed_dim = embedding_dim,\n",
    "                                                num_heads = num_heads,\n",
    "                                                dropout = attn_dropout,\n",
    "                                                batch_first = True)  # (batch, seq, feature) -> (batch, number_of_patches, embedding_dimension)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer_norm(x)\n",
    "    attn_output, _ = self.multihead_attn(query=x,\n",
    "                                         key = x,\n",
    "                                         value = x,\n",
    "                                         need_weights = False)\n",
    "\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c51noSCSm15T",
    "outputId": "ee63c64e-ef58-4295-ee14-ee1b23651a47"
   },
   "outputs": [],
   "source": [
    "# erzeugung einer Instanzierung eines MSA Block's\n",
    "multihead_self_attention_block = MultiHeadSelfAttentionBlock(embedding_dim=768,\n",
    "                                                             num_heads=12,\n",
    "                                                             attn_dropout=0)\n",
    "\n",
    "# Die Patch und Positionsbild-Einbettungssequenz durch den MSA-Block leiten\n",
    "patched_image_trough_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
    "\n",
    "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
    "print(f\"Output shape of MSA block: {patched_image_trough_msa_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b8L8upUm12p",
    "outputId": "08a4f9b7-69eb-4e49-baf3-76e76026e8fa"
   },
   "outputs": [],
   "source": [
    " patch_and_position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4yZbY5Jm10J",
    "outputId": "6655e1f4-0a43-4312-c376-056d37906326"
   },
   "outputs": [],
   "source": [
    "patched_image_trough_msa_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjWFHGaIeWRJ"
   },
   "source": [
    "## Gleichung 3: Multilayer Percepton block (MLP block)\n",
    "\n",
    "* **MLP** = Der MLP BlocK enthält zwei Schichten mit GELU-Nichtlinearität\n",
    "  * MLP = ein oft verwendeter Begriff für ein Block mit einer Serie  von Schicht(en), hierbei kann es sich um mehrere oder auch nur eine einzige versteckte Schicht handeln.\n",
    "  * Schichten können bedeuten: fully-connected, dense, linear, feed-forward, alles ähnliche Namen für dieselbe Sache. In PyTorch, werden sie oft als `torch.nn.Linear()` und in TensorFlow als `tf.keras.Dense()` bezeichnet.\n",
    "  * Gelu in pytorch: https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\n",
    "  * MLP Anzahl der versteckten Einheiten = MLP Größe in Tabelle 1\n",
    "* **Dropout** = Dropout wird, wenn es verwendet wird, nach jeder dichten Schicht angewandt, außer bei den qkv-Projektionen und direkt nach dem Hinzufügen von Positions- zu Patch-Einbettungen.\n",
    " * Der Wert für den dropout wird aus Tabelle 3 des Papers entnommen.\n",
    "\n",
    "in pseudocode:\n",
    "\n",
    "```python\n",
    "# MLP\n",
    "x = linear -> non-linear -> dropout -> linear -> dropout\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06HTs8QOm1x7"
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "               embedding_dim:int=768,\n",
    "               mlp_size:int=3072,\n",
    "               dropout:int=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    # erzeugen des Normalisierungs Layers (LN)\n",
    "    self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "\n",
    "    # erzeugen des MLPs\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features = embedding_dim,\n",
    "                  out_features = mlp_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(in_features = mlp_size,\n",
    "                  out_features = embedding_dim),\n",
    "        nn.Dropout(p = dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.mlp(x)\n",
    "    return x\n",
    "    # return self.mlp(self.layer_norm(x)) // selbes wie oben in einer Zeile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_d4Orwxm1vc",
    "outputId": "9a9886f9-d04a-4eda-d259-bfb6f100b61c"
   },
   "outputs": [],
   "source": [
    "# Erzeugen einer Instanz eines MLP Blocks\n",
    "mlp_block = MLPBlock(embedding_dim = 768,\n",
    "                     mlp_size = 3072,\n",
    "                     dropout = 0.1)\n",
    "\n",
    "# die Ausgabe des MSABlocks durch MLPBlock schicken\n",
    "patched_image_trough_mlp_block = mlp_block(patched_image_trough_msa_block)\n",
    "print(f\"Input shape of MLP block: {patched_image_trough_msa_block.shape}\")\n",
    "print(f\"Output shape of MLP block{patched_image_trough_mlp_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHD_WBWPm1tA",
    "outputId": "1ca5f019-c256-4ea4-b0c1-03538deb9757"
   },
   "outputs": [],
   "source": [
    "patched_image_trough_msa_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDFBmsuwm1q-",
    "outputId": "5eaea594-d919-413f-84d7-3175167c97fd"
   },
   "outputs": [],
   "source": [
    "patched_image_trough_mlp_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5WTV2CYyhAB"
   },
   "source": [
    "## Erstellen des Transformer Encoder\n",
    "\n",
    "Der Transformer Encoder ist eine Kombination aus abwechselnden Blöcken von MSA (Gleichung 2) und MLP (Gleichung 3).\n",
    "\n",
    "Und zwischen jedem Block gibt es \"residual\" Verbindungen.\n",
    "\n",
    "* Encoder = eine Sequenz in eine lernbare Repräsentation umwandeln\n",
    "* Decoder = von der erlernten Darstellung zurück zu einer Sequenz gehen\n",
    "* Residual Verbindungen  = Hinzufügen von Eingaben einer oder mehrerer Schichten zu den darauffolgenden Ausgaben, was die Schaffung tieferer Netze ermöglicht (verhindert, dass die Gewichte zu klein werden)\n",
    "\n",
    "in pseudocode:\n",
    "```python\n",
    "# Transformer Encoder\n",
    "x_input -> MSA_block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + MSA_block_output + x_input] --> ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdIS1RnpN2s3"
   },
   "source": [
    "## Erzeugen eines Transformer Encoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGO4JMz2m1oA"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "               embedding_dim:int=768, # Hidden size D from table 1, 768 ViT-Base\n",
    "               num_heads:int=12, #from table 1\n",
    "               mlp_size:int=3072, #from table 1,\n",
    "               mlp_dropout:int=0.1, #from table 3,\n",
    "               attn_dropout:int=0):\n",
    "    super().__init__()\n",
    "\n",
    "    # create MSA block (Gleichung 2)\n",
    "    self.msa_block = MultiHeadSelfAttentionBlock(embedding_dim = embedding_dim,\n",
    "                                                 num_heads = num_heads,\n",
    "                                                 attn_dropout = attn_dropout)\n",
    "\n",
    "    # create MLP block (Gleichung 3)\n",
    "    self.mlp_block = MLPBlock(embedding_dim = embedding_dim,\n",
    "                              mlp_size = mlp_size,\n",
    "                              dropout = mlp_dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.msa_block(x) + x # residual/skip Verbindung für Gleichung 2\n",
    "    x = self.mlp_block(x) + x # residual/skip Verbindung für Gleichung 3\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYyst13Am1lw",
    "outputId": "c03e588b-5028-4371-a1b6-af77e0623cd2"
   },
   "outputs": [],
   "source": [
    "# Erzeugung einer Instanz TransformerEncoderBlock()\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "# Erzeugung einer Zusammenfassung mit tochinfo.summery\n",
    "summary(model = transformer_encoder_block,\n",
    "        input_size = (1, 197, 768),\n",
    "        col_names= [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width = 20,\n",
    "        row_settings=[\"var_names\"]) # (batch_size, num_of_patches, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS2d042UN-tP"
   },
   "source": [
    "## Erzeugen eines Transformer Encoder Schicht mit eingebauten Pytorch Schichten\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html#transformer-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbAu2SY9m1iu",
    "outputId": "1db77a52-fa55-4825-a31c-8a698fbd8efd"
   },
   "outputs": [],
   "source": [
    "# wir erzeugen das selbe wie oben mit torch.nn.TransformerEncoderLayer()\n",
    "\n",
    "# Gleichung 2 und 3 + Encoder\n",
    "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model = 768, # embedding size from table 1\n",
    "                                                             nhead = 12, # heads from table 1,\n",
    "                                                             dim_feedforward = 3072, # MLP size from table 1\n",
    "                                                             dropout = 0.1,\n",
    "                                                             activation = \"gelu\",\n",
    "                                                             batch_first = True,\n",
    "                                                             norm_first = True)\n",
    "\n",
    "torch_transformer_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnJGmTOYm1N4",
    "outputId": "39d8ed35-fd8d-4b92-a9d4-1e34a7383e0a"
   },
   "outputs": [],
   "source": [
    "# Erzeugen einer Zusammenfassung mit torchinfo.summery\n",
    "summary(model = torch_transformer_encoder_layer,\n",
    "        input_size = (1, 197, 768),\n",
    "        col_names= [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width = 20,\n",
    "        row_settings=[\"var_names\"]) # (batch_size, num_of_patches, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaGptrqjTvgZ"
   },
   "source": [
    "## Zusammenfügen um einen ViT zu erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihoU8Ydom0Vx"
   },
   "outputs": [],
   "source": [
    "# erzeugen einer ViT Klasse\n",
    "\n",
    "class ViT(nn.Module):\n",
    "  def __init__(self,\n",
    "               img_size:int=224, # Tabelle 3 vom VIT Paper\n",
    "               in_channels:int=3,\n",
    "               patch_size:int=16,\n",
    "               num_transformer_layers:int=12, # Tabelle 1 \"Layers\" für ViT-Base\n",
    "               embedding_dim:int=768, # Versteckte Größe D von Tabelle 1 für ViT-Base,\n",
    "               mlp_size:int = 3072, # Tabelle 1\n",
    "               num_heads:int = 12, #T Tabelle 1\n",
    "               attn_dropout:int = 0,\n",
    "               mlp_dropout:int = 0.1,\n",
    "               embedding_dropout:int = 0.1, # Dropout für Patch- und Positionseinbettungen\n",
    "               num_classes:int = 51): # Anzahl der Klassen in unserem Klassifizierungsproblem\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    # eine Assertion machen, dass die Bildgröße mit der Patchgröße kompatibel ist\n",
    "    assert img_size % patch_size == 0, f\"Die Bildgröße muss durch die Größe des Patches teilbar sein, Bildgröße: {img_size}, patch Größe: {patch_size}\"\n",
    "\n",
    "    # Berechung der Anzahl der Patches (height * widht/patch^2)\n",
    "    self.num_patches = (img_size * img_size) // patch_size ** 2\n",
    "\n",
    "    # eine lernfähige Klasseneinbettung erstellen\n",
    "    self.class_embedding = nn.Parameter(data = torch.randn(1, 1, embedding_dim),\n",
    "                                        requires_grad = True)\n",
    "\n",
    "    # lernfähige Positionseinbettung erstellen\n",
    "    self.position_embedding = nn.Parameter(data = torch.randn(1, self.num_patches+1, embedding_dim))\n",
    "\n",
    "    # Erzeugen eines Einbettung-dropout Werts\n",
    "    self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "    # Patch-Einbettungsschicht erstellen\n",
    "    self.patch_embedding = PatchEmbedding(in_channels = in_channels,\n",
    "                                          patch_size = patch_size,\n",
    "                                          embedding_dim = embedding_dim)\n",
    "\n",
    "    # erzeugen des transformer encoder Blocks\n",
    "    self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                       num_heads=num_heads,\n",
    "                                                                       mlp_size=mlp_size,\n",
    "                                                                       mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])   # erzeugen einer Liste von 12 transformer encoder blocks,\n",
    "                                                                                                                                          # Liste in eine eigene Reihe von Schichten umwandeln unter Verwendung des Stern *-Parameters\n",
    "\n",
    "    # erzeugen des Klassifikationsblocks\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "        nn.Linear(in_features = embedding_dim,\n",
    "                  out_features = num_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # einlesen der Batch Größe\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    # erzeugen einer Klassen Token Einbettung und erweitert sodass es auf die Batch Größe passt (Gleichung 1)\n",
    "    class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" bedeutet, dass die Dimensionen abgeleitet werden.\n",
    "\n",
    "    # erzeugen der Patch Einbettung (Gleichung 1)\n",
    "    x = self.patch_embedding(x)\n",
    "\n",
    "    # concat Klassen Token Einbettung und Patch Einbettung (Gleichung 1)\n",
    "    x = torch.cat((class_token, x), dim = 1) # (batch_size, num_patches, embedding dim) in der ersten Dimension\n",
    "\n",
    "    # hinzufügen der Positionseinbettung zu dem Klassen Token und der Patcheinbettung\n",
    "    x = self.position_embedding + x\n",
    "\n",
    "    # Dropout auf Patch-Einbettungen anwenden (direkt nach Hinzufügen von Poistional- zu Patch-Einbettungen)\n",
    "    x = self.embedding_dropout(x)\n",
    "\n",
    "    # Übergabe der Position und der Patch-Einbettung an den Transformator-Encoder (Gleichung 2 & 3)\n",
    "    x = self.transformer_encoder(x)\n",
    "\n",
    "    # Setzen des 0. Index logit durch den Classifier (Gleichung 4)\n",
    "    x = self.classifier(x[:, 0]) # jede Batch und der nullte Index dort\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTTwSQdRm1Gu",
    "outputId": "90312d9b-2ebf-4e2b-8541-e50942867be6"
   },
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "# Erstellen eines zufälligen Bild-Tensors mit derselben Form wie das Einzelbild\n",
    "random_image_tensor = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# eine Instanz von ViT mit der Anzahl der Klassen erstellen, mit denen wir arbeiten\n",
    "vit = ViT(num_classes = 51)\n",
    "\n",
    "# Übergeben Sie den zufälligen Bildtensor an unsere ViT-Instanz \n",
    "vit(random_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUNvC6bdi7cC"
   },
   "source": [
    "## Visueller Überblick über unser ViT-Modell \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyCD1WXJh5BS",
    "outputId": "ef7680eb-080f-4088-c551-401462eb66c7"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# mit tochinfo.summery eine Zusammenfassung erhalten \n",
    "summary(model = ViT(num_classes=len(class_names)),\n",
    "        input_size = (1, 3, 224, 224),\n",
    "        col_names= [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width = 20,\n",
    "        row_settings=[\"var_names\"]) # (batch_size, num_of_patches, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5H6o2LhnflQ"
   },
   "source": [
    "## Einrichten von Trainingscode für benutzerdefinierte ViT \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQyOgboYn3W5"
   },
   "source": [
    "### Definieren eines Optimierers (Optimizer)\n",
    "\n",
    "In dem Paper wird der Adam Optimizer (section 4, Training & finetuning) verwendet.\n",
    "Mit einem $B1$ Wert von 0.9, und $B2$ von 0.999 (defaults) sowie weight decay von 0.1\n",
    "\n",
    "weight decay = ist eine Form der Regularisierung, bei der große Gewichte im Netzwerk bestraft werden. Dies geschieht durch Hinzufügen eines Terms zur Verlustfunktion, der proportional zur Summe der quadrierten Gewichte ist. Dieser Term reduziert die Größe der Gewichte und verhindert, dass sie zu groß werden.\n",
    "\n",
    "regularization Technik = verhindert overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9ZIFTb0nxvF"
   },
   "source": [
    "### Definieren einer Verlustfunktion (Lossfunction)\n",
    "\n",
    "In dem ViT-Paper wird nicht erwähnt, welche Verlustfunktion verwendet wurde.\n",
    "\n",
    "Da es sich um eine Mehrklassen-Klassifikation handelt, verwenden wir die `troch.nn.CrossEntropyLoss()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2R1t9tDUqLMx"
   },
   "source": [
    "## Training des ViT Modell's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223,
     "referenced_widgets": [
      "9a8435209b9a4687a7248fe2776fe9f6",
      "9ba1767e085a4bd09e44a00819482e68",
      "853734f390cb4d7d8e1fd94f9205da5a",
      "9e9b6ea71e874df0b63d8ff316378d0f",
      "8290e33e19cd4074bfad7244a0e61aa3",
      "f2cc24fcf4ee4f1b91d4b918fdc0402b",
      "634af3f26cef4f6c91a1d37d47fc6fbc",
      "33816c40cb224b4b8375c67cd794c8d9",
      "da35aa46fa814cbabf7826eec03e496b",
      "f4ca1c0a7a1c443ab222ddc6b5fbf2f4",
      "34ac85631b5b4fcf83f7ca5fbffd66ff"
     ]
    },
    "id": "wluHxB26qJvi",
    "outputId": "cc191695-3c35-4a1f-ce2e-01f246472e9c"
   },
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "optimizer = torch.optim.Adam(vit.parameters(),\n",
    "                             lr=1e-3,\n",
    "                             betas=(0.9,0.999),\n",
    "                             weight_decay=0.1)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "results = engine.train(model = vit,\n",
    "                       train_dataloader = train_dataloader,\n",
    "                       test_dataloader = test_dataloader,\n",
    "                       epochs = 10,\n",
    "                       optimizer = optimizer,\n",
    "                       loss_fn = loss_fn,\n",
    "                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT3Ysu8tvujz"
   },
   "source": [
    "## Was unserem Trainingsaufbau fehlt\n",
    "\n",
    "Wie unterscheidet sich unser Ausbildungskonzept von dem des ViT-Papiers?\n",
    "\n",
    "Verhindern von underfitting:\n",
    "* Daten - unser setup benutzt viel weniger Daten ( 4506 vs mehrere Millionen)\n",
    "\n",
    "Verhindern von overfitting:\n",
    "* Lernrate warmup - mit einer niedrigen Lernrate beginnen und auf eine Basis-LR erhöhen\n",
    "* Lernrate decay - Wenn sich Ihr Modell der Konvergenz nähert, beginnen Sie, die Lernrate zu verringern.\n",
    "* Gradienten clipping - verhindern dass die Gradienten zu groß werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21Bm6zvxwN-e"
   },
   "source": [
    "## Darstellung der Verlustkurven für unser Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "mbz87siNqRQP",
    "outputId": "5ab26c0f-9a57-47bb-9c3c-6471b6342f8a"
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6yZxHbz1GY5"
   },
   "source": [
    "## Verwendung eines vortrainierten ViT aus `torchvision.models`\n",
    "\n",
    "Wenn Sie beim Deep Learning ein vortrainiertes Modell aus einem großen Datensatz für Ihr eigenes Problem verwenden können, ist das oft ein guter Ausgangspunkt.\n",
    "\n",
    "Dies erzielt oft große Ergebnisse mit wenigen Daten\n",
    "\n",
    "**Warum ein vortrainiertes Modell verwenden?**\n",
    "\n",
    "* oft sind die Daten begrenzt\n",
    "* begrenzte Trainingsressourcen (Hardware)\n",
    "* schneller bessere Ergebnisse erzielen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnXmxUj0wSzB",
    "outputId": "b6b9107e-45db-4f64-b2d1-772ea132f0aa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NulyM9jF6C_O",
    "outputId": "0d446c24-b170-4a4a-8ef9-71c3e90127cf"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "gBjzkhmE60wO",
    "outputId": "5a8e767d-1252-489c-fb68-ab060ac72fcd"
   },
   "outputs": [],
   "source": [
    "# vortrainierte Gewichte für ViT-Base laden\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # DEFAULT = bestes verfügbares\n",
    "\n",
    "# eine ViT-Modellinstanz mit vortrainierten Gewichten aufzusetzen\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights = pretrained_vit_weights).to(device)\n",
    "\n",
    "# Einfrieren der Basisparameter\n",
    "for parameter in pretrained_vit.parameters():\n",
    "  parameter.requires_grad = False\n",
    "\n",
    "# den Klassifizierungskopf aktualisieren\n",
    "set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features = 768, out_features=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "ftuIyr3w7YOt",
    "outputId": "f113b295-1897-4bd8-a960-75e96087df97"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# eine Zusammenfassung mit tochinfo.summery erstellen\n",
    "summary(model = pretrained_vit,\n",
    "        input_size = (1, 3, 224, 224),\n",
    "        col_names= [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width = 20,\n",
    "        row_settings=[\"var_names\"]) # (batch_size, num_of_patches, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten für das vortrainierte ViT Modell vorbereite \n",
    "\n",
    "Wenn Sie ein vorab trainiertes Modell verwenden, sollten Sie sicherstellen, dass Ihre Daten auf dieselbe Weise formatiert sind, auf die das Modell trainiert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRUSo-5l79tQ"
   },
   "outputs": [],
   "source": [
    "# übernehmen der \"Transforms\" von vortarinierten ViT Gewichten\n",
    "\n",
    "vit_transforms = pretrained_vit_weights.transforms()\n",
    "vit_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufstezen des dataloaders \n",
    "from going_modular.going_modular import data_setup\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir = train_dir,\n",
    "                                                                                                     test_dir = test_dir,\n",
    "                                                                                                     transform = vit_transforms,\n",
    "                                                                                                     batch_size = 32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training eines feature extractor ViT Modells\n",
    "\n",
    "Wir nehmen nun das vortrainierte Modell und finetunen es auf unseren Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# erstellen eines Optimierers und eine Verlustsfunktion\n",
    "optimizer = torch.optim.Adam(params = pretrained_vit.parameters(),\n",
    "                             lr = 1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Trainieren des Klassifikationskopf des vortrainierten ViT\n",
    "set_seeds()\n",
    "pretrained_vit_results = engine.train(model = pretrained_vit,\n",
    "                                      train_dataloader = train_dataloader_pretrained,\n",
    "                                      test_dataloader = test_dataloader_pretrained,\n",
    "                                      optimizer = optimizer,\n",
    "                                      loss_fn = loss_fn,\n",
    "                                      epochs = 10,\n",
    "                                      device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darstellung der Verlustkurven des vortrainierten ViT-Feature-Extraktor-Modells\n",
    "\n",
    "Wie wir sehen ist das Modell durch die zuhilfenahme eines vortrainierten VIT-Modells wesentlich zuverlässiger. Dies lässt sich dadurch begründen dass das vortrainierte Modell schon gelernt hat wie gewisse Merkmale in einem Bild aussehen. Wir verwenden dieses vortrainerte Wissen und wenden es auf unseren neuen Datensatz an um eine neue Klassifikationsaufgabe zu lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "plot_loss_curves(pretrained_vit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speichern des besten Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import utils\n",
    "\n",
    "utils.save_model(model = pretrained_vit,\n",
    "                target_dir = \"models\", \n",
    "                model_name = \"pytorch_vit_transfer_learning_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# auslesen der Modellgröße in Bytes und dann in Megabytes konvertieren\n",
    "pretrained_vit_model_size = Path(\"models/pytorch_vit_transfer_learning_model.pth\").stat().st_size // (1024*1024)\n",
    "print(f\"vortranierte ViT extractor Modellgröße: {pretrained_vit_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorhersage auf benutzerdefiniertem Bild treffen\n",
    "\n",
    "Wir können nun unser Modell verwenden und versuchen beliebige Bilder zu klassifizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Importieren der Funktionen um Vorhersagen auf ungesehene Bilder zu treffen und diese zu plotten\n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Aufsetzen des benutzerdefinierten Bildpfads \n",
    "custom_image_path = \"./data/keyboard.jpg\"\n",
    "\n",
    "# Laden Sie das Bild herunter, wenn es noch nicht existiert.\n",
    "with open(custom_image_path, \"wb\") as f:\n",
    "        # Beim Herunterladen von GitHub muss der \"raw\"-Dateilink verwendet werden.\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/AlessandroScherl/LRP/main/keyboard.jpg\")\n",
    "        \n",
    "        print(f\"Downloading {custom_image_path}...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "# Vorhersage auf benutzerdefiniertem Bild\n",
    "pred_and_plot_image(model=pretrained_vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "33816c40cb224b4b8375c67cd794c8d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34ac85631b5b4fcf83f7ca5fbffd66ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "634af3f26cef4f6c91a1d37d47fc6fbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8290e33e19cd4074bfad7244a0e61aa3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "853734f390cb4d7d8e1fd94f9205da5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33816c40cb224b4b8375c67cd794c8d9",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da35aa46fa814cbabf7826eec03e496b",
      "value": 10
     }
    },
    "9a8435209b9a4687a7248fe2776fe9f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ba1767e085a4bd09e44a00819482e68",
       "IPY_MODEL_853734f390cb4d7d8e1fd94f9205da5a",
       "IPY_MODEL_9e9b6ea71e874df0b63d8ff316378d0f"
      ],
      "layout": "IPY_MODEL_8290e33e19cd4074bfad7244a0e61aa3"
     }
    },
    "9ba1767e085a4bd09e44a00819482e68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2cc24fcf4ee4f1b91d4b918fdc0402b",
      "placeholder": "​",
      "style": "IPY_MODEL_634af3f26cef4f6c91a1d37d47fc6fbc",
      "value": "100%"
     }
    },
    "9e9b6ea71e874df0b63d8ff316378d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4ca1c0a7a1c443ab222ddc6b5fbf2f4",
      "placeholder": "​",
      "style": "IPY_MODEL_34ac85631b5b4fcf83f7ca5fbffd66ff",
      "value": " 10/10 [01:42&lt;00:00, 10.14s/it]"
     }
    },
    "da35aa46fa814cbabf7826eec03e496b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2cc24fcf4ee4f1b91d4b918fdc0402b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4ca1c0a7a1c443ab222ddc6b5fbf2f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
