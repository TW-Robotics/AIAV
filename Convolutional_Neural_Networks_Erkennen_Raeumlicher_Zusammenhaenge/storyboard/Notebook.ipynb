{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fbd1823-85a6-413c-8bdb-72c2acae745d",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Erkennen räumlicher Zusammenhänge\n",
    "\n",
    "In den Use Cases [*Logistic Regression: Der einfachste Weg Kleidungsstücke zu klassifizieren*](https://www.aiav.technikum-wien.at/post/logistic-regression-der-einfachste-weg-kleidungsst%C3%BCcke-zu-klassifizieren) und [*Von Nachbarn und deren Merkmalen zur Klassifizierung von Buchstaben*](https://www.aiav.technikum-wien.at/post/von-nachbarn-und-deren-merkmalen-zur-klassifizierung-von-buchstaben) haben wir uns bereits damit beschäftigt, wie Modelle die Fähigkeit des \"Sehens\" lernen können. Der methodische Unterschied zwischen den genannten Use Cases ist dabei die Modellart, welche das Sehen lernt: beim ersten Use Case wurde auf [logistische Regression](https://www.aiav.technikum-wien.at/post/logistic-regression-der-einfachste-weg-kleidungsst%C3%BCcke-zu-klassifizieren) zurückgegriffen, während beim anderen das [k-Nearest Neighbour](https://www.aiav.technikum-wien.at/post/von-nachbarn-und-deren-merkmalen-zur-klassifizierung-von-buchstaben) Verfahren eingesetzt wurde. Wenn Sie sich die genannten Use Cases noch einmal ansehen, werden Sie feststellen, dass das Vorgehen zwischen den beiden tatsächlich relativ ähnlich ist. Die Modellart ist zwar unterschiedlich, aber die Vorverarbeitungsschritte sind gleich: In beiden Fällen werden zunächst erklärbare Faktoren in den Bildern mittels Hauptkomponentenanalyse (PCA, siehe AIAV Video [Principal Component Analysis](https://youtu.be/IVg4Frzp69o)) extrahiert bzw. erlernt, auf deren Basis dann das jeweilige Modell die Bilder klassifiziert. \n",
    "    \n",
    "Bei der Evaluierung der Leistung beider Konfigurationen fällt uns allerdings auf, dass die Genauigkeit der Klassifizierung noch Potential für Verbesserungen hat. Den Grund dafür finden wir in einem wichtigen Merkmal von PCA, da diese keine räumlichen Zusammenhänge, wie sie oft in Bildern vorkommen, abbilden kann. Das schränkt den PCA-basierten Ansatz ein, da in Bilder typischerweise räumliche Zusammenhänge beinhalten und diese mit PCA in der Datenverarbeitungspipeline eventuell verloren gehen. Um dieser Problematik entgegenzuwirken, wollen wir nun neuronale Netze (siehe AIAV Video [Klassische Neuronale Netze](https://youtu.be/pmfIJ3XUw2c)) einsetzen, um ohne vorhergehende Merkmalsextrahierung durch PCA Bilder klassifizieren zu können. Im Speziellen beschäftigen wir uns mit Convolutional Neural Networks (CNN, siehe AIAV Video [CNN Classifier](https://youtu.be/cdVDMm5Wscc)).\n",
    "\n",
    "\n",
    "### Klassische Neuronale Netze\n",
    "Bevor wir die Erweiterung zu CNNs vornehmen, sehen wir uns einmal klassische neuronale Netze an (mehr Informationen zu neuronalen Netzen finden Sie [hier](https://link.springer.com/book/10.1007/978-3-658-26763-6) in den Kapiteln 20 und 21). Ein simples Neuronales Netzwerk besteht aus Neuronen (auch Perzeptron genannt). Die Neuronen sind gewichtet verknüpft und können Signale verarbeiten bzw. an dahinterliegende Schichten weitergeben. Jedes Neuron hat dabei eine simple Aufgabe: es gewichtet die Eingangssignale mit einem einstellbaren Faktor, summiert den Zahlenwert auf und gibt diesen weiter. Abbildung 1 zeigt ein Neuron mit zwei Eingängen.  \n",
    "\n",
    "\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "<center><img src=\"images/images-Abbildung1Perceptron.drawio.png\" alt=\"Konzept Perceptron\" style=\"width: 750px;\"/></center>\n",
    "\n",
    "<i>Abbildung 1: Vereinfachte Darstellung eines Perzeptrons. Das Neuron (blauer Kreis) hat in diesem Fall zwei Eingänge, welche von den Inputneuronen x und y kommen (orange Kreise). Das Neuron summiert die beiden Eingänge und gibt diese weiter.</i>\n",
    "    </div>\n",
    "\n",
    "\n",
    "Besonders intelligent oder komplex ist dieses Modell in Abbildung 1 nicht, der Aufbau erinnert uns sogar an lineare oder logistische Regression. Im Gegensatz zu diesen Modellen ist ein Perzeptron aber nur ein kleiner Bestandteil eines neuronalen Netzes. Neuronale Netze sind dadurch so mächtig, da sie diese kleinen (und simplen) Neuronen zu komplexen Architekturen verschachteln, indem parallele Neuronen Schichten formen, welche wiederum hintereinander angeordnet werden. Schichten, welche aus dieser Art von Neuron bestehen nennt man dabei auch voll vernetzte Schichten. \n",
    "\n",
    "Abbildung 2 zeigt ein einfaches neuronales Netzwerk mit zwei versteckten Schichten, sowie einer Ein- und einer Ausgangsschicht. Durch die Größen von Ein- und Ausgangsschicht legt die Architektur auch bereits das Format der eingelesenen und ausgegebenen Daten fest. Für das Netzwerk in Abbildung 2 werden zwei Zahlen aus einem drei-dimensionalen Eingangssignal abgeleitet.\n",
    "\n",
    "Ein beispielhafter Use Case für das Neuronale Netz in Abbildung 2 ist die Klassifizierung von zwei Objekten (z.B. Schrauben oder Mutter) basierend auf drei Abstandssensoren. Die drei Sensorwerte dienen dann als Eingangsvektor und der Ausgang des Netzwerks spiegelt die die Wahrscheinlichkeit eine Schraube oder eine Mutter zu erkennen wider. Natürlich funktioniert das ganze erst nach dem Training des Netzwerkes mit einem entsprechenden für unseren Use Case passenden Datensatz.\n",
    "\n",
    "\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "<center><img src=\"images/images-Abbildung2NeuronalesNetz.drawio.png\" alt=\"Konzept Neuronales Netz\" style=\"width: 750px;\"/></center>\n",
    "\n",
    "<i>Abbildung 2: Wir organisieren mehrere Neuronen in Schichten, welche wiederum hintereinander angeordnet werden, um ein neuronales Netz zu formen. Die Größen der Ein- und Ausgangsschichten bestimmen dabei das Format der ein- und ausgehenden Daten des Netzwerks.</i>\n",
    "    </div>\n",
    "\n",
    "\n",
    "Theoretisch könnten wir nun dieses oder ein ähnliches Netzwerk nutzen, um (z.B. durch PCA) extrahierte Merkmale zu klassifizieren. Gegenüber der Use Cases zu k-Nearest Neighbour und logistischen Regression wäre das aber keine große Änderung, es würde also wieder die gleiche Problematik mit nicht wahrnehmbaren räumlichen Zusammenhängen auftreten. Alternativ könnten wir auch direkt die Bildpunkte als Eingang des Neuronalen Netzes verwenden und das neuronale Netz damit gleichzeitig Merkmalsextrahierung und Klassifizierung lernen lassen. Dies führt aber zu neuen Problemen, da bereits bei einem 100 mal 100 Pixel großem Farbbild 30.000 (100 mal 100 Pixel mit je drei Kanälen) Neuronen in der ersten Schicht vorhanden sein müssen. Durch diese hohe Anzahl an Neuronen resultiert das direkte Einlesen von Bildern zwischen Eingangs- und den folgenden Schichten bereits bei einem relativ kleinen Eingangsbild in einer so großen Anzahl an Gewichtungen, dass diese während des Trainingsvorgangs nur mithilfe eines unrealistisch großen Datensatzes gesetzt werden müssten.\n",
    "\n",
    "### Vom Neuronalen Netz zu CNNs mittels Faltung\n",
    "Die deutsche Übersetzung von CNN lautet Faltungsnetz oder faltendes neuronales Netzwerk. Die Faltung ist eine mathematische Operation, welche typischerweise in der Bildverarbeitung eingesetzt wird, um räumliche Zusammenhänge zu verarbeiten. Der Trick der CNNs ist es, diese mathematische Operation vorne in ein neuronales Netzwerk zu integrieren. Dabei ergänzt man ein klassisches neuronales Netz mit sogenannten Faltungsschichten, welche Matrix-Rechenoperationen auf Bildausschnitten vornimmt. Durch diese Erweiterung können beispielsweise Kanten, ein klassischer räumlicher Zusammenhang, in Bildern erkannt werden. \n",
    "\n",
    "Um nun einerseits die Anzahl an Parameter zu verringern und gleichzeitig die oben beschriebenen räumlichen Zusammenhänge in das Modell zu integrieren, implementieren wir nun am Eingang des Netzwerkes eine oder mehrere Faltungsschichten (mehr Details zu faltenden neuronalen Netzen finden Sie im AIAV Video [CNN Classifier](https://youtu.be/cdVDMm5Wscc)). Die Faltungsschicht können Sie sich wie ein kleines Fenster vorstellen, welches über jeden Bereich im Bild geschoben wird. Das Fenster filtert dann die Inhalte des aktuellen Bildausschnittes basierend auf seinen internen Parametern. Der Trick dabei ist nun, dass die Parameter des Filters nicht festgesetzt sind, sondern im Zuge des Trainings des CNN gesetzt werden. Das Netz lernt also Merkmale aus dem Bild zu filtern, welche für den Lernprozess notwendig sind. Über einen oder mehrere Filter werden so im CNN zunächst Merkmale extrahiert, bevor klassische voll vernetzte Schichten die Merkmale weiterverarbeiten (und z.B. die Klassifizierung übernehmen). Dieses Vorgehen erlaubt es uns, die Anzahl der Parameter im Netz zu verringern, da die Architektur der voll vernetzten Schichten simpel gehalten werden kann. Nach Abschluss des Trainingsprozesses werden die Parameter der Filter so wie die restlichen Parameter des Netzes festgesetzt, um neue (und unbekannte) Bilder zu verarbeiten. Abbildung 3 zeigt ein einfaches CNN mit zwei Faltungsschichten und zwei voll vernetzten Schichten.\n",
    "\n",
    "\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "<center><img src=\"images/images-Abbildung3CNN.drawio.png\" alt=\"Konzept CNN\" style=\"width: 750px;\"/></center>\n",
    "\n",
    "<i>Abbildung 3: Vereinfachte Darstellung des gesamten CNNs ((1) Eingangsbild, (2) Faltung des Bildes mit einem Filter, (3) Resultierende neue Matrix, (4) Umwandlung der Merkmale in einen Eingang für das neuronale Netzwerk, (5) Ausgang des Netzwerkes (z.B.: Klassifizierung)).</i>\n",
    "    </div>\n",
    "\n",
    "\n",
    "Für den praktischen Einsatz von CNNs verwendet man in der Regel offen zugängliche Architekturen, welche mittels bestehender Datensätze vorab trainiert wurden. Man erweitert diese vorgefertigten und potentiell vortrainierten Netzwerke für die eigene Anwendung. Für die Implementierung dieses Use Cases werden wir uns an den [Beispielimplementierungen](https://www.tensorflow.org/tutorials/images/classification) von [Tensorflow](https://www.tensorflow.org/), einer offenen Bibliothek für die Implementierung von Anwendungen im Bereich des maschinellen Lernens.\n",
    "\n",
    "### Praktische Implementierung\n",
    "Nachdem wir nun die Theorie zu CNNs kennen gelernt haben, wollen wir uns mit einem praktischen Beispiel auseinandersetzen. Unser Beispiel befasst sich mit dem Erkennen von Gebärdensprache. Ziel ist es eine Kamera so einzusetzen, dass unser CNN erkennen kann, welche Ziffer gezeigt wird. Abbildung 4 zeigt den gewünschten Ablauf des Use Cases. Eine Kamera nimmt die zu erkennende Geste auf und das Bild wird unserem trainierten CNN übergeben. Dieses klassifiziert das Kamerabild und sagt voraus, welche Zahl höchstwahrscheinlich gezeigt wird. Um den Use Case einzuschränken, befassen wir uns hier ausschließlich mit Ziffern in Amerikanischer Zeichensprache.\n",
    "\n",
    "\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "<center><img src=\"images/images-Abbildung4Implementierung.drawio.png\" alt=\"Use Case Funktion\" style=\"width: 750px;\"/></center>\n",
    "\n",
    "<i>Abbildung 4: Das Eingangsbild zeigt eine Zahl in Zeichensprache, welches das trainierte CNN verarbeiten muss. Das CNN liefert eine Voraussage, um welche Zahl es sich handelt.</i>\n",
    "    </div>\n",
    "\n",
    "\n",
    "Zum Training unserer Gestenerkennung verwenden wir [diesen](https://github.com/ardamavi/Sign-Language-Digits-Dataset) frei zugänglichen Datensatz. Er besteht aus insgesamt 2062 Bildern, wobei es zu jeder Klasse/Ziffer von 0 bis 9 rund 205 Bilder gibt. In einem Vorverarbeitungsschritt teilen wir die annotierten Bilder zufällig in einen Test und einen Trainingsdatensatz, sodass 80% der Bilder (1650 Bilder) für das Training und 20% (412 Bilder) für das Testen herangezogen werden können. Dieses Vorgehen erlaubt es uns, das fertig trainierte Modell zu testen und sicher zu stellen, dass es nicht nur \"auswendig lernt\".\n",
    "\n",
    "Obwohl unser Datensatz über 2000 aufbereitete und sortierte Bilder enthält, reicht diese Menge an Bildern nicht aus, um unser CNN vollständig zu trainieren. Deshalb wenden wir eine Methode namens [Image Augmentation](https://towardsdatascience.com/image-augmentations-in-tensorflow-62967f59239d) an, mit der man Datensätze erweitern kann. Dazu überlegen wir uns zunächst, ob es für unsere Anwendung wichtig ist, ob die rechte oder linke Hand auf dem Trainingsbild die Ziffer zeigt. In unserem Fall ist das egal, also spiegeln wir die Bilder einfach vertikal und verdoppeln damit die Größe unseres Datensatzes. Weiters können wir die Bilder ein wenig drehen, sie vergrößern oder verkleinern und den Kontrast ändern. Trotz dieser kleinen Veränderungen hoffen wir, dass die für das Modell relevanten Informationen erhalten bleiben, sodass wir das CNN auf Basis eines wesentlich größeren Datensatzes trainieren können.\n",
    "\n",
    "Als nächsten Schritt legen wir die Architektur unseres CNNs fest. Diese wird durch folgende Faktoren beeinflusst: \n",
    "\n",
    "- **Eingangsformat:** Unsere Bilder haben eine Auflösung von 100 mal 100 Pixeln und liegen im Farbformat mit drei Kanälen vor. Deshalb ist das Eingangsformat 100x100x3.\n",
    "- **Faltungen:** Eine Faltung liefert höchstwahrscheinlich nicht genug Merkmale für unseren Use Case. Deshalb haben wir uns für drei Faltungen entschieden.\n",
    "- **Ausgangsformat:** Wir wollen zwischen zehn Ziffern unterscheiden. Deshalb benötigen wir einen Ausgangsvektor der Größe zehn.\n",
    "\n",
    "Die Architektur wird mittels Tensorflow definiert und instanziiert. Dabei verwenden wir ein sequentielles Modell; das heißt, dass der Ausgang der vorherigen Schicht jeweils den Eingang der nächsten Schicht darstellt. Im Anschluss finden Sie die von Tensorflow ausgegebene Zusammenfassung des Modells:\n",
    "\n",
    "```text\n",
    "_________________________________________________________________\n",
    " Layer (type)                  Output Shape               Param #   \n",
    "=================================================================\n",
    " sequential (Sequential)      (None, 100, 100, 3)        0         \n",
    "                                                                 \n",
    " rescaling (Rescaling)        (None, 100, 100, 3)        0         \n",
    "                                                                 \n",
    " conv2d (Conv2D)              (None, 100, 100, 16)       448       \n",
    "                                                                 \n",
    " MaxPooling2D                 (None, 50, 50, 16)         0                                                                        \n",
    "                                                                 \n",
    " conv2d_1 (Conv2D)            (None, 50, 50, 32)         4640      \n",
    "                                                                 \n",
    " MaxPooling 2D                (None, 25, 25, 32)         0                                                 \n",
    "                                                                 \n",
    " dropout (Dropout)            (None, 25, 25, 32)         0         \n",
    "                                                                 \n",
    " conv2d_2 (Conv2D)            (None, 25, 25, 64)         18496     \n",
    "                                                                 \n",
    " MaxPooling2D                 (None, 12, 12, 64)         0                         \n",
    "                                                                 \n",
    " dropout_1 (Dropout)          (None, 12, 12, 64)         0         \n",
    "                                                                 \n",
    " flatten (Flatten)            (None, 9216)               0         \n",
    "                                                                \n",
    " dense (Dense)                (None, 128)                1179776   \n",
    "                                                                 \n",
    " dense_1 (Dense)              (None, 10)                 1290      \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 1,204,650\n",
    "Trainable params: 1,204,650\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "<i>Abbildung 5: Nach der Instanziierung des Modells gibt die verwendete Tensorflow Bibliothek eine Zusammenfassung der im Modell enthaltenen Schichten aus.</i>\n",
    "    </div><br>\n",
    "    \n",
    "Jede der hier gezeigten Schichten erfüllt eine bestimmte Aufgabe. Als erstes wird das Bild normalisiert und die Pixelwerte zwischen 0 und 1 skaliert (Rescaling). Anschließend werden Faltungs- und Pooling-Schichten definiert; diese lernen eine Faltung und konzentrieren sich auf die am stärksten vorkommenden Merkmale (conv2d und MaxPooling2D Paare). Zusätzlich dazu, werden sogenannte Dropout-Schichten eingefügt, die bestimmte Eingänge deaktivieren. Die Deaktivierung der Eingänge hilft dem Modell zu generalisieren und sogenanntes Overfitting, also reines auswendig lernen, zu vermeiden. Abschließend führen wir die extrahierten Merkmale zu einem einfachen zweidimensionalen Vektor über (Flatten) und treffen die Voraussage mithilfe von zwei voll vernetzten Schichten (Dense). Wie vorhin bereits angesprochen, kann man an den Parameteranzahl jeder Schicht in der dritten Spalte auch sehen, dass die Faltungsschichten wesentlich weniger Parameter lernen müssen als die voll vernetzten Schichten.\n",
    "\n",
    "Nach der Instanziierung des Modells trainieren wir dieses basierend auf unserem vorverarbeiteten und erweiterten Trainingsdatensatz. Dabei evaluieren wir das Modell während des Trainingsvorgangs stets mittels der Genauigkeit (Accuracy) und einer Loss-Funktion, welche darstellt, wie sehr aktuelle Voraussagen des Modells (Ist-Ausgang) vom Soll-Ausgang, also unseren Trainingsdaten, abweichen. Die Genauigkeit soll gegen 1.0 beziehungsweise 100% gehen, während der Loss gegen 0 gehen soll. Beide Kenngrößen werden nach jeder Trainingsepoche für den Trainingsdatensatz sowie unseres nicht zum Training verwendeten Validierungsdatensatzes ermittelt.\n",
    "\n",
    "\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "<center><img src=\"images/Abbildung5TrainingLossAccuracy.png\" alt=\"Trainingsmetriken\" style=\"width: 450px;\"/></center>\n",
    "\n",
    "<i>Abbildung 6: Genauigkeit und Loss werden über die 50 Trainingsepochen dargestellt, um den Fortschritt des Modells zu visualisieren. Beide Kenngrößen werden einerseits anhand des Trainingsdatensatzes und andererseits anhand des Validierungsdatensatzes ermittelt. Je höher die Genauigkeit und je niedriger der Loss sind, desto wahrscheinlicher ist es, dass wir ein gutes Ergebnis beim Anwenden des CNN bekommen.</i>\n",
    "    </div>\n",
    "\n",
    "\n",
    "Abbildung 7 zeigt Voraussagen unseres fertig trainierten CNNs von Bildern aus dem Validierungsdatensatz (Bilder, die das CNN noch nicht kennt). Dabei inkludieren wir drei richtige (obere Zeile) sowie drei falsche Klassifizierungen (untere Zeile). Neben der Voraussage wird auch die Sicherheit der Voraussage angegeben. In der oberen Zeile werden dem Modell die Zahlen 2, 7 und 9 gezeigt. Diese werden auch mit 61,16%, 67,10% und 94,94% Sicherheit klassifiziert. In der unteren Zeile werden eine 2 und eine 7 jeweils der Klasse 1 zugeordnet. Im letzten Bild wird eine 9 der Klasse 7 zugeordnet.  \n",
    "\n",
    "\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "<center><img src=\"images/Abbildung6Voraussagen.png\" alt=\"Testbilder\" style=\"width: 550px;\"/></center>\n",
    "\n",
    "<i>Abbildung 7: Experimente mit dem trainierten Model: 50 Testbilder werden dem trainierten Model gezeigt. Die obere Zeile zeigt richtig zugeordnete Gesten (links nach rechts: 2, 7, 9 richtig klassifiziert). Die untere Zeile zeigt falsche Voraussagen (links nach rechts) Klasse 1 statt Klasse 2, Klasse 1 statt Klasse 7, Klasse 7 statt Klasse 9. Insgesamt 72% der Beispielbilder werden richtig klassifiziert, während für 28% eine falsche Voraussage getroffen wird.</i>\n",
    "    </div>\n",
    "\n",
    "\n",
    "### Fazit\n",
    "Nach 50 Trainingsepochen und einem augmentierten Datensatz erreicht das Modell eine Genauigkeit von 93,82%. Die Frage, ob diese Genauigkeit zufriedenstellend ist, beantworten wir mittels eines Beispiels. Stellen wir uns vor, es wird eine Telefonnummer mittels Zeichensprache angesagt. Telefonnummern in Österreich/Deutschland haben meist 13 Ziffern. Übersetzt nun unser CNN mit einer Genauigkeit von 93.82% diese Telefonnummer in eine ASCII Darstellung, sind wahrscheinlich nur 12 der 13 Zahlen richtig. Gleich wie bei anderen Use Cases muss vor dem praktischen Einsatz gegeben der Rahmenbedingungen eines Use Cases von der Person, die das Modell implementiert, entschieden werden, ob die Genauigkeit des Modells ausreicht.\n",
    "\n",
    "\n",
    "Der Einsatz eines CNN erlaubt es uns aber, räumliche Zusammenhänge in Bildern wahrzunehmen; das war bei bisherigen Bildverarbeitungs-Use Cases nicht der Fall. Wie Sie bereits auf unserer Plattform gelesen haben, ist die Auswahl passender Modelle für einen Use Case meistens komplex und theoretisch [ohne Experimente nicht machbar](https://ieeexplore.ieee.org/document/585893). Dennoch werden aktuell meistens CNNs für die Verarbeitung von Bildern verwendet. Abschließend wollen wir Ihnen noch einige Empfehlungen zur Arbeit mit CNNs geben: Sind Sie an der Mathematik und einer ausführlichen Analyse interessiert, empfehlen wir an dieser Stelle immer die Nutzung mehrerer Modelle. Sollten Sie mit AI erst beginnen, empfehlen wir aufgrund der einfachen Handhabung mit CNNs zu beginnen.\n",
    "\n",
    "### Weiterführende AIAV Inhalte\n",
    "Lesen Sie sich auch unseren [Beitrag zum klugen Hans](https://www.aiav.technikum-wien.at/post/der-kluge-hans-und-die-betr%C3%BCgende-ai) aufmerksam durch. Dieser arbeitet die Problematik, die aus der Wahrnehmung räumlicher Zusammenhänge einhergeht, praxisorientiert auf.\n",
    "\n",
    "Wenn neuronale Netze Ihr Interesse geweckt haben, dann verweisen wir Sie gerne auf unseren Use Case Wanderung in Richtung des geringsten Fehlers: Wie Programme Lernen (Link Coming Soon), welcher von Grund auf die Funktionsweise von neuronalen Netzen aufarbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9b09e-ad58-4456-a16b-efb7f668b91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
