{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7275b1f-28e9-4386-816d-6213a1673bb6",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour für Bildsegmentierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e7312-5976-4375-9873-ad63878c234d",
   "metadata": {},
   "source": [
    "## Storyboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5f634-6e8c-4026-9348-5fb074bc9a0d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "    In den Micro Usecases TODO (Moritz CNN) und TODO (Moritz SVM) haben wir uns bereits mit Methoden der Klassifizierung beschäftigt (siehe AIAV Video <a href=\"https://www.youtube.com/watch?v=5dJyt0m5PJw&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=31\">Classifier vs. Detector</a>). Die dort gezeigten Modelle erkennen ob sich ein Hammer im Sichtfeld einer Kamera befindet. Dafür wurden die Modelle zunächst anhand eines beschrifteten Datensatzes (siehe Abbildung 1), bestehend aus Bildern von Hämmern und Bildern ohne Hämmer, trainiert. Da der Trainingsdatensatz beschriftet ist, weist jedes Bild darin eine Beschriftung auf. Diese gibt an, ob sich im jeweiligen Bild ein Hammer befindet oder nicht. Das ist die sogenannte Klasse, welche wir später erkennen wollen. Nach Abschluss der Trainingsphase werden neue, unbekannte Bilder in das trainierte Modell gegeben, welches entscheidet ob im Bild ein Hammer zu sehen ist, oder nicht.\n",
    "    <br /><br />\n",
    "    \n",
    "    Das Modell ist nach dem Training in der Lage, Bilder mit Hämmern von Bildern ohne Hämmer zu unterscheiden. Das heißt also, dass diese Modelle in der Trainingsphase  eine Grenze zwischen den Klassen in den Trainingsdaten finden. In unseren Beispielen ist diese Grenze zwischen Bildern, welche einen Hammer enthalten und Bildern ohne Hammer. Aber wie sehen solche Grenzen aus und wie werden sie gefunden?\n",
    "    </div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d39ea1-2d7e-4027-88a0-5751679f100d",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung1Trainingsdaten.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "_Abbildung 1: Dieser beilspielhafte Trainingsdatensatz besitzt mehrere Einträge auf einer Ebene, welche einer von zwei Klassen (blaue Quadrate oder goldene Raute) angehören. Wir stellen uns die Frage, wie man in diesem Datensatz eine Grenze zwischen den Klassen finden kann, um neue, unbekannte Punkte einer der Klassen zuzuordnen._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb98d28-3c6c-4b15-932d-40e69a31098e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">    \n",
    "    Ein Verfahren, welches dieses Problem sehr einfach und zuverlässig löst, ist <a href=\"https://cs.nyu.edu/~roweis/papers/ncanips.pdf\">k-Nearest Neighbour</a> (kNN). kNNs ermitteln die Grenzen zwischen Klassen geometrisch; die Grenze wird anhand des Abstandes zwischen benachbarten Dateneinträgen gefunden. Dafür werden beschriftete Trainingsdaten zunächst gespeichert. Soll nun ein Punkt klassifiziert werden, werden alle bekannten Datenpunkte in einem festlegbaren Radius k rund um den Punkt gezählt. Diese gezählten Einträge sind dann dei Grundlage für eine Mehrheitsentscheidung - die Klasse mit den am öftesten innerhalb des Radius vorkommenden Datenpunkten gewinnt. So Entsteht eine klare Trennung zwischen den verschiedenen Klassen basierend auf beschrifteten Trainingsdaten. Das k, also der maximale Abstand in dem Nachbarn berücksichtigt werden, ist dabei bei den meisten Implementierungen wählbar und beeinflusst das Verhalten der Klassifizierung stark.\n",
    "    <br /><br />\n",
    "    \n",
    "    Abbildung 2 zeigt das Konzept hinter kNNs für den Datensatz in Abbildung 1 mit zwei verschiedenen k Werten. Dabei versuchen wir den zu klassifizierenden Eintrag (grüner Punkt) einer der beiden Klassen zuzuordnen. Die verschiedenen Werte für k stellen dabei den Radius um den Eintrag dar, in dem benachbarte Datenpunkte gezählt werden. Je größer das k ist, desto mehr Nachbarn werden also gezählt. Dabei sieht man die Auswirkung, welche die Wahl von k auf den Ausgang der Klassifizierung hat. Beim kleinen k wird der Eintrag Klasse 2 zugeordnet, während große Werte für k in einer Zuordnung zu Klasse 1 resultieren.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae690c2-6270-432b-905d-7c3c7496f284",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung2Klassifizierung.png\" alt=\"Drawing\" style=\"width: 750px;\"/>\n",
    "\n",
    "_Abbildung 2: Der k-Nearest Neighbour Algorithmus erlaubt es uns neue Einträge Klassen zuzuordnen indem alle bekannten Datenpunkte in einem Bestimmten Radius gezählt werden. Die Klasse, die am öftesten innerhalb des Radius vorkommt, gewinnt. Dabei stellt k den Radius, in dem gezählt wird, dar. Für verschiedene Werte von k kann die Klassifizierung verschiedene Werte liefern._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fb861-01d2-4f81-a67e-913c7cb5a793",
   "metadata": {},
   "source": [
    "### Praktische Implementierung\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "    Für die Praktische Implementierung dieses Usecases wollen wir Vorder- und Hintergrund eines Bildes trennen. Dies passiert mithilfe des kNN Algorithmus, welcher es uns wie oben beschrieben erlaubt die Grenze zwischen mehreren Klassen zu finden. Das nutzen wir, indem wir Vorder- und Hintergrund je als eigene Klasse definieren. Die Idee ist es, dann mittels kNN im Bild die Grenzen zwischen den beiden Klassen zu ermitteln, um die verschiedenen Bildbereiche dem Vorder- oder Hintergrund zuzuordnen.\n",
    "    <br /><br />\n",
    "    \n",
    "    Diese Implementierung benötigt ein Bild vom Hintergrund, welches zunächst aufgenommen und gespeichert wird. Dieses Bild wird mit den zu analysierenden Bildern verglichen; die Unterschiede zwischen den Bildern sind dabei der Vordergrund und die Gemeinsamkeiten der Hintergrund. Verglichen wird dabei mittels des <a href=\"https://scikit-image.org/docs/dev/auto_examples/transform/plot_ssim.html\">Structural Similarity Indexes</a>, welcher für jeden Bildpunkt einen Wert zurückgibt. Dieser Wert gibt an, wie sehr sich die beiden Bilder, also das Bild vom Hintergrund und das zu analysierende Bild, an der jeweiligen Position ähneln. Das Resultat ist ein \"Wärmebild\" welches angibt, wie sicher es ist, dass die Bilder unterschiedlich sind. Dieses Wärmebild wird anschließend auf eine niedrigere Auflösung skaliert, um den erforderlichen Rechenaufwand gering zu halten. Dannach setzen wir alle Werte unter einem Schwellenwert auf 0 und alle Werte über dem Schwellenwert auf 1.\n",
    "     <br /><br />\n",
    "     \n",
    "    Grundsätzlich sind mit diesen Verarbeitungsschritten bereits alle Bildpunkte entweder dem Vorder- oder Hintergrund zugeordnet, da die resultierende Karte angibt, welcher Klasse der jeweilige Bildpunkt angehört. Einträge mit dem Wert 1 gehören zum Hintergrund, während Einträge mit dem Wert 0 zum Vordergrund gehören. Dabei weist die Karte aber oft Artefakte und unschöne Kanten um das zu erkennende Objekt auf. Diese entstehen durch sich änderende Lichtverhältnisse sowie die Verarbeitung des Bildes direkt in der Kamera. Damit wir nun schöne Grenzen zwischen Vorder- und Hintergrund erhalten und Artefakte beseitigen, verwenden wir kNN Klassifizierung. Dafür wird ein kNN Klassifikator aus der <a href=\"https://scikit-learn.org/stable/modules/neighbors.html\">Scikit Learn Bibliothek</a> anhand der Karte trainiert. Wie oben beschrieben, werden dann die Grenzen zwischen den beiden Klassen anhand der Nachbarn jedes Eintrags berechnet. Je höher die Anzahl der Nachbarn, desto gleichmäßiger sind die berechneten Grenzen. Bei zu hoher Anzahl an Nachbarn kann es aber vorkommen, dass Details im Umriss um ein zu erkennendes Objekt weggeglättet werden. Zusätzlich dazu steigt mit wachsender Anzahl der Nachbarn auch der benötigte Rechenaufwand erheblich.\n",
    "    <br /><br />\n",
    "    \n",
    "    Die Maske aus den von der kNN Klasse ermittelten Grenzen wird wieder auf die originale Größe der eingehenden Bilder hochskaliert und auf das eingehende Bild angewendet. Dabei werden nur als Vordergrund gekennzeichnete Teile des Bildes übernommen und als Hintergrund gekennzeichnete Teile verworfen. Damit ergibt sich ein Bild, welches nur den Vordergrund enthält. Der gesamte Ablauf der Bildverarbeitungspipeline ist in Abbildung 3 mit Visualisierungen von jedem Verarbeitungsschritt dargestellt.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913155f-a710-4050-a6bf-1d4dd9ad62d2",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung3Pipeline.png\" alt=\"Drawing\" style=\"width: 1200px;\"/>\n",
    "\n",
    "_Abbildung 3: Um Vorder- und Hintergrund zu trennen wird das zu analysierende Bild zunächst mit einem Bild des Hintergrunds verglichen. Der aus dem Vergleich resultierende Structural Similarity Index (SSIM) wird anhand eines Schwellenwertes quantisiert. Mittels kNN werden dann Artefakte im SSIM entfernt, welche durch sich ändernde Lichtverhältnisse und Bildverarbeitung in der Kamera verursacht wurden._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc90db-ab49-4e41-8567-b9a5227d3e87",
   "metadata": {},
   "source": [
    "### Fazit\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "    k-Nearest Neighor erlaubt es uns Klassifizierung anhand eines relativ kleinen Datensatzes zu betreiben, um die Grenzen zwischen Klassen zu ermitteln. Dadurch dass das Konzept hinter kNN relativ einfach ist, ist nur ein geringer Rechenaufwad für Training und Klassifizierung notwendig. Ein weiterer Vorteil von kNN ist der niedrige Notwendige Implementierungsaufwand, da diese Art von Klassifikation bereits in Form von gängigen Bibliotheken, wie <a href=\"https://scikit-learn.org/stable/\">Scikit Learn</a> oder nativ in der <a href=\"https://www.rdocumentation.org/packages/class/versions/7.3-19/topics/knn\">Programmiersprache R</a> verfürbar ist.\n",
    "    </div>\n",
    "    \n",
    "<P style=\"page-break-before: always\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13628c-7e14-4821-a947-6e10208ed4a2",
   "metadata": {},
   "source": [
    "## Codedokumentation\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "    Die Implementierung erfolgt in <a href=\"https://www.python.org/downloads/\">Python 3</a> mithilfe einiger Bibliotheken. Die <a href=\"https://opencv.org/\">OpenCV</a> Implementierung für Python (<a href=\"https://pypi.org/project/opencv-python/\">opencv-python</a>), <a href=\"https://pillow.readthedocs.io/en/stable/\">Pillow</a> und <a href=\"https://scikit-image.org/\">Skimage</a> stellen umfangreiche Methoden zur Bildverarbeitung zur Verfügung. Sie wurden für das Einlesen und Vorverarbeiten der Bilder verwendet. <a href=\"https://numpy.org/\">Numpy</a> wurde für generelles Datenmanagement verwendet, während zur Klassifizierung die kNN Implementierung aus der <a href=\"https://scikit-learn.org/stable/\">Scikit Learn Bibliothek</a> zur Verwendung kommt.\n",
    "    <br /><br />\n",
    "    \n",
    "    Durch die Verwendung von OpenCV zum Einlesen des Eingangsbildes, ist es möglich sowohl einzelne Bilder, Videos oder direkt die Aufnahme einer Webcam zu verarbeiten. In der Dokumentation wurden einfachheitshalber nur zwei abgespeicherte Bilder miteinander verglichen.\n",
    "    <br /><br />\n",
    "    \n",
    "    Zunächst werden alle benötigten Bibliotheken importiert.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828fa35-72f5-44f5-8a00-7f5e3b29e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.measure import block_reduce\n",
    "from sklearn import neighbors\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2e836-a563-4ae4-a5c0-40e6fe4a3942",
   "metadata": {},
   "source": [
    "Dannach definieren wir Hilfsfunktionen für die k-Nearest Neighbour (kNN) Klassifizierung. _formatInput_ formatiert ein übergebenes Bild als Liste von Einträgen für Übergabe an den Klassifikator. _formatImage_ formatiert die Einträge wieder als Bild, welches mittels Pillow oder OpenCV dargestellt werden kann. _applykNN_ führt kNN Klassifikation am übergebenen Graustufenbild aus, um Artefakte aus dem quantisierten Structural Similarity Indexes zu beseitigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d62308-6aaa-40ef-8197-64e0b14f7a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatInput(frame):\n",
    "        \"\"\" Formatiert frame als Liste von Einträgen für einfachere Klassifizierung \"\"\"\n",
    "        # zurückgegebenes Format: [[position_x, position_y, pixelwert_an_xy], [], [], ...]\n",
    "        ret = []\n",
    "        for x in range(len(frame)):\n",
    "                for y in range(len(frame[0])):\n",
    "                        ret.append([x, y, frame[x][y]])\n",
    "        return ret\n",
    "\n",
    "def formatImage(inp):\n",
    "        \"\"\" Formatiert die Liste von Einträgen nach der Klassifizierung wieder als Bild \"\"\"\n",
    "        # zurückgegebenes Format: 2-dimensionales Array, compatibel mit Pillow & OpenCV\n",
    "        xRes = max([entry[0] for entry in inp])+1\n",
    "        yRes = max([entry[1] for entry in inp])+1\n",
    "        ret = []\n",
    "        for x in range(xRes):\n",
    "                ret.append([0 for y in range(yRes)])\n",
    "        for entry in inp:\n",
    "                ret[entry[0]][entry[1]]=entry[2]\n",
    "        return np.array(ret)\n",
    "\n",
    "def applykNN(frame, K=15):\n",
    "        \"\"\" Wendet kNN Klassifizierung auf ein Schwarz/Weiß Bild an, um Artefakte zu entfernen \"\"\"\n",
    "        # Formatierung des quantisierten SSIM Bildes\n",
    "        frame = formatInput(frame)\n",
    "        # Generierung der Trainingsdaten \n",
    "        # bestehend aus Positionen der Pixel und ihrer Beschriftungen\n",
    "        X = np.array([[entry[0], entry[1]] for entry in frame])\n",
    "        y = np.array([entry[2] for entry in frame])\n",
    "        # Definition und Training des Klassifikators\n",
    "        clf = neighbors.KNeighborsClassifier(K, weights='uniform')\n",
    "        clf.fit(X, y)\n",
    "        # Alle Pixel im Bild werden vom Klassifikator klassifiziert\n",
    "        Z = clf.predict(X)\n",
    "        # Formatierung und Rückabe des Bildes \n",
    "        ret = [ [X[i][0], X[i][1], Z[i]] for i in range(len(X)) ]\n",
    "        return formatImage(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd896e-ae3e-431e-b481-a024d50e70e6",
   "metadata": {},
   "source": [
    "Referenz- und Testbilder werden eingelesen, in Graustufenbilder umgewandelt und für eine schnellere Berechnung auf eine niedrigere Auflösung Skaliert. Dannach wird der Structural Similarity Index beider Bilder berechnet und noch einmal auf eine niedrigere Auflösung skaliert. Das resultierende Bild wird anhand des Schwellenwertes von 0,5 quantisiert und mittels der _applykNN_ Funktion klassifiziert. Dabei erhalten wir eine Maske, welche Vorder- und Hintergrund trennt. Die Maske wird auf die Auflösung der Eingangsbilder hochskaliert und angewendet. Dabei setzen wir alle Pixel, welche als Hintergrund klassifiziert wurden, weg. Abschließend speichern wir das Bild ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b353df-553a-4989-8876-50f8556b8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen und Vorbereiten von Referenzbild und zu untersuchendem Bild\n",
    "\n",
    "def scaleImage(img, scaleFactor=2):\n",
    "        \"\"\" Skaliert das Bild um scaleFactor herunter \"\"\"\n",
    "        width = int(len(img[0])/scaleFactor)\n",
    "        height = int(len(img)/scaleFactor)\n",
    "        frame = cv2.resize(img,(width, height), fx=0, fy=0, interpolation=cv2.INTER_LINEAR)\n",
    "        return img\n",
    "\n",
    "# Festelgen von Referenzbild und zu untersuchendem Bild\n",
    "img1Path = './testImages/bild1.jpg'\n",
    "img2Path = './testImages/bild2.jpg'\n",
    "\n",
    "img1 = cv2.imread(img1Path)\n",
    "img2 = cv2.imread(img2Path)\n",
    "\n",
    "# Skalierung der Bilder\n",
    "img1 = scaleImage(img1)\n",
    "img2 = scaleImage(img2)\n",
    "\n",
    "# Konvertierung der Bilder zu Graustufen\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Ermittlung des Unterschieds zwischen Vorder- und Hintergrund\n",
    "ssimRet, ssimImage = ssim(img1, img2, data_range=img2.max() - img2.min(), full=True)\n",
    "\n",
    "# Herunterskalierung des Bildes zur Anwendung von kNN\n",
    "dim = (4,4)\n",
    "ssimImage = block_reduce(ssimImage, block_size=dim, func=np.mean)\n",
    "\n",
    "# Jeder Pixel wird als Vorder- oder Hintergrund beschriftet\n",
    "# Vordergrund => 0, Hintergrund => 255\n",
    "threshhold = 0.5\n",
    "ssimImage[ssimImage>threshhold] = 255\n",
    "ssimImage[ssimImage<=threshhold] = 0\n",
    "\n",
    "# Training und Klassifizierung mittels kNN\n",
    "# So werden die Grenzen zwischen Vorder- und Hintergrund gefunden\n",
    "tmp = applykNN(ssimImage, K=30)\n",
    "\n",
    "# Hochskalierung der Maske auf die originale Auflösung\n",
    "width = len(img2[0])\n",
    "height =len(img2) \n",
    "tmp = cv2.resize(tmp,(width, height), fx=0, fy=0, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# Hintergrund wird anhand der Maske aus dem originalen Bild entfernt\n",
    "result = np.array(img2)\n",
    "result[tmp==255] = 0\n",
    "\n",
    "Image.fromarray(result).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
