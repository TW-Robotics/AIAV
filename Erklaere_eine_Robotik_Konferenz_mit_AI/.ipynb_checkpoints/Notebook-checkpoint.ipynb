{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed69764-1645-45a4-ba7f-02148aba8822",
   "metadata": {},
   "source": [
    "# Erkläre eine Robotik Konferenz... mit AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd2fe1-4521-413b-bd91-0cd5446f1502",
   "metadata": {},
   "source": [
    "## Storyboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda74c94-5b51-4ee1-a460-0676623cbca2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "    Für uns ist es immens wichtig, über die aktuellen Tätigkeiten anderer Forschungseinrichtungen informiert zu sein. In Österreich gibt es eine Plattform wo Forschungseinrichtungen die Ergebnisse ihrer Arbeit im Bereich Robotik und AI präsentieren können - diese Plattform ist der <a href=\"https://roboticsworkshop.at\">Austrian Robotics Workshop</a>. Dort werden jedes Jahr zwischen 30 und 50 Paper vorgestellt, welche auf zwei bis sechs Seiten die wissenschaftlichen Ergebnisse präsentieren. Wir wollen diese Publikationen verwenden, um einen Überblick über den Stand der Wissenschaft in der österreichischen Szene zu gewinnen. Aber wie kann man mit so viel Text umgehen?\n",
    "    <br /><br />\n",
    "    \n",
    "    Um große Mengen an Text effizient zu erfassen, arbeiten wir mit Natural Language Processing und Regular Expressions (siehe die AIAV Videos zu <a href=\"https://www.youtube.com/watch?v=B-weR5ngYIU&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=24\">Sprachverarbeitung</a>, <a href=\"https://www.youtube.com/watch?v=XCu1srONTMc&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=16\n",
    "\">Textverarbeitung 1</a> und <a href=\"https://www.youtube.com/watch?v=wvRb1dWjzyQ&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=21\">Textverarbeitung 2</a>). Natural Language Processing (NLP) ist eine Mischung aus künstlicher Intelligenz und Linguistik. Das Ziel von NLP ist es Statistik zu verwenden, um große Mengen an Text zu analysieren [1]. Probleme die dabei von NLP addresiert werden sind vor allem maschinelle Übersetzung, Einholen von Informationen, Textkategorisierung und Datenextahierung aus Text [2].\n",
    "    <br /><br />\n",
    "    \n",
    "    Aber wie wendet man NLP in der Praxis an? Neben kompletten Toolkits, wie z.B. dem <a href=\"https://www.nltk.org/\">Python Natural Langugae Toolkit</a>, können sogennante <a href=\"https://docs.python.org/3/howto/regex.html\">Regular Expressions (RegEx)</a> verwendet werden. Regular Expressions sind in mehreren Programmiersprachen verfügbar - hier wurden sie innerhalb von Python mittels des <a href=\"https://docs.python.org/3/library/re.html\">re Moduls</a> verwendet. RegEx können als eigene kleine Programmiersprache gesehen werden. Sie erlauben es, effizient nach Mustern in einem Text zu suchen. Dabei wird das Muster mittels sogenannter Matching Characters spezifiziert. \n",
    "    <br /><br />\n",
    "    \n",
    "    <a href=\"https://docs.python.org/3/howto/regex.html\">Das Python Regular Expressions HOWTO</a> beschreibt die genaue Funktionalität der Matching Characters im Detail. Die Grundidee ist es, auf effiziente Weise ein Muster vorzugeben, welches im Text gefunden werden kann. RegEx erlauben es in ihrer einfachsten Form, aufeinanderfolgende Charaktere im Text zu finden. RegEx sind in den folgenden Absätzen und genauer erklärt. \n",
    "    <br /><br />\n",
    "    \n",
    "    In ihrer grundlegensten Form können RegEx bestimmte Wörter in einem Text finden. Im folgenden Beispiel setzen wir eine RegEx zur Suche des Wortes 'uns' ein.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d061b3ef-d554-40db-b77c-3d610c2291d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(41, 44), match='uns'>\n"
     ]
    }
   ],
   "source": [
    "# Wir importieren das re Modul zum Testen der Regular Expressions und definieren einen Beispieltext.\n",
    "import re\n",
    "\n",
    "text = \"Das richtige Verwenden von AI erlaubt es uns, Probleme flexibel und nachvollziehbar zu lösen. \\\n",
    "Wenn wir Probleme haben, wenden wir uns an die AIAV Wissensdrehscheibe.\"\n",
    "\n",
    "# Die Regular Expression wird als 'uns' definiert. \n",
    "# Die Suchfunktion sucht dann nach dem ersten Vorkommen der RegEx im Text.\n",
    "expression = 'uns'\n",
    "\n",
    "# Die Suchfunktion liefert uns den gefundenen Text und seine Position\n",
    "print(re.search(re.compile(expression), text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693f1de-bf17-4725-9e86-a62ca0abf62b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Wollen wir nun alle Vorkimnisse der RegEx finden, können wir anstatt re.search() re.findall() verwenden. Im Beispieltext kommt 'uns' zwei mal vor.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88b6c53-bb7e-40d3-897f-f0fba19763ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uns', 'uns']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(re.compile(expression), text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3dd82-4edd-4172-8d01-601d8800310f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Neben Buchstaben und Zahlen können auch Sonderzeichen in Regular Expressions verwendet werden. Während die meisten Zeichen einfach nur mit sich selbst matchen, bieten einige Sonderzeichen, die sogenannten Matching Characters, extra Funktionalität für den Ausdruck einer Regular Expression. So ist es möglich, bestimmte Gruppen an Charakteren von der Suche auszuschließen oder nach sich wiederholenden Sequenzen zu suchen. Beispiele für Matching Charaktere und ihre Funktionen sind in Tabelle 1 zusammengefasst.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1965c15-7863-4016-8a56-8426b712df54",
   "metadata": {},
   "source": [
    "  _Tabelle 1: Zusammenfassung der wichtigsten Matching Charaktere:_\n",
    "  \n",
    "  | Symbol  | Funktion                                                 |\n",
    "  | ------- | --------------------------------------------------------:|\n",
    "  | ^       | Negiert den vorhergehenden Teil der Expression           |\n",
    "  | \\*      | Spezifiziert 0 oder mehr Wiederholungen                  |\n",
    "  | \\+      | Spezifiziert 1 oder mehr Wiederholungen                  |\n",
    "  | ?       | Spezifiziert mindestens 0 und maximal 1 Wiederholungen   |\n",
    "  | {m,n}   | Spezifiziert mindestens m und maximal n Wiederholungen   |\n",
    "  | \\[ \\]   | Definieren eine Klasse von Ziffern                       |\n",
    "  | \\       | Definiert ein set von Ziffern oder macht aus einem Metacharakter eine normale Ziffer                            |\n",
    "  | ()      | Definiert eine Gruppe                                    |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c11cc-5760-45b2-807e-7827a01255a3",
   "metadata": {},
   "source": [
    "### Praktische Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373bf8e1-d4ea-4add-82a5-f699f3d9717b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">    \n",
    "    Für den praktischen Usecase wurden Regular Expressions zusammen mit anderen Textverarbeitungsmethoden zur Klassifizierung der Veröffentlichungen vom <a href=\"https://roboticsworkshop.at\">Austrian Robotics Workshop (ARW)</a> verwendet. Wir wollen die Veröffentlichungen den vom Workshop angegebenen Themengebieten zuordnen um uns einen Überblick über aktuelle Forschungstrends zu verschaffen. Da es in der Formatvorlage des ARW keine händisch angegebenen Schlagwörter gibt, müssen wir zunächst potentiell wichtige Wörter aus den Texten ermitteln. Anhand dieser Schlagwörter werden die Texte dann den Themengebieten zugeordnet.\n",
    "    <br /><br />\n",
    "    \n",
    "    Der Ablauf dafür ist wie folgt: Zunächst wird der rohe Text aus den Artikeln importiert. Da nicht alle Teile des rohen Textes für die Schlagwörter Interessant sind, wird der Text zunächst mittels RegEx formatiert. Das Literaturverzeichnis wird entfernt, indem die Überschrift \"REFERENCES\" im Text gesucht und alle folgenden Charaktere weggelassen werden. Dannach werden Textformatierende Charaktere (wie z.B. \"\\n\" für neue Zeilen) und Zitationen (bestehend aus einer Zahl in eckigen Klammern) entfernt. Kapitelüberschriften und Zahlen im Text werden ebenfalls mittels Regular Expressions gesucht und entfernt. Idealerweise erhalten wir so einen Text, welcher nur Wörter aus dem Fließtext des Artikels und Satzzeichen beinhaltet.\n",
    "    <br /><br />\n",
    "    \n",
    "    Nun wird der Text in Sätze aufgeteilt. Das passiert anhand der immer noch im Text vorhandenen Satzzeichen. Die Sätze werden wiederum in Token aufgeteilt. Diese Token stellen Schlagwort Kandidaten dar. Die Aufteilung in Kandidaten erfolgt wiederum anhand der Satzzeichen (Satzende heißt auch Abschluss des aktuellen Kandidaten) und auf Basis einer Stoppliste. Die Stoppliste ist eine Liste aus Wörtern welche die Kandidaten voneinander trennen. Solche Stopplisten können automatisch generiert werden und sind innerhalb der gleichen Sprache für verschiedene Anwendungsgebiete unterschiedlich [3].\n",
    "    <br /><br />\n",
    "    \n",
    "    Nach der Generierung der Kandidaten wird jeder Kandidat bewertet. Dafür wird gezählt, wie oft jedes Wort im Text vorkommt (Wortgrad) und in wie vielen Sätzen es vorkommt (Wortfequenz). Aus diesen beiden Metriken wird jeder Schlagwort Kandidat nummerisch bewertet (Wortgrad/Wortfrequenz). Dadurch ergibt sich eine nach ihrer Wichtigkeit sortierte Liste an potentiellen Schlüsselwörtern.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7ab53-6f62-4fde-ac20-e96c01b2e87f",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung1Ablauf.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "_Abbildung 1: Damit wir uns über die Veröffentlichungen des Austrian Robotics Workshops einen Überblick verschaffen können, wollen wir die Themen der Publikationen automatisch ermitteln. Dafür wird der Text aus jeder Publikation zuerst geladen und formatiert. Stoppwörter teilen den Text dann in Phrasen auf. Diese Phrasen werden gezählt und anhand der Anzahl, wie oft sie im Text (Wortgrad) und in wievielen Sätzen die vorkommen (Wortfrequenz), bewertet. Die Phrasen mit der besten Bewertung werden dann zur Klassifizierung verwendet._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3e71a-bccd-48f1-a8ca-e13477ba0f94",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  \n",
    "    Diese Extrahierung wird nun für alle Publikationen des <a href=\"roboticsworkshop.at\">Austrian Robotics Workshops</a> durchgeführt (siehe Abbildung 1). Die Schlagwörter werden dabei für alle Publikationen aus einem Jahr gruppiert. Jedes Paper wird anschließend anhand seiner automatisch generierten Schlagwörter innnerhalb der Themengebiete in Tabelle 2 klassifiziert. Die Themengebiete spiegeln die auf <a href=\"roboticsworkshop.at\">roboticsworkshop.at</a> angegebenen Themengebiete für den Austrian Robotics Workshop wieder. Für jedes Themengebiet wurde eine Liste an Schlagwörtern mittels <a href=\"https://ads.google.com/intl/de_at/home/tools/keyword-planner/\">Google Keyword Planner</a> ermittelt und händisch überarbeitet. RegEx wurde hier wiederum verwendet, um zu überprüfen, welche Schlagwörter aus welchen Publikationen mit den Schlagwörtern in Tabelle 2 übereinstimmen.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47481e-25e1-452a-91d9-e3133fcf278f",
   "metadata": {},
   "source": [
    "  | Themengebiet            | Schlagwörter                                                 |\n",
    "  |:----------------------- | ------------------------------------------------------------:|\n",
    "  | RobotSensingPerception  | sensor, filter, sensing, vision, camera |\n",
    "  | MachineLearningAI       | machine learning, artificial intelligence agent, feature, classification, network |\n",
    "  | RobotModelling          | model estimation, pose, kinematic, structure |\n",
    "  | SoftwareDesign          | software middleware, simulation, communication, verification |\n",
    "  | MobileAndServiceRobots  | mobile, plan, rescue, ontology, constraint |\n",
    "  | HumanRobotInteraction   | human user, interact, safe, workspace |\n",
    "  | EducationalRobots       | education, show, demonstrate, workshop, school |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73af4bc-7ecb-4275-856f-2f6c49ce7d18",
   "metadata": {},
   "source": [
    "  _Tabelle 2: Regular Expressions ordnen die Paper anhand ihrer Schlagworte verschiedenen Themengebieten zu. Dazu wurde zu jedem Thema eine Liste von Schlagworten basieren auf dem Google Keyword Planner erstellt._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba7aa8-ce6f-436e-bcd6-b10dffd54f7c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  \n",
    "    Die Ergebnisse der einzelnen Klassifikationen werden pro Jahr aufsummiert und der prozentuale Anteil jedes Themas für das jeweilige Jahr wird berechnet. Letzendlich werden diese Anteile, wie in Abbildung 2 gezeigt, als Balkendiagramm dargestellt.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b62eb-43ad-4f94-a5eb-82421fcdf22b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"images/Abbildung2Resultate.png\" alt=\"Drawing\" style=\"width: 1200px;\"/>\n",
    "\n",
    "_Abbildung 2: Die Veröffentlichungen des Austrian Robotics Workshops wurden mittels Regular Expressions auf Schlagwörter untersucht. Diese extrahierten Schlagwörter erlauben es uns, die Paper anhand ihres Themengebiets zu klassifizieren um einen Überblick über aktuelle Trends in der Forschung in Österreich zu erlangen._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d5d96-2372-45db-af8a-8692d0536f95",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "Regular Expressions erlauben es uns, Natural Language Processing schnell und effizient zu betreiben. Dadurch das RegEx so optimiert sind, können Untersuchungen von großen Mengen Text lokal, ohne viel Rechenleistung durchgeführt werden.\n",
    "\n",
    "Die Themenbereiche der Veröffentlichungen zeigen einen klaren Trend zur Verwendung von Künstlicher Intelligenz in Robotik. Besonders im Jahr 2018, dem einzigen Jahr zwischen 2016 und 2020, in dem der Austrian Robotics Workshop nicht zusammen mit einer anderen Konferenz abgehalten wurde, fällt der Anteil an Publikationen mit Fokus auf AI sehr hoch aus. Dies zeigt uns, dass AI in der österreichischen Forschung an Robotik Anwendung findet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87e5de-5abe-4bcc-a473-b1cebc004574",
   "metadata": {},
   "source": [
    "### Quellen\n",
    "\n",
    "[1] Nadkarni, P.M., Ohno-Machado, L. and Chapman, W.W., 2011. Natural language processing: an introduction. Journal of the American Medical Informatics Association, 18(5), pp.544-551.\n",
    "\n",
    "[2] Russell, S. and Norvig, P., 2002. Artificial intelligence: a modern approach.\n",
    "\n",
    "[3] Rose, S., Engel, D., Cramer, N. and Cowley, W., 2010. Automatic keyword extraction from individual documents. Text mining: applications and theory, 1, pp.1-20.\n",
    "\n",
    "<P style=\"page-break-before: always\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aafb66-caf6-426a-b10f-a43b76319e7c",
   "metadata": {},
   "source": [
    "## Codedokumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b1f88-ec69-4f32-84b4-db41472a147e",
   "metadata": {},
   "source": [
    "Die Implementierung des Usecases wurde in <a href=\"python.org\">Python 3</a> durchgeführt und verwendet das <a href=\"https://github.com/chrismattmann/tika-python\">Tika-Python Modul</a> zum Einlesen der Pdf Dateien und das <a href=\"https://www.nltk.org/\">Python Natural Langugae Toolkit</a> für das Aufspalten der Texte in Sätze und Wörter. <a href=\"https://numpy.org/\">Numpy</a> und <a href=\"https://matplotlib.org/\">Matplotlib</a> ermöglichen dabei eine anschauliche Visualisierung der Ergebnisse.\n",
    "\n",
    "Zunächst werden alle benötigten Module installiert und geladen. Mittels des Natural Language Toolkits wird die neueste Version des Tokenizers, einer Klasse zum Aufspalten des Textes in Sätze, heruntergeladen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29054392-e0e8-4b75-a0a1-6d1d13f922b7",
   "metadata": {},
   "source": [
    "Danach wird die _phraseStorage_ Klasse implementiert. Diese Klasse wird verwendet, um jeden Schlagwort Kandidaten, sowie die Anzahl seiner Vorkomnisse im Text und in jedem Satz festzuhalten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c817d-7326-4cdc-900c-572329814d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m apt install default-jre\n",
    "!{sys.executable} -m pip install nltk tika matplotlib\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from tika import parser\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Download des englischen Tokenisers, zum Aufschlüsseln nach Sätzen\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Festlegung der trennenden Satzzeichen und Import der Stoppwörter aus stopwords.py\n",
    "punctuation = ['.', ',', '-', ';', '(', ')', '%', '=', '’', '&']\n",
    "from stopwords import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085bfdcf-8a10-4b94-b50e-ecadcc1aa5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einführung einer Hilfsklasse um das Zählen der Kandidaten zu vereinfachen\n",
    "class phraseStorage():\n",
    "    def __init__(self):\n",
    "        \"\"\" Klassenkonstruktor \"\"\"\n",
    "        self.phrases = []\n",
    "        self.count = []\n",
    "        self.scentenceCount = []\n",
    "        self.score = []\n",
    "    def getPhraseStats(self, phrase):\n",
    "        \"\"\" Gibt die aktuellen Zählstände einer Phrase zurück \"\"\"\n",
    "        c = 0\n",
    "        s = 0\n",
    "        if phrase in self.phrases:\n",
    "            c = self.count[self.phrases.index(phrase)] \n",
    "            s = self.scentenceCount[self.phrases.index(phrase)]\n",
    "        return phrase, c, s\n",
    "    def removeExtraSpaces(self, phrase):\n",
    "        \"\"\" Entfernt extra Leerzeichen \"\"\"\n",
    "        phrase = re.sub(' {2,2}', '', phrase.strip())\n",
    "        return phrase\n",
    "    def add(self, phrases):\n",
    "        \"\"\" Fügt eine oder mehrere Phrasen den bekannten Phrasen hinzu \"\"\"\n",
    "        if (not isinstance(phrases, list)): phrases = [phrases]\n",
    "        for phrase in phrases:\n",
    "            if phrase == '': continue\n",
    "            elif phrase in self.phrases:\n",
    "                # Ist eine Phrase bereits bekannt, wird gezählt, wie oft sie vorkommt\n",
    "                self.count[self.phrases.index(phrase)] += 1\n",
    "            else:\n",
    "                # Ist sie nicht bekannt, wird ein neuer Eintrag angelegt\n",
    "                self.phrases.append(self.removeExtraSpaces(phrase))\n",
    "                self.count.append(1)\n",
    "                self.scentenceCount.append(0)\n",
    "                self.score.append(0.0)\n",
    "    def addBatch(self, phrases, counts):\n",
    "        \"\"\" Fügt eine oder mehrere Phrasen mit Zählstand hinzu \"\"\"\n",
    "        if (not isinstance(phrases, list)): phrases = [phrases]\n",
    "        if (not isinstance(counts, list)): counts = [counts]\n",
    "        for phrase, count in zip(phrases, counts):\n",
    "            if phrase in self.phrases:\n",
    "                # Ist eine Phrase bereits bekannt, wird gezählt, wie oft sie vorkommt\n",
    "                self.count[self.phrases.index(phrase)] += count\n",
    "            else:\n",
    "                # Ist sie nicht bekannt, wird ein neuer Eintrag angelegt\n",
    "                self.phrases.append(self.removeExtraSpaces(phrase))\n",
    "                self.count.append(count)\n",
    "                self.scentenceCount.append(0)\n",
    "                self.score.append(0.0)\n",
    "    def updateScentenceCount(self, phrases):\n",
    "        \"\"\" Update des sekundären Zählstands der Phrase \"\"\"\n",
    "        if (not isinstance(phrases, list)): phrases = [phrases]\n",
    "        # Heausfiltern von mehrmals vorkommenden Wörtern eines Satzes\n",
    "        uniquePhrases = []\n",
    "        for phrase in phrases:\n",
    "            if phrase not in uniquePhrases:\n",
    "                uniquePhrases.append(phrase)\n",
    "        # Update des Zählstandes\n",
    "        for phrase in uniquePhrases:\n",
    "            if phrase not in self.phrases:\n",
    "                self.add(phrase)\n",
    "            self.scentenceCount[self.phrases.index(phrase)] += 1\n",
    "    def updateScore(self):\n",
    "        for i, _ in enumerate(self.phrases):\n",
    "            self.score[i] = self.count[i]/self.scentenceCount[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7b461-7a08-4572-959d-311f004104ad",
   "metadata": {},
   "source": [
    "Die _generateKeywords_ Funktion verwendet RegEx, um aus dem rohen Text aus den Paper, Schlagwörter zu extrahieren. Dazu wird der Text folgendermaßen formatiert:\n",
    "\n",
    "- Weglassen des Literaturverzeichnisses\n",
    "- Weglassen der Newline Charakter \"\\n\"\n",
    "- Weglassen der Kapitelüberschriften\n",
    "- Weglassen von Zitationen\n",
    "- Weglassen der \"Abstract\" Überschrift\n",
    "- Weglassen von Zahlen\n",
    "- Weglassen von Sonderzeichen\n",
    "- Weglassen von i.e. und e.g.\n",
    "\n",
    "Der verbleibende wird Text mittels des Tokenizers aus dem Python Natural Language Processing Toolkit in Sätze gespalten. Die Sätze werden wiederum mittes Stoppwörtern in Phrasen gespalten. Diese resultierenden Phrasen sind Kandidaten für Schlagwörter.\n",
    "\n",
    "Die oben implementierte _phraseStorage_ Klasse zählt wie oft eine Phrase im gesamten Text (Wortgrad) und in jedem Satz (Wortfrequenz) vorkommt. Diese Information wird anschließend verwendet, um jede Phrase anhand von Wortgrad/Wortfrequenz zu bewerten. Die Phrasen werden anhand der Bewertung sortiert und die Funktion gibt die angegebene Anzahl an Schlagwörtern mit der höchsten Bewertung zurück. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9183da5-9706-4117-8c38-297950b0d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateKeywords(data, keywordAmount):\n",
    "    \"\"\" Generiert keywordAmount Keywords aus einem Text (data, als String) \"\"\"\n",
    "    # Nutzung von Regular Expressions, um unerwünschte Teile aus dem Text zu entfernen\n",
    "    # Weglassen der Literatur\n",
    "    i = re.compile('REFERENCES', flags=re.IGNORECASE)\n",
    "    ret = re.search(i, data)\n",
    "    if ret is not None:  data = data [0:ret.span()[0]]\n",
    "    # Weglassen der Newline Charakter\n",
    "    i = re.compile('-\\\\n')\n",
    "    data = re.sub(i, '', data)\n",
    "    i = re.compile('\\\\n')\n",
    "    data = re.sub(i, ' ', data)\n",
    "    # Weglassen von Kapitelüberschriften\n",
    "    # Kapitelüberschrift = Römische Zahl + '. ' + großgeschriebener Titel\n",
    "    i = re.compile('[IVX]+[.][ ]([A-Z]+[ ])+')\n",
    "    data = re.sub(i, '', data)\n",
    "    # Weglassen von Zitationen\n",
    "    # Zitation = '[' + Zahl + ']'\n",
    "    i = re.compile('\\[[0-9]+\\]')\n",
    "    data = re.sub(i, '', data)\n",
    "    # Weglassen des 'Abstract' Keywords am Anfang der IEEE Vorlage\n",
    "    #data = re.sub('abstract.', '', data, flags=re.IGNORECASE)\n",
    "    ret = re.search('abstract', data, flags=re.IGNORECASE)\n",
    "    if ret is not None: data = data[ret.span()[1]:]\n",
    "    # Weglassen von Sonderzeichen\n",
    "    i = re.compile('[^A-Za-z0-9\\s,.()]')\n",
    "    data = re.sub(i, '', data)\n",
    "    # Weglassen von Zahlen\n",
    "    # Zahl = Leerzeichen/Klammer Auf + kein oder ein Vorzeichen + Zahl + kein oder ein Punkt + Zahl + Leerzeichen/Klammer Zu\n",
    "    i = re.compile('\\s+[(]*[-+±]*\\d+[.,]*\\d*[)]*\\s+')\n",
    "    data = re.sub(i, '', data)\n",
    "    # Spezialfall = Zahl am Satzende\n",
    "    i = re.compile('\\s+[(]*[-+±]*\\d+[.,]*\\d*[)]*[.]')\n",
    "    data = re.sub(i, '', data)\n",
    "    # Weglassen von i.e. und e.g.\n",
    "    i = re.compile('e\\.g\\.')\n",
    "    data = re.sub(i, '', data)\n",
    "    i = re.compile('i\\.e\\.')\n",
    "    data = re.sub(i, '', data)\n",
    "    # Erzeugung der Keyword Kandidaten\n",
    "    localStorage = phraseStorage()      # Speicher für Kandidaten eines Satzes\n",
    "    globalStorage = phraseStorage()     # Globaler Speicher für alle Kandidaten\n",
    "    # Aufschlüsselung nach Sätzen anhand des Tokenisers\n",
    "    scentences = tokenizer.tokenize(data.lower())\n",
    "    buf = []\n",
    "    # Iteration über alle Sätze\n",
    "    for scentence in scentences:\n",
    "        # Iteration über alle Sätze\n",
    "        words = nltk.word_tokenize(scentence)\n",
    "        # Iteration über alle Wörter im Satz\n",
    "        for word in words:\n",
    "            # Überprüfung, ob das aktuelle Wort in der Stoplist ist\n",
    "            if (word in stopwords) or (word in punctuation):\n",
    "                if buf == []:\n",
    "                    # Überspringe die aktuelle Phrase, falls sie leer ist\n",
    "                    pass\n",
    "                else:\n",
    "                    # aktuelle Phrase wird den bekannten Phrasen hinzugefügt\n",
    "                    localStorage.add(' '.join(buf))\n",
    "                    buf = []\n",
    "            else:\n",
    "                # Aktuelles Wort wird der Phrase hinzugefügt\n",
    "                if buf != []:\n",
    "                    buf.append(' ')\n",
    "                buf.append(word)\n",
    "        # Satzende schließt immer die laufende Phrase ab\n",
    "        localStorage.add(' '.join(buf))\n",
    "        buf = []\n",
    "        # LocalStorage wird am Satzende ausgelesen\n",
    "        # Inhalt wird in globalStorage gespeichert\n",
    "        # Und localStorage wird zurückgesetzt\n",
    "        globalStorage.addBatch(localStorage.phrases, localStorage.count)\n",
    "        globalStorage.updateScentenceCount(localStorage.phrases)\n",
    "        localStorage = phraseStorage()\n",
    "    # Berechnung der Scores\n",
    "    globalStorage.updateScore()\n",
    "    # Ausgabe der Kandidaten basierend auf ihren Scores\n",
    "    result = []\n",
    "    keywordMinLen = 3\n",
    "    # Gernerierung der Keywords anhand davon, wie oft sie vorkommen\n",
    "    # Reihung der Keywords anhand der Scores\n",
    "    tmp = [x for _, x in sorted(zip(globalStorage.count, globalStorage.phrases), reverse=True)]\n",
    "    keywords = []\n",
    "    for phrase in tmp:\n",
    "        if len(phrase) < keywordMinLen: continue\n",
    "        elif len(keywords) >= keywordAmount: break\n",
    "        else: keywords.append(phrase)\n",
    "    for phrase in [x for _, x in sorted(zip(globalStorage.score, globalStorage.phrases), reverse=True)]:\n",
    "        if phrase in keywords: result.append(phrase)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99be009-7ff8-467b-9360-4da3fc62dbd5",
   "metadata": {},
   "source": [
    "Die _getTitle_ Funktion sucht mittels RegEX im Text nach dem Titel und gibt diesen zurück. Deser ist in der Formatvorlage des Austrian Robotics Workshops von mehreren Zeilenumbrüchen umgeben. Falls ein einzeiliger Titel vorliegt, wird nach dem muster \"\\n\\nTITEL\\n\\n\" gesucht, während bei mehrzeiligen Titeln nach \"\\n\\nZEILE1\\nZEILE2\\n\\n\" gesucht wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b863d-7144-40a4-abd3-657a574fc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(data):\n",
    "    \"\"\" Ermittelt den Titel einer Publikation \"\"\"\n",
    "    # Es wird nach den \"\\n\" am Anfang der Formatvorlage gesucht\n",
    "    i = re.compile('(\\\\n)*.*(\\\\n)?.*')\n",
    "    ret = re.match(i, data)\n",
    "    if ret is not None:\n",
    "        span = ret.span()\n",
    "        data = data[span[0]:span[1]]\n",
    "    # Test, ob ein Zeilenumbruch im Titel ist\n",
    "    # Wenn ja, wird er durch ein Leerzeichen ersetzt\n",
    "    i = re.compile('.\\\\n.')\n",
    "    idx = re.search(i, data)\n",
    "    if idx is not None:\n",
    "        idx = idx.span()\n",
    "        data = data[:idx[0]+1] + ' ' + data[idx[1]-1:]\n",
    "    # Weglassen der Zeilenumbrüche im resultierenden Titel\n",
    "    i = re.compile('(\\\\n)*')\n",
    "    return re.sub(i, '', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82250a-3a3c-41e7-8b1f-4703ac5bd1df",
   "metadata": {},
   "source": [
    "Die _analyseYear_ Funktion versucht alle Dateien in einem Verzeichnis einzulesen und mittels der oben vorgestellten Methode zu verarbeiten. Dazu wird der Inhalt jeder Datei geladen, der Titel wird aus dem Text extrahiert und gespeichert und Schlagwörter werden aus dem Text generiert und gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e19dc-7745-4736-8ca7-6dbd309426f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyseYear(mypath, numKeywords):\n",
    "    \"\"\" Ermittelt die Titel und Keywords aller Publikationen in einem Verzeichnis \"\"\"\n",
    "    # Ermittlung alles Dateien im pdfs Verzeichnis\n",
    "    files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    # Iteration über alle Dateien im Verzeichnis\n",
    "    # Schlagwörter und Titel jeder Publikation werden gespeichert\n",
    "    totalKeywords = []\n",
    "    titles = []\n",
    "    for filename in files:\n",
    "        # Formatierung des Dateinamens\n",
    "        filename = '{}/{}'.format(mypath, filename)\n",
    "        # Laden des Inhalts der Datei\n",
    "        raw = parser.from_file(filename)\n",
    "        data = raw['content']\n",
    "        # Weglassen von \"D ra  ft\" (in den Formatvorlagen eingebettet)\n",
    "        i = re.compile('D\\s{0,3}ra\\s{0,3}ft')\n",
    "        data = re.sub(i, '', data)\n",
    "        # Ermittlung von Schlagwörtern und Titel der Publikation\n",
    "        titles.append(getTitle(data))\n",
    "        tmp = generateKeywords(data, numKeywords)\n",
    "        totalKeywords.append(tmp)\n",
    "    return totalKeywords, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80144b39-f6cd-4bac-b2f4-ebd4b6cacfcb",
   "metadata": {},
   "source": [
    "Anschließend werden Regular Expressions zur Klassifizierung der ermittelten Schlagwörter definiert. Dazu werden zunächst die Themen zugehörige Schlüsselwörter händisch festgelegt. Die Themengebiete representieren dabei die Themengebiete die auf der Webseite des [Austrian Robotics Workshops](https://www.roboticsworkshop.at/) angegeben sind. Die entsprechenden Schlagwörter zu den Themen wurden mittels [Google Keyword Planner](https://ads.google.com/intl/de_at/home/tools/keyword-planner/) ermittelt und händisch überarbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5632021f-d71c-4ae8-ba3d-b6b5066e5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der klassifizierung der publikationen\n",
    "# Die klassifizierung basiert darauf, ob die spezifizierten schlagwärter in den texten gefunden wurden\n",
    "classes = [\n",
    "    'RobotSensingPerception',\n",
    "    'MachineLearningAI',\n",
    "    'RobotModelling',\n",
    "    'SoftwareDesign',\n",
    "    'MobileAndServiceRobots',\n",
    "    'HumanRobotInteraction',\n",
    "    'EducationalRobots'\n",
    "]\n",
    "\n",
    "classKeywords = [\n",
    "    ['sensor', 'filter', 'sensing', 'vision', 'camera'],\n",
    "    ['machine learning', 'artificial intelligence', 'agent', 'feature', 'classification', 'network'],\n",
    "    ['model', 'estimation', 'pose', 'kinematic', 'structure'],\n",
    "    ['software', 'middleware', 'simulation', 'communication', 'verification'],\n",
    "    ['mobile', 'plan', 'rescue', 'ontology', 'constraint'],\n",
    "    ['human', 'user', 'interact', 'safe', 'workspace'],\n",
    "    ['education', 'show', 'demonstrate', 'workshop', 'school']\n",
    "]\n",
    "\n",
    "# Generierung der regular expressions zum absuchen der schlagwörter\n",
    "regexes = []\n",
    "for c in classKeywords:\n",
    "    regexes.append('\\\\b(?:' + '|'.join(c) + ')\\\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21f34d-af95-438e-92a3-12b25388dcf3",
   "metadata": {},
   "source": [
    "Die _getClassificationScores_ Funktion sucht mittels RegEx nach den definierten Schlagwörtern der Themengebiete in den ermittelten Schlafwörtern aus den Publikationen. Dabei wird gespeichert, wie viele Übereinstimmungen es mit den Schlagwörtern eines bestimmten Themas gibt. Gibt es zu einer Publikation keine Übereinstimmung, wird diese übersprungen. Die prozentuale Übereinstimmung aller überprüften Paper wird zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38809a61-4cbd-4e00-9ff3-b39b3bccf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassificationScores(totalKeywords, regexes):    \n",
    "    scores = []\n",
    "    for phrases in totalKeywords:\n",
    "        text = ' '.join(phrases)\n",
    "        classScore = [0 for i in range(len(classes))]\n",
    "        for i, expression in enumerate(regexes):\n",
    "            classScore[i] += len(re.findall(re.compile(expression), text))\n",
    "        scores.append(classScore)\n",
    "    scoreSum = [np.sum(score) for score in scores]\n",
    "    print('{} out of {} papers could not be classified'.format(scoreSum.count(0), len(scoreSum)))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27df0c3-9271-4c9f-b6dd-cad0c4e58bba",
   "metadata": {},
   "source": [
    "Die Klassifizierung wird nun für alle Jahre (alle Publikationen aus einem Jahr sind in einem eigenen Verzeichnis) durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d18b8-5c1b-418f-b405-130948ef412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['./pdfs/ARW2016', './pdfs/ARW2017', './pdfs/ARW2018', './pdfs/ARW2019', './pdfs/ARW2020']\n",
    "scores = []\n",
    "totalKeywords = []\n",
    "titles = []\n",
    "for mypath in paths:\n",
    "    tmpKeywords, tmpTitle = analyseYear(mypath, 40)\n",
    "    totalKeywords.append(tmpKeywords)\n",
    "    titles.append(tmpTitle)\n",
    "    scores.append(getClassificationScores(tmpKeywords, regexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85c12c-c1af-4e63-a5cd-9eff28339a46",
   "metadata": {},
   "source": [
    "Abschließend werden die ermittelten Ergebnisse mittels [Matplotlib](https://matplotlib.org/) visualisert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e73888-576f-4dcb-adf5-b81c3eb511cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import von Matplotlib zur Visualisierung\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 18\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Formatierung der Platzhalter für Balkendiagram von jedem Jahr\n",
    "plots = len(scores)\n",
    "classArrange = np.arange(0, len(classes))\n",
    "fig, ax = plt.subplots(1,plots)\n",
    "fig.suptitle('Automatisch ermittelte Themen der Paper des Austrian Robotics Workshops')\n",
    "\n",
    "# Definition der Beschriftungen und Farben\n",
    "years = ['ARW 2016', 'ARW 2017', 'ARW 2018', 'ARW 2019', 'ARW 2020']\n",
    "plotColors = ['navy', 'mediumpurple', 'navy', 'mediumpurple', 'navy', 'mediumpurple', 'navy']\n",
    "textColors = ['black', 'gray', 'black', 'gray', 'black', 'gray', 'black']\n",
    "\n",
    "# Iteration über alle Jahre und Einzeichnen der Balken und Beschriftungen\n",
    "for i in range(plots):\n",
    "    scoreSum = [np.sum(score) for score in scores[i]]\n",
    "    values = np.array([0.0 for j in range(len(classes))])\n",
    "    for j in range(len(scoreSum)):\n",
    "        if scoreSum[j] != 0: values += np.array(scores[i][j])/scoreSum[j]\n",
    "        else: continue\n",
    "    nonZeroEntries = len(scoreSum) - scoreSum.count(0)\n",
    "    values /= nonZeroEntries/100\n",
    "    values = np.round(values, 0)\n",
    "    ax[i].set_xlim([-1, 7])\n",
    "    ax[i].set_ylim([0, 45])\n",
    "    ax[i].bar(classArrange,values, zorder=3, color=plotColors, width=0.6)\n",
    "    ax[i].grid(True, zorder=0)\n",
    "    ax[i].set_xticks(classArrange)\n",
    "    for j, c in enumerate(classes):\n",
    "        ax[i].annotate(c, (j+0.2,0), rotation=45, horizontalalignment='right', verticalalignment='top', color=textColors[j])\n",
    "    ax[i].set_xticklabels([' ' for _ in classArrange])\n",
    "    ax[i].set_title(years[i], fontsize=22)\n",
    "\n",
    "# Anpassung der Achsenbeschriftung Puffers um die Grafik\n",
    "ax[0].set_ylabel('Anteil jedes Themas in den Papern [%]', fontsize=20)\n",
    "plt.gcf().subplots_adjust(bottom=0.4)\n",
    "\n",
    "# Anzeigen der Grafik\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
