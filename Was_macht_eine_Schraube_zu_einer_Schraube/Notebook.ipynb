{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215a74be-c3d9-4c2e-afb7-a1019c72fabd",
   "metadata": {},
   "source": [
    "# Was macht eine Schraube zu einer Schraube?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c0c1d-b8d0-4ae6-a08f-72d1379f27ef",
   "metadata": {},
   "source": [
    "## Storyboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2f35f-0475-4182-ad4b-eebb3313d779",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "    In der Technikum Digital Factory stellen wir zurzeit Roboter für Forschung- und Ausbildungszwecke her. Die Maschinen in der Fertigungsstraße werden dabei automatisch mit belieferten Komponenten wie z.B. Schrauben bestückt, damit die Produktion vollautomatisch abläuft. Durch diesen hohen Grad der Automatisierung fällt es aber oftmals erst spät auf, wenn fehlerhafte oder beschädigte Komponenten geliefert wurden. Beschädigungen fallen oft erst dadurch auf, dass das Teil nicht eingebaut werden kann; wodurch die ganze Maschine zum Stehen kommen kann. Also liegt es nahe, Schrauben und andere Teile vor dem Bestücken der Maschine zu kontrollieren. Da die händische Kontrolle bei großen Stückzahlen sehr zeitaufwändig ist, wollen wir diese automatisiert durchführen.\n",
    "    <br /><br />\n",
    "    \n",
    "    Aber wie erkennen wir automatisch Beschädigungen an gelieferten Teilen? \n",
    "    <br /><br />\n",
    "    \n",
    "    Um diese Frage zu beantworten, wollen wir mittels eines Kamerabilds Schrauben auf Beschädigungen überprüfen, bevor sie in eine Maschine eingelegt werden. Da die Produktion aber noch nicht in vollem Gange ist, haben wir nur einige wenige beschädigte Schrauben als Referenz zur Verfügung. Also können wir nur wenige Beispielbilder von beschädigten Schrauben aufnehmen, um unser System zu trainieren.\n",
    "    <br /><br />\n",
    "    \n",
    "    Wir benötigen also ein System welches lernt, visuell beschädigte von unbeschädigten Schrauben zu unterscheiden. Aber was macht eine Schraube überhaupt zu einer beschädigten Schraube?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414961d-88e4-40d1-a841-e36fc0b20076",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks\n",
    "<div style=\"text-align: justify\">\n",
    "    In den AIAV Videos <a href=\"https://www.youtube.com/watch?v=5dJyt0m5PJw&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=31\">Classifier vs. Detector</a>, <a href=\"https://www.youtube.com/watch?v=cdVDMm5Wscc&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=35\">CNN Classifier</a>, <a href=\"https://www.youtube.com/watch?v=ioDdAE6AOMQ&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=39\">Multinomial Naive Bayes Classifier</a> und <a href=\"https://www.youtube.com/watch?v=yhY9O-5BW74&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=40\">Gaussian Naive Bayes Classifier</a> haben wir uns bereits verschiedene Arten der Klassifizierung angesehen. Dabei lernen Modelle bestimmte Aufgaben, z.B. das Erkennen eines Hammers, mithilfe von Trainingsdaten.\n",
    "    <br /><br />\n",
    "    \n",
    "    Dabei stellt sich die Frage, wo diese Daten für die Trainingssequenzen herkommen. Will man z.B. einen Hammer erkennen, braucht man mehrere Bilder von dem Hammer, den man erkennen will. Sollen jetzt mehrere verschiedene Arten von Hammern erkannt werden, sind mehrere Bilder von allen zu erkennenden Hammerarten notwendig. Die Anforderungen an qualitative Trainingsdaten werden schnell sehr hoch; je genereller die Klassifizierung, desto mehr Trainingsdaten braucht man.\n",
    "    <br /><br />\n",
    "    \n",
    "    Dieses Problem kann man mit <a href=\"https://ieeexplore.ieee.org/abstract/document/8253599\">Generative Adversarial Networks (GAN)</a> umgehen. Die Idee von GANs ist es, zwei neuronale Netzwerke (siehe AIAV Video <a href=\"https://www.youtube.com/watch?v=pmfIJ3XUw2c&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=37\">Klassische Neuronale Netze</a>) gegeinander antreten zu lassen. Diese beiden Netzwerke heißen Generator und Diskriminator. Die Aufgabe des Generators ist es, aus Rauschen Daten zu generieren, welche zu einem vorgegebenen Datensatz passen. Der Diskriminator versucht dann, die echten von den genierierten Daten zu unterscheiden.\n",
    "    <br /><br />\n",
    "    \n",
    "    Abbildung 1 zeigt echte sowie generierte Bilder von Schrauben. Die echten Bilder sind aus einem <a href=\"https://www.mvtec.com/company/research/datasets/mvtec-ad\">offenen Datensatz</a>, während die generierten Bilder von einem, anhand diesen Datensatzes trainierten, GAN erzeugt wurden. Da das GAN anhand des Datensatzes trainiert wurde, erzeugt sein Generator Bilder, welche zu den bestehenden Bildern im Datensatz passen. Die echten Bilder wurden dabei auf die gleiche Auflösung (32x32 Pixel) skaliert, wie die Bilder aus dem GAN. Können Sie erkennen, welche Bilder echt sind und welche nicht?  \n",
    "     </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec6a44-6e77-4ef9-b374-dd7b108ae785",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung1Vergleich.jpg\" width=\"600\" />\n",
    "\n",
    "_Abbildung 1: Hier sehen Sie echte sowie generierte Bilder von Schrauben. Die echten Bilder sind aus einem <a href=\"https://www.mvtec.com/company/research/datasets/mvtec-ad\">offenen Datensatz</a>, während die generierten Bilder von einem, anhand diesen Datensatzes, trainierten Generative Adversarial Networks kommen. Können Sie erkennen, welche Bilder echt sind und welche nicht?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e3f65-571f-4bed-b8ee-4dd6dfa62933",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "    Abbildung 2 zeigt den Ablauf, wie GANs trainiert werden. Zunächst erzeugt der Generator auf Basis eines rauschenden Signals eine Probe. Diese Probe entspricht einem Eintrag im Trainingsdatensatz. Danach wird eine weitere Probe, dieses Mal aus dem Trainingsdatensatz, genommen. Nun werden beide Proben in den Diskriminator gegeben, welcher für jede der Proben schätzt, ob diese echt, also aus dem Trainingsdatensatz ist, oder vom Generator erzeugt wurde. Während des Trainings wird dieser Ablauf mehrmals durchgeführt, mit dem Ziel, gleichzeitig Generator und Diskriminator zu optimieren.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf3a9b-6105-4b5d-873b-9485a175be13",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung2KonzeptGAN.png\" width=\"800\" />\n",
    "\n",
    "_Abbildung 2: Die Idee von Generative Adversarial Networks (GAN) ist es, gleichzeitig den Generator und einen Diskriminator zu trainieren. Der Generator versucht dabei aus Rauschen einen Ausgang zu erzeugen, welcher der Probe aus den Trainingsdaten ähnlich ist. Diese generierten Datenproben werden dann zusammen mit Proben aus dem Trainingsdatensatz dem Diskriminator übergeben. Die Aufgabe des Diskriminators ist dann zu unterscheiden, welche Proben echt und welche generiert sind. Der Ablauf von GAN ist hier mittels dem <a href=\"https://www.mvtec.com/company/research/datasets/mvtec-ad\">Datensatz von Schraubenbildern</a> gezeigt._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eeaebb-1e9f-4456-a88f-d1c2d004bda2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "    Nach dieser Trainingssequenz können wir nun Generator und Diskriminator speichern und für andere Aufgaben verwenden. In unserem Beispiel kann der Generator nach genug Trainingsepisoden Bilder von Schrauben erzeugen, welche nicht von den Trainingsbildern unterscheidbar sind. Der Diskriminator hat währenddessen gelernt zu erkennen, ob das eingegebene Bild von einer Schraube ist, oder nicht.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3077aa-6703-4993-b322-cae4d160e467",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Praktische Implementierung\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "    Für die praktische Implementierung des Usecases trainieren wir ein GAN anhand von aufgenommenen Bildern von unbeschädigten Schrauben. Dabei werden die Bilder zunächst als Graustufenbild eingelesen und auf eine Auflösung von 32 mal 32 Pixel skaliert. Die Skalierung auf so eine niedrige Auflösung ist notwendig, um den Rechenaufwand des Trainings gering zu halten. Nach dem Training werden Generator und Diskriminator zur Klassifizierung wiederverwendet. Dabei wird ein Modell zur <a href=\"https://arxiv.org/abs/1703.05921\">Erkennung von Anomalien</a> basierend auf Generator und Diskriminator trainiert. Dieses entscheidet, ob neue und damit unbekannte Bilder die gleichen Merkmale aufweisen, wie die Bilder im Trainingsdatensatz.\n",
    "    <br /><br />\n",
    "    \n",
    "    Abbildung 3 zeigt den Ablauf der Lösung. Dabei führen wir die Trainingssequenz vorab anhand der Trainingsdaten durch und speichern die Trainierten Netzwerke ab, da der benötigte Rechenaufwand für das Training relativ hoch ist. Während des Betriebs lesen wir dann Schraubenbilder ein, bearbeiten sie genauso wie die Trainingsdaten vor und lassen das Modell entscheiden ob das Bild Anomalien aufweist. Anomalien sind in diesem Fall Beschädigungen an der Schraube.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050ac86-8d8b-4f54-a979-8919ce263131",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung3KonzeptImplementierung.png\" width=\"700\" />\n",
    "\n",
    "_Abbildung 3: Wir verwenden Generator und Diskriminator aus dem GAN wieder, um zu erkenen ob unsere Schrauben beschädigt sind. Die beschädigten Schrauben passen sie nicht zu den Schrauben aus dem Trainingsdatensatz und werden daher als beschädigt erkannt._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f0786-09bf-4c3b-b75e-b73941ed84ee",
   "metadata": {},
   "source": [
    "### Fazit\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "Generative Adversarial Networks erlauben es uns zu lernen, auch wenn nicht genug Daten für das Training von anderen Arten von Modellen vorhanden sind. Obwohl wir mit kleinen Neuronalen Netzwerken und in einer niedrigen Auflösung von 32x32 Pixeln gearbeitet haben, war die benötigte Rechenleistung für das Training relativ hoch. Dafür sind GANs aber eine sehr flexible Lösung. Vor allem in Kombination mit anderen Verfahren aus AI erlauben sie es uns, praktische Probleme zu lösen, ohne zuerst eine große Menge an Trainingsdaten aufnehmen zu müssen.\n",
    "    </div>\n",
    "\n",
    "<P style=\"page-break-before: always\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1051c2b5-b005-4fb0-83d9-b1d9a7c90d72",
   "metadata": {},
   "source": [
    "## Codedokumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf97fdb-ec42-4643-9264-8196f967075b",
   "metadata": {},
   "source": [
    "Die Implementierung wurde mit <a href=\"https://github.com/plaidml/plaidml\">PlaidML</a> als Machine Learning Bibliothek und <a href=\"https://keras.io/\">Keras</a> als Anwenderinterface für die Bibliothek in <a href=\"https://www.python.org/\">Python 3</a> realisiert. <a href=\"https://pillow.readthedocs.io/en/stable/\">Pillow</a> und <a href=\"https://pypi.org/project/opencv-python/\">opencv-python</a> wurden für allgemeine Bildverarbeitung und <a href=\"https://matplotlib.org/\">Matplotlib</a> sowie <a href=\"https://numpy.org/\">Numpy</a> für Visualisierung und Datenmanagement genutzt. PlaidML wurde für die Implementierung gewählt, da mit dieser Bibliothek das Training mit Grafikkarten von allen gängigen Herstellern beschleunigt werden kann. Dadurch, dass relativ viele Trainingsepisoden (70000 im Beispiel oben) durchgeführt werden müssen, benötigt diese Lösung zum Training die Beschleunigung durch eine Grafikkarte. Als Basis für die Implementierung wurden zwei Paper zu <a href=\"https://arxiv.org/pdf/1511.06434.pdf\">Representation Learning mit GANs</a> und <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-59050-9_12\">Anomalieerkennung mittels GANs</a> sowie zwei Implementierungen auf Github <a href=\"https://github.com/tkwoo/anogan-keras\">[1]</a>, <a href=\"https://github.com/neverrop/anogan_1\">[2]</a> verwendet.\n",
    "\n",
    "### Aufbau der Applikation\n",
    "\n",
    "Das Skript [*prepareWorkspace.sh*](./app/prepareWorkspace.sh) installiert automatisch alle benötigten Software Komponenten in einem Python 3 Virtual Environment, lädt die Trainingsdaten herunter und bearbeitet die Trainingsbilder vor. Das Skript führt dabei auch die Konfiguration von PlaidML durch, bei welcher Sie einfach den Anweisungen am Terminal folgen können. Dabei kann die Grafikkarte als Methode zur Beschleunigung des Trainings ausgewählt werden. Bitte beachten Sie, dass das Training selbst mit Grafikkarte mehrere Stunden dauert.\n",
    "\n",
    "\n",
    "Anschließend kann das [trainModels.py](./app/trainModels.py) Python Skript ausgeführt werden, um das Generative Adversarial Network anhand der heruntergeladenen Bilder zu trainieren.\n",
    "\n",
    "Nach erfolgreichem Training können sie das [anomalyDetection.py](./app/anomalyDetection.py) Skript ausführen, um ein Bild aus den Testdaten zu laden und auf Beschädigungen zu untersuchen. Die Ergebnisse werden vom Programm visualisiert.\n",
    "\n",
    "Das Skript [*deleteWorkspace.sh*](./app/deleteWorkspace.sh) löscht den Workspace sowie alle heruntergeladenen und generierten Dateien.\n",
    "\n",
    "\n",
    "### Vorbereitung der Bilder\n",
    "\n",
    "Zunächst werden alle benötigten Module importiert und einige Funktionen zur Bildverarbeitung implementiert. _readImageDir_ liest alle Bilder in einem Verzeichnis ein, _clusterImage_ wendet <a href=\"https://docs.opencv.org/4.5.2/d1/d5c/tutorial_py_kmeans_opencv.html\">K-Means Clustering</a> auf ein Bild an um ein Bild in eine Bestimmte Anzahl an Bereichen aufzuteilen, _getBiggestContour_ ermittelt die größte durchgehende Kontur in einem Bild, _getMask_ ermittelt die Maske zur Hintergrundentfernung und _removeBackground_ wendet diese Maske an, um den Hintergrund zu entfernen.\n",
    "\n",
    "_preProcessImages_ führt die Konvertierung zu Graustufen, Skalierung und optionale Hintergrundentfernung für alle eingelesenen Bilder durch. Skalierung und Konvertierung zu Graustufen werden dabei mittels OpenCV durchgeführt. Für die Hintergrundentfernung wird das skalierte Graustufenbild zunächst auf Konturen im Bild untersucht. Diese werden eingezeichnet und gefüllt, um die Schraube grob vom Hintergrund zu tennen. Da dabei aber durch die Belichtung einige Artefakte entstehen, wenden wir Unschärfe auf das Bild an und verwenden K-Means Clustering, um das Bild in zwei Bereiche aufzuteilen. Diese Bereiche werden mittels Konturerkennung noch einmal klar getrennt, um den Hintergrund bestmöglich von der Schraube zu unterscheiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026cfb7c-8879-40d9-a754-b00b18de72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Anlegen der Verzeichnisse für Trainingsdaten, generierte Bilder und Modelle\n",
    "def createDir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "createDir('./processedImages')\n",
    "createDir('./genImages')\n",
    "createDir('./savedModels')\n",
    "\n",
    "#############################################################\n",
    "#####           Bildverarbeitungsfunktionen             #####\n",
    "\n",
    "def readImageDir(path, scaleFactor=2):\n",
    "    \"\"\" Liest alle Bilder in einem Verzeichnis ein und skaliert sie \"\"\"\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    imageData = []\n",
    "    # Jedes Bild wird eingelesen und auf 128x128 Pixel Skaliert\n",
    "    # Die Skalierung ist notwendig, um die Performance zu erhöhen\n",
    "    for filename in files:\n",
    "        # Einlesen der Bilddatei\n",
    "        filename = '{}/{}'.format(path, filename)\n",
    "        img = cv2.imread(filename)\n",
    "        width = int(img.shape[0]/scaleFactor)\n",
    "        height = int(img.shape[1]/scaleFactor)\n",
    "        img = cv2.resize(img, (width, height), fx=0, fy=0, interpolation=cv2.INTER_LINEAR)\n",
    "        imageData.append(np.asarray(img))\n",
    "    # Alle Bilder werden in einem Numpy Array gespeichert\n",
    "    return np.array(imageData)\n",
    "\n",
    "\n",
    "def clusterImage(frame, K=2):\n",
    "    \"\"\" Verwendet OpenCV K-Means Clustering um ein Bild in K Bereiche aufzuteilen \"\"\"\n",
    "    # Konvertierung des Graustufenbildes in ein Numpy float32 Array\n",
    "    Z = frame.reshape((-1,1))\n",
    "    Z = np.float32(Z)\n",
    "    # Festlegen, wie viele Cluster gesucht werden (in diesem Fall 2)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    # Durchführung des K-Means Clusterings\n",
    "    _,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_PP_CENTERS)\n",
    "    # Zurückkonvertierung in uint8 (natives Bildformat) \n",
    "    # und Generierung des Ausgabebildes\n",
    "    center = np.uint8(center)\n",
    "    res = center[label.flatten()]\n",
    "    res2 = res.reshape((frame.shape))\n",
    "    return res2\n",
    "\n",
    "\n",
    "def getBiggestContour(img_bw):\n",
    "    \"\"\" Gibt die Kontur mit der größten Fläche im Bild zurück \"\"\"\n",
    "    img_bw = img_bw.copy()\n",
    "    # Ermittlung aller Konturen im Bild\n",
    "    _, thresh = cv2.threshold(img_bw, 127, 255, 0)\n",
    "    #contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    contours, _ = cv2.findContours(thresh, 1, 1)\n",
    "    # Da wir die Ganze Schraube abdecken wollen, \n",
    "    # zeichnen wir die Kontur mit der größten Fläche in der Maske ein\n",
    "    return cv2.drawContours(img_bw, [max(contours, key = cv2.contourArea)], -1, 255, thickness=-1)\n",
    "\n",
    "\n",
    "def getMask(img_in, blur=20):\n",
    "    \"\"\" Erstellt eine Maske, welche im Bild die Schraube vom Tisch trennt \"\"\"\n",
    "    img = img_in.copy()\n",
    "    # Erkennung von Konturen im Bild\n",
    "    _, thresh = cv2.threshold(img, 127, 255, 0)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    # Einzeichnen der Konturen\n",
    "    img_cont = cv2.drawContours(img, contours, -1, (0,255,0), -1)\n",
    "    # Blur des Bildes um Artefakte zu beseitigen\n",
    "    img_blur = cv2.blur(img_cont,(blur,blur))\n",
    "    # K-Means Clustering, um eine Maske zu bekommen\n",
    "    img_bw = clusterImage(img_blur, K=2)\n",
    "    # Vordergrund = Weiß, Hintergrund = Schwarz\n",
    "    img_bw[img_bw!=0] = 255\n",
    "    # Füllen der Maske um Artefakte zu vermeiden\n",
    "    return getBiggestContour(img_bw)\n",
    "\n",
    "\n",
    "def removeBackground(img_in):\n",
    "    \"\"\" Entfernt den Hintergrund der Schrauben \"\"\"\n",
    "    # Pipeline Hintergrundentfernung:\n",
    "    #   Konturen -> K-Means Clustering -> Konturen & nur größte Fläche übernehmen -> Kontrast erhöhen\n",
    "    mask = getMask(img_in)\n",
    "    img_out = img_in.copy()\n",
    "    img_out[mask==0]=255\n",
    "    return img_out\n",
    "\n",
    "\n",
    "def preProcessImages(images, scaleFactor=2, rmBG=False):\n",
    "    \"\"\" Führt die Vorverarbeitung der Bilder für das Training durch \"\"\"\n",
    "    outImages =  []\n",
    "    for img in images:\n",
    "        # Konvertierung zu Graustufen\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Entfernen des Hintergrunds\n",
    "        if rmBG: img = removeBackground(img)\n",
    "        # Skalierung\n",
    "        width = int(img.shape[0]/scaleFactor)\n",
    "        height = int(img.shape[1]/scaleFactor)\n",
    "        img = cv2.resize(img, (width, height), fx=0, fy=0, interpolation=cv2.INTER_LINEAR)\n",
    "        outImages.append(img)\n",
    "    return np.array(outImages)\n",
    "\n",
    "def normaliseImageData(images):\n",
    "    \"\"\" Konvertiert die Bilder von [0;255] zu [-1,1] mit Median=0 & Standardabweichung=1 \"\"\"\n",
    "    images = (images / 127.5) - 1\n",
    "    outImages = []\n",
    "    for x in images:\n",
    "        x -= np.mean(x)\n",
    "        x /= np.std(x)\n",
    "        outImages.append(x)\n",
    "    return np.array(outImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cc37d-839c-47ae-acbe-628dc91801cb",
   "metadata": {},
   "source": [
    "Wir lesen nun die Trainingsbilder ein und bereiten diese mittels den oben vorgestellten Bildverarbeitungsfunktionen vor. Dabei werden die Bilder auf eine Auflösung von 32x32 Pixel skaliert. Diese Skalierung ist notwendig um die Benötigte Rechenleistung für das Training so gering zu halten, dass es auf normalen PCs mit Grafikkarte durchfürhbar ist. Anschließend werden die Bilder als numpy Datei abgespeichert, damit sie im Trainingsskript verwendet werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753128b3-462d-47be-8b15-94d4fccb7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#####       Einlesen der Trainings- und Testdaten       #####\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Liest alle Bilder ein, bearbeitet sie vor und speichert sie als Numpy Datei ab.  \"\"\"\n",
    "    # Import aller Bilder\n",
    "    trainDataPath = './screw/train/good'\n",
    "    trainImages = readImageDir(trainDataPath, scaleFactor=1)\n",
    "    # Die Bilder werden in Graustufen umgewandelt und um scaleFactor skaliert.\n",
    "    # Optional kann noch der Hintergrund der Bilder mittels K-Means Clustering entfernt werden (rmBG=True)\n",
    "    trainData = preProcessImages(trainImages, scaleFactor=16, rmBG=False)\n",
    "    # Abspeicherung der Bilder\n",
    "    np.save('./processedImages/trainData', trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b385d-1955-4247-b859-e01d973223b5",
   "metadata": {},
   "source": [
    "### Training des Generative Adversarial Networks\n",
    "\n",
    "Nach Vorbearbeitung der Bilder werden Generator, Diskriminator und ein kombiniertes Modell aus den beiden mittels Keras definiert. Der Generator nimmt dabei einen Vektor der Länge 128 als Eingang und ein Graustufenbild mit einer Auflösung von 32x32 Pixeln als Ausgang. Dieser Eingangsvektor wird auch latente Variable genannt und mit zufälligen Werten aus einer Gleichverteilung gefüllt. Aus diesen Werten soll der Generator Bilder erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b254a-f648-4d25-a3fa-eae05fb983d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(latentDim=128, imgShape=(32, 32, 1)):\n",
    "    \"\"\" Gibt den Generator als Keras Modell zurück. \"\"\"\n",
    "    # Eingang: Vector mit Länge 128\n",
    "    # Ausgang: Graustufenbild (32, 32, 1)\n",
    "    model =  Sequential(\n",
    "        [\n",
    "            # Eingang: 128\n",
    "            Dense(1024, input_dim=latentDim),\n",
    "            Activation('relu'),\n",
    "            # Reshape zu Bild mit Kanälen\n",
    "            Dense(128*8*8),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Reshape((8,8,128)),\n",
    "            # 8x8\n",
    "            Conv2DTranspose(64, (2,2), strides=(2,2), padding='same'),\n",
    "            Conv2D(64, (5,5), padding='same'),\n",
    "            Activation('relu'),\n",
    "            # 16x16\n",
    "            Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same'),\n",
    "            Conv2D(1, (5, 5), padding='same'),\n",
    "            # Ausgang: 32x32\n",
    "            Activation('tanh')\n",
    "        ],\n",
    "        name=\"generator\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943b9f9-3f1e-49d1-8096-b2426afb05c8",
   "metadata": {},
   "source": [
    "Der Diskriminator nimmt ein 32x32 Graustufenbild als Eingang und hat einen einzigen Wert als Ausgang. Dieser Ausgangswert gibt an, ob der Diskriminator das eingegebene Bild als echt oder unecht einschätzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c33143f-6243-4b04-9581-f837984dde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(imgShape=(32, 32, 1)):\n",
    "    \"\"\" Gibt den Diskriminator als Keras Modell zurück. \"\"\"\n",
    "    # Eingang: Graustufenbild (32, 32, 1)\n",
    "    # Ausgang: Einschätzung, ob Bild echt ist, oder nicht\n",
    "    model =  Sequential(\n",
    "        [\n",
    "            # Eingang: 64x64\n",
    "            Conv2D(64, (5, 5), input_shape=imgShape, padding='same'),\n",
    "            LeakyReLU(),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Conv2D(128, (5, 5), padding='same'),\n",
    "            LeakyReLU(),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(1024),\n",
    "            LeakyReLU(),\n",
    "            # Ausgang: 1\n",
    "            Dense(1),\n",
    "            Activation('sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d2fdf-e307-4c78-baa3-2922f2693771",
   "metadata": {},
   "source": [
    "Das Kombinierte Modell setzt Generator und Diskriminator zusammen. Damit nimmt es die latente Variable als Eingang und die Einschätzung, ob das Bild echt oder unecht ist, als Ausgang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e74caa-53c9-4918-8def-d6e3b3bb35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorContainingDiscriminator(g, d, latentDim=128):\n",
    "    \"\"\" Kombiniertes Modell zum Training des Generators \"\"\"\n",
    "    ganInput = Input(shape=(latentDim,))\n",
    "    x = g(ganInput)\n",
    "    ganOutput = d(x)\n",
    "    gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4309376-6d5e-4e3f-882c-c5dee0216884",
   "metadata": {},
   "source": [
    "Anschließend implementieren wir die GAN Klasse. Diese instanziert und trainiert die oben gezeigten Modelle. Die durchgeführten Schritte sind dabei in jeder Trainingsepoche gleich: Zufällige Werte für die latente Variable werden ermittelt. Aus diesen erzeugt der Generator Bilder. Diese werden dann gemeinsam mit Bildern aus dem Trainingsdatensatz zum training des Diskriminators verwendet. Dabei markieren wir die generierten Bilder als unecht (0) und die Bilder aus den Trainingsdaten als echt (1).\n",
    "\n",
    "Dannach nehmen wir die Gewichte des Diskriminator als nicht trainierbar an, um mittels des kombinierten Modells den Generator zu trainieren. Dabei geben wir die zufälligen Werte der latenten Variable als Eingang und die Einschätzung, das Bild sei echt, als Ausgang an. Da der Diskriminator im gemeinsamen Modell nicht trainierbar ist, muss der Generator also lernen realistischere Bilder zu erzeugen, damit der gewünschte Ausgangswert erreicht wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b1a22-49fe-4bac-98be-66f621c52944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, imgRows=64, imgCols=64, imgChannels=1, loadModel=False):\n",
    "        # Festlegen des Bildformats\n",
    "        self.imgShape = (imgRows, imgCols, imgChannels)\n",
    "        self.latentDim = 128\n",
    "        self.epoch = 0\n",
    "        if not loadModel:\n",
    "            # Erstellen von Generator und Diskriminator\n",
    "            d = discriminator(imgShape=self.imgShape)\n",
    "            g = generator(latentDim=self.latentDim, imgShape=self.imgShape)\n",
    "            # Instanzierung der Optimiser\n",
    "            d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "            g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "            # Festlegen der Diskriminator Schichten als \"nicht trainierbar\"\n",
    "            # Dies ist notwendig, damit beim Kombinierten Modell nur der Generator traininert wird\n",
    "            #for layer in d.layers: layer.trainable = False\n",
    "            d.trainable = False\n",
    "            # Bauen des kombinierten Modells\n",
    "            d_on_g = generatorContainingDiscriminator(g, d, self.latentDim)\n",
    "            # Kompilation des Generators und kombinierten Modells\n",
    "            g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "            d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "            print('Generator:')\n",
    "            g.summary()\n",
    "            print('Kombiniertes Modell:')\n",
    "            d_on_g.summary()\n",
    "            # Kompilation des Diskriminators\n",
    "            d.trainable = True\n",
    "            d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "            print('Diskriminator:')\n",
    "            d.summary()\n",
    "        else:\n",
    "            pass\n",
    "        # Speichern der Modelle als Klassenatribute\n",
    "        self.d = d\n",
    "        self.g = g\n",
    "        self.d_on_g = d_on_g\n",
    "    def train(self, epochs, trainData, batch_size=128, sample_interval=100):\n",
    "        \"\"\" Trainiert das GAN eine bestimmte Anzahl an Epochen. \"\"\"\n",
    "        # Vorbereitung der Trainingsdaten\n",
    "        X_train = trainData / 127.5 - 1.\n",
    "        if self.imgShape[2] == 1: X_train = np.expand_dims(X_train, axis=3)\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        y = np.concatenate((valid, fake))\n",
    "        nIter = int(X_train.shape[0]/batch_size)\n",
    "        # Anzeigen des Fortschritt mittels eines Progress Bars\n",
    "        progressBar = Progbar(target=self.epoch + epochs)\n",
    "        for self.epoch in range(self.epoch, self.epoch + epochs + 1):\n",
    "            # Probenentnahme aus den Trainingsdaten\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            # Zufällige Werte für Latente Variable\n",
    "            noise = np.random.uniform(-1, 1, (batch_size, self.latentDim))\n",
    "            # Generierung künstlicher Bilder\n",
    "            gen_imgs = self.g.predict(noise)\n",
    "            # Training des Diskriminators\n",
    "            X = np.concatenate((imgs, gen_imgs))\n",
    "            d_loss = self.d.train_on_batch(X, y)\n",
    "            # Training des Generators\n",
    "            self.d.trainable = False\n",
    "            g_loss = self.d_on_g.train_on_batch(noise, valid)\n",
    "            self.d.trainable = True\n",
    "            # Werden mehrere Metriken überprüft, ermitteln wir aus dem Array den Loss\n",
    "            if isinstance(g_loss, np.ndarray): g_loss = g_loss.item()\n",
    "            if isinstance(d_loss, np.ndarray): d_loss = d_loss.item()\n",
    "            # Anzeigen des Trainingsfortschritts\n",
    "            progressBar.update(self.epoch, values=[('D loss',d_loss), ('G loss',g_loss)])\n",
    "            # Ausgabe der Beispielbilder und Speichern der Modelle\n",
    "            if self.epoch % sample_interval == 0:\n",
    "                self.sample_images(self.epoch)\n",
    "                self.saveModels()\n",
    "    def saveModels(self):\n",
    "        \"\"\" Speichert die Keras Modelle und Anzahl der trainierten Epochen. \"\"\"\n",
    "        self.g.save('./savedModels/generator')\n",
    "        self.d.save('./savedModels/discriminator')\n",
    "        self.d_on_g.save('./savedModels/combined')\n",
    "        np.save('./savedModels/epoch', self.epoch)\n",
    "    def sample_images(self, epoch):\n",
    "        \"\"\" Speichert aus dem Generator generierte Beispielbilder. \"\"\"\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.uniform(-1, 1, size=(r * c, self.latentDim))\n",
    "        gen_imgs = self.g.predict(noise)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"genImages/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "    def print_input_images(self, imgs, name):\n",
    "        \"\"\" Speichert Trainingsbilder.  \"\"\"\n",
    "        if not isinstance(imgs, np.ndarray): imgs = np.array(imgs)\n",
    "        r, c = 5, 5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        try:\n",
    "            for i in range(r):\n",
    "                for j in range(c):\n",
    "                    axs[i,j].imshow(imgs[cnt, :,:], cmap='gray')\n",
    "                    axs[i,j].axis('off')\n",
    "                    cnt += 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "        fig.savefig(\"genImages/\" + name + \".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3836500-4842-40a9-905a-2727f502647a",
   "metadata": {},
   "source": [
    "Da alle benötigten Komponenten implementiert sind, kann das Training starten. Dazu werden die Trainingsdaten geladen und die GAN Klasse instanziert. Die GAN Klasse speichert dann eine Visualisierung der Trainingsdaten als Bild ab, bevor das Training gestartet wird. Wir trainieren das GAN für 70000 Episoden und erzeugen in jeder Episode 64 Bilder. Eine Visualisierung der generierten Bilder, sowie die aktuellen Modelle werden alle 5000 Episoden abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453112c2-6cde-40cc-9f9b-e48866af5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#####                Training des GANs                  #####\n",
    "\n",
    "# Laden der Vorbearbeiteten Bilder\n",
    "trainData = np.load('./processedImages/trainData.npy')\n",
    "\n",
    "# Initialisierung des GAN Modells\n",
    "gan = GAN(imgCols=len(trainData[0][0]), imgRows=len(trainData[0]), imgChannels=1, loadModel=False)\n",
    "\n",
    "# Plot der vorab bearbeiteten Trainings- und Testdaten\n",
    "gan.print_input_images(trainData, \"trainData\")\n",
    "\n",
    "# Training des GAN\n",
    "gan.train(trainData=trainData, epochs=70000, batch_size=64, sample_interval=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07c9ae-20c1-40e6-a5f8-800c90f46199",
   "metadata": {},
   "source": [
    "### Erkennung von Anomalien\n",
    "Nach erfolgreichem Training können Generator und Diskriminator zur Erkennung von beschädigten Schrauben eingesetzt werden. Dazu importieren wir zunächst die benötigten Module und implementieren einige Hilfsfunktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f2bc7-3ed5-43ab-8301-453f5026abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setzen von PlaidML als Backend\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# Import der benötigten Module\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Reshape, Dense, Dropout, MaxPooling2D, Conv2D, Flatten\n",
    "from keras.layers import Conv2DTranspose, LeakyReLU, Activation, BatchNormalization, ZeroPadding2D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.utils.generic_utils import Progbar\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import trainModels\n",
    "from trainModels import generator, discriminator\n",
    "\n",
    "from preProcessImages import preProcessImages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def genImages(batch_size=32, latentDim=128):\n",
    "    \"\"\" Lädt den trainierten Generator und generier batch_size an Testbildern \"\"\"\n",
    "    g = load_model('./savedModels/generator')\n",
    "    noise = np.random.uniform(0, 1, (batch_size, latentDim))\n",
    "    return g.predict(noise)\n",
    "\n",
    "def anomalyLossFunc(yTrue, yPred):\n",
    "    \"\"\" Wendet die Loss Funktion aus AnoGAN an \"\"\"\n",
    "    return K.sum(K.abs(yTrue - yPred))\n",
    "\n",
    "def featureExtractor():\n",
    "    \"\"\" Kompilliert und gibt ein Modell zur Feature Extraction ausgehend vom Diskriminator zurück \"\"\"\n",
    "    d = load_model('./savedModels/discriminator')\n",
    "    intermidiateModel = Model(inputs=d.layers[0].input, outputs=d.layers[-5].output)\n",
    "    intermidiateModel.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "    return intermidiateModel\n",
    "\n",
    "def anomalyDetector(latentDim=128):\n",
    "    \"\"\" Setzt Feature Extractor und Generator zusammen, um das Modell zur Anomalieerkennung zu erhalten \"\"\"\n",
    "    g = load_model('./savedModels/generator')\n",
    "    intermidiateModel = featureExtractor()\n",
    "    intermidiateModel.trainable = False\n",
    "    #g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)\n",
    "    g.trainable = False\n",
    "    # \n",
    "    aInput = Input(shape=(latentDim,))\n",
    "    gInput = Dense((latentDim), trainable=True)(aInput)\n",
    "    #gInput = Activation('sigmoid')(gInput)\n",
    "    # \n",
    "    G_out = g(gInput)\n",
    "    D_out= intermidiateModel(G_out)    \n",
    "    model = Model(inputs=aInput, outputs=[G_out, D_out])\n",
    "    model.compile(loss=anomalyLossFunc, loss_weights= [0.90, 0.10], optimizer='adam')\n",
    "    #\n",
    "    return model\n",
    "\n",
    "def getAnomalyScore(model, x, nIter=500, batchSize=64, latentDim=128):\n",
    "    \"\"\" Berechnet den Anomalie Score eines Bildes und findet Bildregionen die zum Modell passen \"\"\"\n",
    "    z = np.random.uniform(-1, 1, (batchSize, latentDim))\n",
    "    intermidiateModel = featureExtractor()\n",
    "    d_x = intermidiateModel.predict(x)\n",
    "    loss = model.fit(z, [x, d_x], batch_size=batchSize, epochs=nIter, verbose=0)\n",
    "    similar_data, _ = model.predict(z)\n",
    "    loss = loss.history['loss'][-1]\n",
    "    return loss, similar_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd28ac-176c-403e-a108-f1505a5156d9",
   "metadata": {},
   "source": [
    "Anschließend öffnen wir die Testbilder, bearbeiten diese vor und laden die trainieren Modelle. Dabei wird auf Basis von Generator und Diskriminator ein neues Modell trainiert, welches verschiedenen Bildbereichen einen Anomalie Score zuweist.\n",
    "\n",
    "Die Einschätzung, ob es sich bei bestimmten Bildbereichen um Anomalien handelt, wird anschließend mittels Matplotlib visualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95559dc6-0743-4512-80c9-b4c956dbe808",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Trainiert einen AnoGAN Anomalie Detektor basierend auf Generator und Diskriminator \"\"\"\n",
    "    # Öfnnen und Vorbearbeiten eines Testbildes\n",
    "    testImage = cv2.imread('./screw/test/manipulated_front/003.png')\n",
    "    Image.fromarray(testImage).show()\n",
    "    testImage = preProcessImages([testImage], scaleFactor=32)\n",
    "    testImage = np.squeeze(testImage, axis=0)\n",
    "    Image.fromarray(testImage).show()\n",
    "    #\n",
    "    # Instanzierung des Modells\n",
    "    model = anomalyDetector()\n",
    "    score, similarities = getAnomalyScore(model, testImage.reshape(1, 32, 32, 1), batchSize=1)\n",
    "    #\n",
    "    # Visualisierung der gefundenen Bildbereiche\n",
    "    plt.imshow(testImage.reshape(32,32), cmap=plt.cm.gray)\n",
    "    residual  = testImage.reshape(32,32) - similarities.reshape(32, 32)\n",
    "    plt.imshow(residual, cmap='Reds', alpha=.5)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
