{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d894d87-8816-41c7-ad9c-a87308ab8e6e",
   "metadata": {},
   "source": [
    "# Sprachverarbeitung für Roboter: Robbi, mach mir einen Kaffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb9399-2740-4f71-83b5-6ac7d7180ef0",
   "metadata": {},
   "source": [
    "## Storyboard\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "    Im 5 Stock des Technikum Wien haben wir einen Roboter, welcher die Mitarbeiter automatisch mit Kaffee versorgt. Die Mitarbeiter geben dabei Bestellungen über eine Benutzeroberfläche auf und der Roboter fährt zum jeweiligen Tisch, um den Kaffee zuzubereiten.\n",
    "    <br /><br />\n",
    "    \n",
    "    Nun wollen wir die Interaktion zwischen Nutzer und Roboter natürlicher gestalten, indem wir Bestellungen mittels Sprache ermöglichen. Ähnlich zu gängigen Sprachassistenten, wie z.B. <a href=\"https://www.amazon.de/b?ie=UTF8&node=12775495031\">Amazon Alexa</a> oder dem <a href=\"https://assistant.google.com/\">Google Assistant</a>, soll der CoffeeBot in der Lage sein, gesprochene Bestellungen aufzunehmen und zu verarbeiten. Aber wie kann ein Roboter gesprochene Sprache so verarbeiten, dass er sie versteht?\n",
    "    <br /><br />\n",
    "    \n",
    "    Zur Verarbeitung von Sprache muss diese erst aufgenommen werden. Neben einfachen Mikrofonen, welche direkt an den PC angeschlossen werden, gibt es speziell für die Anwendung der Spracherkennung ausgelegte Hardware. Solche sogenannten Microphone Arrays bestehen aus mehreren Mikrofonen, um effektiv Befehle aus verschiedenen Richtungen wahrnehmen und Hintergrundgeräusche von der gesprochenen Sprache trennen zu können. Ein Beispiel für eine offene Plattform für solche Mikrofone ist das <a href=\"https://wiki.seeedstudio.com/ReSpeaker_Mic_Array/\">Mic Array von ReSpeaker</a>. Nach Aufnahme der Sprache muss diese von Audio- in Textform gebracht werden. Diesen Vorgang nennt man Speech to Text oder Spracherkennung. Letztendlich wenden wir Natural Language Processing (NLP, siehe AIAV Video <a href=\"https://www.youtube.com/watch?v=B-weR5ngYIU&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=24\">Sprachverarbeitung</a>) an. NLP hat das Ziel, mittels Statistik den Sinn von Text zu verstehen. Wenn wir den Sinn des Textes verstehen können, können wir diesen als Grundlage für Roboteraktionen verwenden.\n",
    "    <br /><br />\n",
    "    \n",
    "    Abbildung 1 zeigt den Ablauf, wie wir von gesprochener Sprache zu einem Befehl, welcher der Roboter verarbeiten kann, kommen. Mit Natural Language Processing haben wir uns dabei schon im AIAV Usecase <i>Erkläre eine Robotikkonferenz... mit AI!</i> beschäftigt. Also fehlt uns für das volle Verständnis der Pipeline die Übersetzung von gesprochener Sprache zu Text. \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af6508-5d2b-44c4-865a-bead55c2013d",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung1Ablauf.png\" alt=\"Drawing\" style=\"width: 1100px;\"/>\n",
    "\n",
    "_Abbildung 1: Für die Verarbeitung von gesprochener Sprache müssen wir diese zunächst aufnehmen und in Text übersetzen (Spech to Text). Anschließend wird der Text ausgewertet (Natural Language Processing) und verstanden, um einen Roboterbefehl erzeugen zu können._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82bcb5-5d5e-402d-b6ea-2e6f882e6844",
   "metadata": {},
   "source": [
    "### Spracherkennung\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "    Ziel der Spracherkennung ist es, gesprochene Sprache in Audioform zu Text zu übersetzen, der einen Befehl darstellt. Die Grundsätzliche Idee hinter Text to Speech ist es, das Audiosignal in kleine Ausschnitte aufzuteilen. Diese Ausschnitte werden dann mit in Frage kommenden Kombinationen von bekannten Worten verglichen, um eine Phrase zu ermitteln, welche die bestmögliche Übereinstimmung mit dem Audioausschnitt hat. Um diese Übereinstimmung zu finden, werden Merkmale, sogenannte Features, des Audioausschnitts ermittelt. Diese Merkmale werden dann in ein Modell, z.B. einem Neuronalen Netz (siehe AIAV Video <a href=\"https://www.youtube.com/watch?v=pmfIJ3XUw2c&list=PLfJEPw9Zb0EPLEZZlNCQc9F3F7RWG6EsK&index=37\">Neuronale Netze</a>), gegeben, welches die Merkmale analysiert (siehe Abbildung 2).\n",
    "    <br /><br />\n",
    "    \n",
    "    Die praktische Umsetzung dieser Idee wird aber durch mehrere Faktoren erschwert. Einerseits ist es nicht möglich, alle Kombinationen aller bekannten Wörter zu prüfen, da der benötigte Rechenaufwand viel zu groß wäre. Deswegen wird neben dem Ausschnitt des Audiosignals auch noch der Kontext berücksichtigt. Dieser Kontext kann von bereits erkannten Phrasen, aber auch von manuell angegebenen Wörtern kommen. Mittels des Kontexts wird die Suche eingegrenzt, um die Spracherkennung zu beschleunigen.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5013b7-946e-4db6-8547-6823c468a379",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung2Spracherkennung.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n",
    "_Abbildung 2: Für die Durchführung der Spracherkennung werden Ausschnitte aus der Audioaufnahme anhand bestimmter Merkmale (Features) untersucht. Dabei werden die Merkmale zusammen mit aus dem sprachlichen Kontext erzeugten Wortkombinationen in drei Modelle gegeben, um die Übereinstimmung zwischen Wortkombination und Audio zu erhalten. Die Wortkombination mit der besten Übereinstimmung wird dann als erkannter Text zurückgegeben._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa74bea-b9dc-4b91-b9d8-f54419b36f8e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "    Neben der Geschwindigkeit, mit der Text aus Sprache erkannt wird, ist auch ein Problem, wie die Übereinstimmung zwischen Ausschnitten der Audiodatei und Wortkombinationen überhaupt gefunden wird. Die Dokumentation von <a href=\"https://cmusphinx.github.io/\">CMUSphinx</a>, einer offene Plattform zur Spracherkennung, beinhaltet eine detaillierte <a href=\"https://cmusphinx.github.io/wiki/tutorialconcepts/\">Beschreibung</a>, wie diese Übereinstimmung gefunden wird. Die dort vorgestellte Lösung basiert auf drei Modellen: einem akustischen Modell, einem sprachlichen Modell und einem Modell, welches sich mit dem Zusammenhang zwischen Wörtern und ihren Lauten beschäftigt. Das akustische Modell beinhaltet dabei die Merkmale (Features) der Ausschnitte, während das sprachliche Modell aus dem Kontext vorgibt, welche Wörter abgesucht werden. Die Ergebnisse aus allen drei Modellen werden kombiniert, um jeden überprüften Ausschnitt zu bewerten und so die gesprochene Sprache zu erkennen. Das Resultat ist eine Liste von Kandidaten mit absteigender Bewertung.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838836a-1b69-47b0-85c4-674541baf472",
   "metadata": {},
   "source": [
    "### Praktische Implementierung\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "    Für die praktische Implementierung des Use Cases wollen wir Sprachbefehle aufnehmen, in Text umwandeln und so auswerten, dass eine Bestellung automatisch aufgegeben werden kann. Dabei sollte es möglich sein, mehrere Kaffee auf einmal zu bestellen und zwischen den beiden möglichen Kaffeearten (Espresso und verlängerter Kaffee) zu unterscheiden.\n",
    "    <br /><br />\n",
    "    \n",
    "    Um Sprache aufzunehmen und in Text umzuwandeln wurde das <a href=\"https://pypi.org/project/SpeechRecognition/\">SpeechRecognition</a> Modul für Python verwendet. Dieses Modul bietet Verbindungen zu gängigen Schnittstellen zur Spracherkennung. Damit kann z.B. das oben erwähnte <a href=\"https://cmusphinx.github.io/\">CMUSphinx</a> verwendet werden, um lokal Spracherkennung durchzuführen. Es besteht auch die Möglichkeit, die Spracherkennung online, durch Anbieter wie z.B. Google oder Microsoft, durchführen zu lassen.\n",
    "    <br /><br />\n",
    "    \n",
    "    Abbildung 2 zeit den Programmablauf. Zunächst wird der Befehl aufgezeichnet und von Sprache in Text umgewandelt. Die Umwandlung erfolgt mittels der <a href=\"https://cloud.google.com/speech-to-text#section-3\">Google Webspeech API</a>, da diese keine Installation außerhalb der Python Komponenten erfordert und ohne Registrierung gratis verwendet werden kann. Da hier die Spracherkennung in der Cloud durchgeführt wird, ist allerdings eine Internetverbindung notwendig. Nach Ermittlung des Textes führen wir Natural Language Processing, ähnlich wie im AIAV Use Case <i>Erkläre eine Robotik Konferenz... mit AI!</i>, durch. Dabei wenden wir Regular Expressions an um zu zählen, wie oft verschiedene Wörter wie z.B. Espresso oder Kaffee im Text vorkommen. Anhand der Anzahl der vorkommenden Wörter wird dann entschieden, welcher Kaffee bestellt wird. Zusätzlich zur Kaffeeart wird auf die gleiche Weise entschieden, wie viele Kaffee bestellt werden.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa301e-23d0-40e8-bd8d-2ee837198803",
   "metadata": {},
   "source": [
    "<img src=\"images/Abbildung3Programmablauf.png\" alt=\"Drawing\" style=\"width: 1100px;\"/>\n",
    "\n",
    "_Abbildung 3: Um gesprochene Bestellungen zu verarbeiten, wird der aufgenommene Ton zunächst in Textform gebracht. Danach wird im Text gezählt, wie oft bestimmte Schlüsselwörter für die beiden Kaffeearten und Zahlen vorkommen. Die Kaffeeart, die am öftesten vorkommt, wird so oft bestellt, wie die Zahl die am öftesten vorkommt._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8579f-237d-41df-aa05-aaa9dc20a970",
   "metadata": {},
   "source": [
    "### Fazit\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "Spracherkennung erlaubt es uns, benutzerfreundliche Steuerungen von Robotern mittels Sprache zu realisieren. Bereits vorgefertigte Modelle wie die <a href=\"https://cloud.google.com/speech-to-text#section-3\">Google Webspeech API</a> ermöglichen performante Lösungen, welche wenig Implementierungsaufwand benötigen. Leider sind die meisten gängigen Spracherkennungsmodelle proprietär und kostenpflichtig (z.B. <a href=\"https://azure.microsoft.com/en-us/services/cognitive-services/speech-services/\">Microsoft Bing Speech</a>, <a href=\"https://cloud.google.com/speech-to-text\">Google Cloud Speech</a> oder <a href=\"https://www.ibm.com/cloud/watson-speech-to-text\">IBM Speech to Text</a>). Es gibt aber auch offene Lösungen, wie <a href=\"https://cmusphinx.github.io/\">CMUSphinx</a> oder <a href=\"https://github.com/mozilla/DeepSpeech\">Mozilla's Project DeepSpeech</a> die lokal ausgeführt werden.\n",
    "    <br /><br />\n",
    "    \n",
    "    Diese Implementierung von Natural Language Processing resultiert in einigen Grenzen der Applikation. Die Google Webspeech API unterstützt mehrere Sprachen. Die zu erkennende Sprache muss aber im Vorhinein festgelegt werden; man kann also bei diesem Use Case ohne den Code zu ändern nur auf Deutsch bestellen. Zusätzlich dazu kann pro Bestellung nur eine der möglichen Kaffeearten bestellt werden. Diese Limitierung kommt daher, dass mittels Schlagwörtern nach den Kaffeearten gesucht wird. Die Kaffeeart mit den am öftesten vorkommenden Schlagwörtern wird dann bestellt.\n",
    "    </div>\n",
    "    \n",
    "<P style=\"page-break-before: always\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c2cb3-b4f8-4852-b448-90dd6c97edaf",
   "metadata": {},
   "source": [
    "## Codedokumentation\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "    Die Implementierung basiert auf dem <a href=\"https://pypi.org/project/SpeechRecognition/\">SpeechRecognition</a> Modul für <a href=\"https://www.python.org/downloads/\">Python 3</a> (siehe <a href=\"https://realpython.com/python-speech-recognition/\">hier</a> für eine Einführung in die Funktionsweise des Moduls). Für direkten Zugriff auf das Mikrofon wurde das <a href=\"https://pypi.org/project/PyAudio/\">PyAudio</a> verwendet. Die Klassifizierung des erkannten Textes erfolgt mittels <a href=\"https://docs.python.org/3/howto/regex.html\">Regular Expressions</a>.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837bcdcb-1ef8-46ce-a10e-4a92f23507e2",
   "metadata": {},
   "source": [
    "Zunächst importieren wir die benötigten Module und implementieren die Hilfsklasse _textFromSpeech_, welche Spracherkennung direkt mittels eines angeschlossenen Mikrofons oder anhand von gespeicherten Demodateien durchführt und den erkannten Text zurückgibt. Danach implementieren wir noch die _chooseMic_ Methode, welche es uns erlaubt, das zu verwendende Mikrofon aus allen am PC angeschlossenen Audiogeräten auszuwählen. Dabei ist die erste Auswahlmöglichkeit (0) das Standardmikrofon des Systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1f965-df51-4680-a615-1fe7d3ebac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import random\n",
    "import re\n",
    "\n",
    "class textFromSpeech:\n",
    "    \"\"\" Hilfsklasse zur Durchführung von Speech to Text mittels Google Webspeech API \"\"\"\n",
    "    def __init__(self, mic_id=0):\n",
    "        self.recogniser = sr.Recognizer()\n",
    "        self.mic = sr.Microphone(device_index=mic_id)\n",
    "        self.demofilePath = './voiceClips/'\n",
    "        pass\n",
    "    def fromMic(self):\n",
    "        \"\"\" Fertigt eine Aufnahme mittels dem Standardmikrofon des Systems an und führt Speech to Text aus. \"\"\"\n",
    "        # Öffnen des Mikrofons\n",
    "        with self.mic as source:\n",
    "            # Einstellung der Umgebungslautstärke\n",
    "            print('Kalibriere das Mikrofon auf Zimmerlautstärke. Bitte kurz Ruhe.')\n",
    "            self.recogniser.adjust_for_ambient_noise(source)\n",
    "            # Aufnahme der Audiodatei\n",
    "            print('Jetzt kann gesprochen werden.')\n",
    "            audio = self.recogniser.listen(source)\n",
    "            print('Danke, verarbeite das Audiosignal.')\n",
    "        # Speech to Text mittels Googles Webspeech API\n",
    "        text = None\n",
    "        error = None\n",
    "        try:\n",
    "            text = self.recogniser.recognize_google(audio, language=\"de-DE\")\n",
    "        except sr.RequestError:\n",
    "            text = 'Service API not available'\n",
    "            error = True\n",
    "        except sr.UnknownValueError:\n",
    "            text = 'Could not recognise Speech'\n",
    "            error = True\n",
    "        else:\n",
    "            error = False\n",
    "        # Rückgabe des Textes\n",
    "        return error, text\n",
    "    def fromDemofile(self):\n",
    "        \"\"\" Führt Speech to Text anhand einer zufällig ausgewählten Demodatei aus. \"\"\"\n",
    "        # Zufällige Auswahl einer der vorab aufgenommenen Kommandos\n",
    "        sample = random.randint(1, 4)\n",
    "        path = self.demofilePath + '{:02d}'.format(sample) + '.wav'\n",
    "        print('Benutzte Demodatei: {}'.format(path))\n",
    "        audiofile = sr.AudioFile(path)\n",
    "        # Aufnahme der Audiodatei\n",
    "        with audiofile as source:\n",
    "            audio = self.recogniser.record(source)\n",
    "        # Speech to Text mittels Googles Webspeech API\n",
    "        text = None\n",
    "        error = None\n",
    "        try:\n",
    "            text = self.recogniser.recognize_google(audio, language=\"de-DE\")\n",
    "        except sr.RequestError:\n",
    "            text = 'Service API not available'\n",
    "            error = True\n",
    "        except sr.UnknownValueError:\n",
    "            text = 'Could not recognise Speech'\n",
    "            error = True\n",
    "        else:\n",
    "            error = False\n",
    "        # Rückgabe des Textes\n",
    "        return error, text\n",
    "\n",
    "def chooseMic():\n",
    "    \"\"\" Assistent zur Auswahl des zu verwendenden Mikrofons \"\"\"\n",
    "    print(\"Verfügbare Mikrofone:\")\n",
    "    for i, entry in enumerate(sr.Microphone.list_microphone_names()):\n",
    "        print('{} - {}'.format(i, entry))\n",
    "    print(\"Bitte ID des zu verwendenden Mikrofons eingeben:\")\n",
    "    while True:\n",
    "        try:\n",
    "            num = int(input())\n",
    "        except ValueError:\n",
    "            print(\"Bitte ID des Mikrofons als Zahl eingeben.\")\n",
    "        else:\n",
    "            break\n",
    "    return num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7dc8b-2488-4fbd-931a-f29a720d6302",
   "metadata": {},
   "source": [
    "Mittels einer Variable kann festgelegt werden, ob die Spracherkennung anhand der Demodateien oder direkt eines Mikrofons durchgeführt wird. Bei Verwendung der Demodateien, wird eine der verfügbaren Dateien zufällig ausgewählt und verarbeitet.\n",
    "\n",
    "Die oben implementierte Hilfsklasse zur Spracherkennung wird instanziiert, um die Spracherkennung durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bbcb0-b629-4763-a792-09e0b5a9cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanzierung der Hilfsklasse für Speech to Text\n",
    "# Soll ein Mikrofon statt den Democlips verwendet werden, muss useDemofile=False gesetzt werden\n",
    "useDemofile = True\n",
    "\n",
    "# Einlesen einer Demodatei oder Eingabe durch Mikrofon\n",
    "if useDemofile:\n",
    "    rec = textFromSpeech()\n",
    "    error, text = rec.fromDemofile()\n",
    "else:\n",
    "    mic_id = chooseMic()\n",
    "    rec = textFromSpeech(mic_id=mic_id)\n",
    "    error, text = rec.fromMic()\n",
    "\n",
    "if error:\n",
    "    print('Die Audioeingabe konnte nicht zu Text verarbeitet werden. Aufgetretener Fehler:')\n",
    "    print(text)\n",
    "else:\n",
    "    print('Erkannter Text:', end=\" \")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea1ada-bd5c-4c57-ab68-f16775ab8351",
   "metadata": {},
   "source": [
    "Nun wenden wir Regular Expressions an, um zu zählen, wie oft bestimmte Schlüsselwörter im Text vorkommen. Die _keywordClassification_ implementiert diese Funktionalität. Die _amax_ Methode gibt den Index des Elements mit höchstem Wert in einer Liste an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75dc88-2465-491a-8c9a-7f22b27fda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verarbeitung des Textes mittels Natural Language Processing\n",
    "\n",
    "def keywordClassification(classes, classKeywords, text):\n",
    "    \"\"\" Zählt wie oft angegebene Schlüsselwörter im Text vorkommen \"\"\"\n",
    "    # Generierung der Regular Expressions zum Absuchen der Klassen\n",
    "    regexes = []\n",
    "    for c in classKeywords:\n",
    "        regexes.append('\\\\b(?:' + '|'.join(c) + ')\\\\b')\n",
    "    # Der Text wird anhand von Schlagwörtern einer Klasse zugeordnet\n",
    "    classScore = [0 for i in range(len(classes))]\n",
    "    for i, expression in enumerate(regexes):\n",
    "        classScore[i] += len(re.findall(re.compile(expression), text.lower()))\n",
    "    return classScore\n",
    "\n",
    "def amax(a):\n",
    "    \"\"\" Gibt den Index des maximalen Elements in einer Liste an \"\"\"\n",
    "    return a.index(max(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614463f4-bb49-435e-9c87-eb44972fce2b",
   "metadata": {},
   "source": [
    "Letztendlich legen wir die Schlüsselwörter für beide Klassen (Kaffee und Espresso) fest, zählen welche Kaffeeart und welche Zahl am öftesten Vorkommen und erstellen daraus die Bestellung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07900ce9-ac07-4c36-b36a-97f421bbb87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coffeebot kann Bestellungen für einen oder mehrere Kaffee oder Espressi aufnehmen\n",
    "# Wir wollen dabei ermitteln, was er dem Nutzer bringen soll\n",
    "\n",
    "# Festlegen der Schlüsselwörter und Klassen\n",
    "coffeeClasses = [\n",
    "    'Lungo',\n",
    "    'Espresso'\n",
    "]\n",
    "\n",
    "coffeeClassKeywords = [\n",
    "    ['kaffee', 'lungo', 'coffee', 'verlängerter', 'verlängerte', 'verlängert', 'großer', 'große'],\n",
    "    ['espresso', 'brauner', 'klein', 'kleine', 'espressi']\n",
    "]\n",
    "\n",
    "amountClassKeywords = [\n",
    "    ['ein', '1' 'eins', 'einen'],\n",
    "    ['zwei', '2'],\n",
    "    ['drei', '3'],\n",
    "    ['vier', '4'],\n",
    "    ['fünf', '5']\n",
    "]\n",
    "\n",
    "amountClasses = [str(i+1) for i in range(len(amountClassKeywords))]\n",
    "\n",
    "# Zählen, wie oft Schlüsselwörter jeder Klasse vorkommen\n",
    "coffeeType = keywordClassification(coffeeClasses, coffeeClassKeywords, text)\n",
    "coffeeAmount = keywordClassification(amountClasses, amountClassKeywords, text)\n",
    "\n",
    "# Ist die Bestellung nicht eindeutig, wird nichts beestellt\n",
    "# Ist sie eindeutig, wird die Bestellung ausgegeben\n",
    "if coffeeType[0] == coffeeType[1]:\n",
    "    print(\"Bestellung ist nicht eindeutig, bitte noch einmal versuchen.\")\n",
    "else:\n",
    "    print(\"Erkannte Bestellung: {} mal {}\".format(amountClasses[amax(coffeeAmount)], coffeeClasses[amax(coffeeType)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
